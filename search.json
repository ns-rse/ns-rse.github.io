[
  {
    "objectID": "posts/r-resources/index.html",
    "href": "posts/r-resources/index.html",
    "title": "R Resources",
    "section": "",
    "text": "R is a statistical programming languages and one of the most popular languages for data analysis, statistics and plotting in academia and industry. Learning a new language can be daunting, particularly if you have no experience of scripting and are used to Graphical User Interfaces (GUIs) where you point and click to perform your statistical analysis.\nFear not though, there are lot of resources and very friendly, enthusiastic and helpful R users out there who can help you on your journey learning R. This post details some of them, and I’d welcome additions.\nMost of these resources are links websites that are free and openly available. Where books are linked they are very often freely available on-line, but there will also often be the possibility of purchasing a hard copy, which you may want to consider doing if you find the resource useful to help support the authors.\nR has a number of bodies, organisations and companies associated with it."
  },
  {
    "objectID": "posts/r-resources/index.html#software",
    "href": "posts/r-resources/index.html#software",
    "title": "R Resources",
    "section": "Software",
    "text": "Software\nR is software and will need installing on your computer. Because it is Free Open Source Softrware (FOSS) you can download and install it on your computer for free. You will have to install it to use it and the isntr\n\nGit Version Control\nIt is good practice to version control the code you write, it provides an electronic paper trail of how your code has evolved over time and allows you to keep track not just of the code itself but why it has changed or been written.\nThese days the most popular version control system is Git and projects are often hosted/backed up on popular “forges” such as GitHub or GitLab. Sign up with an academic email address (@&lt;institute&gt;.ac.uk or @&lt;institute&gt;.edu) and you will have a few extra benefits.\nLearning Git is a whole, vast, topic in and of itself, but to get started with R and Git see the recommendation below. If you are a student or researcher at The University of Sheffield you may want to consider taking the Research Software Engineering (RSE) Teams popular Git, GitHub and GitKraken : Zero to Hero course which runs regularly throughout the year. Sign up to their mailing list and you’ll be notified of when the course runs. Alternatively email them to find out when the next course is scheduled to run.\n\n\nIDEs\nIntegrated Development Environments (IDE) are software that help you write code faster and more consistently courtesy of various features such as syntax highlighting, automatic bracket and quote pairing, automatic indentation and a suite of functions for performing common tasks such as version controlling files or rendering documents.\nThe most popular IDE for R is RStudio Desktop which has excellent support for R, RMarkdown/Quarto and basic Git support. If you are new to version control with Git you may want to consider using GitKraken which provides an intuitive point and click interface for version controlling your files and working with GitHub/GitLab.\nMy personal preference is to use Emacs and the package Emacs Speaks Statistics (ESS). This is a robust solution (ESS) has been around for decades and you get the convenience of using Emacs and its many packages such as the amazing Magit for carrying out all Git related tasks. It has a steeper learning curve than RStudio but in my opinion is completely worth the effort."
  },
  {
    "objectID": "posts/r-resources/index.html#books",
    "href": "posts/r-resources/index.html#books",
    "title": "R Resources",
    "section": "Books",
    "text": "Books\nIf you’re using R the chances are you want to perform some sort of Statistical Analysis on your data. This often involves cleaning data that has been received, writing code to summarise, tabulate and plot your data, often in a literate manner (which means reports are open and can be reproduced easily). If you read nothing else to get you started using R for this work then you should read R for Data Science by Hadley Wickham and Garrett Grolemund. This is an excellent book that is available for free online.\n\nteacheR - Teach Yourself or Others R by Adam Rawles\nCookbook for R by Winston Chang is a useful reference for many common tasks.\n\n\nQuarto/RMarkdown\nR has its own Markdown language for writing literate documents and a comprehensive resources covering all aspects is R Markdown: The Definitive Guide by Yihui Xie, J.J. Allaire and Garret Grolemund. By writing your work in R Markdown you are performing literate programming and it means your report can updated automatically if the underlying data changes. Output to HTML, PDF, LibreOffice, Microsoft Office and many other formats. The underlying source can be version controlled using Git so that it is documented, backed up (e.g. on GitHub or GitLab) and it is easy to collaborate with colleagues.\nMore recently Posit (nee RStudio) have developed Quarto the next iteration of RMarkdown. It supports more document types (e.g. blogs and RevealJS slides) and has excellent documentation and a growing number of extensions. If you are just starting out I would recommend using Quarto over RMarkdown.\n\n\nGit Book\nIt is good practice to version control your code and literate documents as you develop them. This can be achieved using the version control system Git. Get yourself an account on GitHub and/or GitLab and settle down to read Jenny Bryans excellent Happy Git and GitHub for the useR.\n\n\nTidyverse\nYou will hear a lot about the Tidyverse which is an opinionated collection of R packages designed for data science. They are well worth learning as they make writing code considerably easier than with the base R packages. You won’t need all of the packages immediately but key ones to learn are\n\ndplyr (or\ntidyr for tidying your data.\nforcats for working with categorical variables.\nlubridate for working with date variables.\nstringr for working with string variables.\n\nIf you’ve large datasets the dtplyr which uses the data.table package in the background but with dplyr code. data.table is considerably faster than dplyr for many operations. This is particularly noticeable when you have large datasets.\n\n\nStatistics\nThere is a wealth of resources out there for learning and using R for different topics. The following is that which I’m aware of, if there is an omission please open an issue on my blog\n\nRegression Modeling Strategies by Frank E. Harrell, Jr.\nAn Introduction to Statistical Learning with Applications in R/Python an excellent book on modern “machine learning” techniques.\nTidy Modeling with R by Max Kuhn and Julia Silge\nApplied Predictive Modeling by Max Kuhn and Kjell Johnson (site to accompany physical book)\nHands-On Machine Learning with R by Bradley Boehmke and Brandon Greenwell\nInterpretable Machine Learning by Christoph Molnar\nThe Hitchikers Guide to Responsible Machine Learning by Przemsylaw Biecek, Anna Kozak and Aleksander Zawada\nIntroduction to Data Science and Advanced Data Science by Rafael A. Irizarry\nTelling Stories with Data With Applications in R by Rohan Alexander\nIntroduction to Modern Statistics (1st Ed)\nThe Epidemiologist R Handbook R for applied epidemiology and public health\nR for Health Data Science by Ewen Harrison and Riinu Pius\nForecasting: Principles and Practice (3rd ed)\n\n\nBayesian Statistics\nThere are some excellent resources for learning Bayesian Analyses with R. Perhaps the most comprehensive and in-depth is Statistical Rethinking by Richard McElreath. He runs regular free courses teaching the material in the book (Statistical Rethinking 2023) and the book content has been translated to other R frameworks and Python. Another very good book is Bayes Rules! An Introduction to Applied Bayesian Modeling. These are both covered in the Bayesian Statistics - Syllabus course by Andrew Heiss.\n\nStatistical Rethinking by Richard McElreath\n\n2023 lectures and course material\nStatistical rethinking with brms, ggplot2, and the tidyverse: Second edition\n\nBayes Rules! An Introduction to Applied Bayesian Modeling by Alicia A. Johnson, Miles Q. Ott and Mine Dogucu\nGaussian Processes for Machine Learning by Carl Edward Rasmussen and Christopher K. I. William\n\n\n\n\nPlotting\nR has excellent support for producing graphs, figures and data visualisations. There is the base graphics that have been around since the beginning, but more recently the ggplot2 framework introduced by Hadley Wickham which implements Leland Wilkinson’s Grammar of Graphics has been very popular.\n\nggplot2: Elegant Graphics for Data Analysis (3e) by Hadley Wickham\nThe R Graph Gallery – Help and inspiration for R charts\nData visualization with R and ggplot2 | the R Graph Gallery\nggplot2tor\n\n\n\nAdvanced Topics\nThere is a lot to R than just Statistical analysis and one day you may want to investigate these in greater detail. The links below are to more advanced topics such as writing and maintaining packages or specific tasks such as text mining.\n\nR Packages by Hadley Wickham and Jenny Bryan\nAdvanced R by Hadley Wickham\nAdvanced Statistical Computing by Roger D. Peng\nMastering Shiny by Hadley Wickham\nOutstanding User Interfaces with Shiny by Kenton Russel\nReproducible Analytical Pipelines by Bruno Rodrigues\nText Mining with R by Julia Silge and David Robinson"
  },
  {
    "objectID": "posts/r-resources/index.html#cran",
    "href": "posts/r-resources/index.html#cran",
    "title": "R Resources",
    "section": "CRAN",
    "text": "CRAN\nThe Comprehensive R Archive Network (CRAN) is the primary place to look for R packages. It also contains a number of subject specific Task Views which are pages that summarise the packages and resources associated with a particular topic. There are also links to the official manuals, FAQs and user contributed documentation."
  },
  {
    "objectID": "posts/r-resources/index.html#the-r-journal",
    "href": "posts/r-resources/index.html#the-r-journal",
    "title": "R Resources",
    "section": "The R Journal",
    "text": "The R Journal\nThe R Journal is the peer-reviewed, open-access scientific journal published by the R Foundation. It includes articles on packages, reviews and proposals, comparisons and benchmarking, applications of existing techniques and special issue articles to accompany conferences or particular topics."
  },
  {
    "objectID": "posts/r-resources/index.html#cheatsheats",
    "href": "posts/r-resources/index.html#cheatsheats",
    "title": "R Resources",
    "section": "Cheatsheats",
    "text": "Cheatsheats\nCheatsheets come in handy as a reference to packages and commands. A central repository of cheatsheets is maintained by Postit."
  },
  {
    "objectID": "posts/r-resources/index.html#community",
    "href": "posts/r-resources/index.html#community",
    "title": "R Resources",
    "section": "Community",
    "text": "Community\nThe R community is incredibly supportive, welcoming and helpful. There are over 600 User Groups around the world where R users meet up and share their experience and knowledge and support each other. Sheffield has its own SheffieldR User Group.\n\nR Ladies\nR-Ladies is a worldwide organisation whose mission is to promote gender diversity in the R Community. Groups around the world have their own meetups and activities.\n\n\nR4DS\nThere is also the R4DS Online Learning Commuity which helps you work through the R for Data Science book. They have an active Slack channel for coordinating the courses and run Tidy Tuesday, a weekly podcast and community activity which is a great way of learning now tasks in R.\n\n\nNHS R Community\nThe NHS R Community is focused on applications of R in the NHS Research community. They have blogs, a Slack channel and conferences."
  },
  {
    "objectID": "posts/r-resources/index.html#blogs",
    "href": "posts/r-resources/index.html#blogs",
    "title": "R Resources",
    "section": "Blogs",
    "text": "Blogs\nThe R Bloggers site aggregates blogs from people who write about R and is a brilliant resource. A few highlights are noted as well but R Bloggers is probably the best resource. If you want to subscribe to these most have RSS feeds\n\nTidyverse blog\nNotes from a data witch a blog by Danielle Navarro"
  },
  {
    "objectID": "posts/r-resources/index.html#mastodon",
    "href": "posts/r-resources/index.html#mastodon",
    "title": "R Resources",
    "section": "Mastodon",
    "text": "Mastodon\nFind posts and resources on Mastodon by searching for the #rstats hashtag. Here are some people I follow and find useful information from.\n\nBots\n\n@rpodcast@podcastindex.social\n@rweekly@fosstodon.org toots weekly updates from the R Community\n@CRANberriesFeed@mas.to a bot that toots about packages released or updated on CRAN.\n\n\n\nPeople\n\n@e3mma@mastodon.social Emma is a bioinformatician and lecturer at York University who teaches R and reproducibility.\n@f2harrel@mastodon.online Frank is the author of the Regression Modelling Strategies book.\n@Cmastication@mastodon.social JD Long an avid R user.\n@hadleywickham@fosstodon.org doyen of R packages and the Tidyverse.\n@HeathrTurnr Heather Turner is a Research Software Engineering Fewllow in Statistics at University of Warrick and an R Foundation board member.\n@eddelbuettel@mastodon.social ESS and R developer.\n@sje@fosstodon.org ESS developer.\n@robjhyndman@aus.social co-author of Forecasting Principles and Practice.\n@topepo@fosstodon.org author of Applied Predictive Modelling and co-author of other R books; R package developer (tidymodels).\n@annakrystalli@fosstodon.org Research Software Engineer, [ReproHack founder] and editor at rOpenSci\n\n\n\nOrganisations\n\n@ropensci@fosstodon.org\n@Posit@fosstodon.org the company that develops RStudio, Quarto and Shiny"
  },
  {
    "objectID": "posts/r-resources/index.html#podcasts",
    "href": "posts/r-resources/index.html#podcasts",
    "title": "R Resources",
    "section": "Podcasts",
    "text": "Podcasts\n\nThe R-Podcast\nShiny Developer Series"
  },
  {
    "objectID": "posts/r-resources/index.html#miscellaneous",
    "href": "posts/r-resources/index.html#miscellaneous",
    "title": "R Resources",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nGenerative Art\nMany people enjoy playing with R and ggplot2 to create Generative Art. One example is aRtsy, an R package that implements algorithms for making generative art in a straightforward and standardized manner using ‘ggplot2’ (see archive of posts on @aRtsy_package@mastodon). Another package is aRt by Nicola Rennie.\n\naRtsy\naRt by Nicola Rennie\nGenerative art resources in R"
  },
  {
    "objectID": "posts/pytest-xfail/index.html",
    "href": "posts/pytest-xfail/index.html",
    "title": "Pytest Fail and Skip",
    "section": "",
    "text": "Pytest is an excellent framework for writing tests in Python. Sometimes tests don’t pass though and you might want to mark them as failing or skip them.\nPytest has a few decorators available to help with skipping tests using @pytest.mark.skip or @pytest.mark.skipif or allowing tests to fail with @pytest.mark.xfail\nWe’ll use the pytest-examples repository for looking at how these work."
  },
  {
    "objectID": "posts/pytest-xfail/index.html#why",
    "href": "posts/pytest-xfail/index.html#why",
    "title": "Pytest Fail and Skip",
    "section": "Why?",
    "text": "Why?\nThere are a number of reasons why you may wish to deliberately and Eric covers them nicely. In brief…\n\nIncompatible Python or Package Version - some tests don’t pass under a certain version.\nPlatform specific issues - some tests fail on a specific platform.\nExternal dependencies - if you haven’t got round to mocking a service.\nLocal dependencies - excluding tests running under Continuous Integration that rely on local dependencies."
  },
  {
    "objectID": "posts/pytest-xfail/index.html#choose-your-partner---skip-to-my-lou",
    "href": "posts/pytest-xfail/index.html#choose-your-partner---skip-to-my-lou",
    "title": "Pytest Fail and Skip",
    "section": "Choose your Partner - Skip (to my lou)",
    "text": "Choose your Partner - Skip (to my lou)\nIf you want to unconditionally skip a test prefix it with @pytest.mark.skip(), adding a reason can be useful and there is the argument reason=\"&lt;reason&gt;\"to do so, it helps others, including your future self. If we use the tests/test_divide.py from the pytest-examples repository we can skip the redundant test_divide_unparameterised() function as its already covered by the parameterised test that follows.\n@pytest.mark.skip(reason=\"redundant - covered by test_divide()\")\ndef test_divide_unparameterised() -&gt; None:\n    \"\"\"Test the divide function.\"\"\"\n    assert divide(10, 5) == 2\nWhen we run the test we are told it is skipped. To keep things fast we run just that test using the command line version pytest &lt;file&gt;::&lt;test_function&gt; but your IDE may support running individual tests (in Emacs you can use pytest.el to the same effect).\n❱ pytest tests/test_divide::test_divide_unparameterised\n======================================= test session starts ============================================\nplatform linux -- Python 3.11.9, pytest-7.4.4, pluggy-1.5.0\nMatplotlib: 3.8.4\nFreetype: 2.6.1\nrootdir: /mnt/work/git/hub/ns-rse/pytest-examples/main\nconfigfile: pyproject.toml\nplugins: durations-1.2.0, xdist-3.5.0, pytest_tmp_files-0.0.2, mpl-0.17.0, lazy-fixture-0.6.3, cov-5.0.0\ncollected 1 item\n\ntests/test_divide.py s                                                                            [100%]\n\n---------- coverage: platform linux, python 3.11.9-final-0 -----------\nName                        Stmts   Miss  Cover\n-----------------------------------------------\npytest_examples/divide.py      16      8    50%\npytest_examples/shapes.py       5      5     0%\n-----------------------------------------------\nTOTAL                          21     13    38%\n\n====================================== short test summary info =========================================\nSKIPPED [1] tests/test_divide.py:9: redundant - covered by test_divide()\n====================================== 1 skipped in 0.59s =============================================="
  },
  {
    "objectID": "posts/pytest-xfail/index.html#choose-your-partner---failing",
    "href": "posts/pytest-xfail/index.html#choose-your-partner---failing",
    "title": "Pytest Fail and Skip",
    "section": "Choose your Partner - Failing (…)",
    "text": "Choose your Partner - Failing (…)\nNothing in the the old dance about failing but you can selectively allow tests to fail using the pytest.mark.xfail() fixture. If you know a test is going to fail you can, rather than commenting it out, mark it as such. If we update the test condition so we know it will fail we mark that it will fail as follows.\n@pytest.mark.xfail(reason=\"demonstrate expected failure\")\ndef test_divide_unparameterised() -&gt; None:\n    \"\"\"Test the divide function.\"\"\"\n    assert divide(10, 5) == 3\nAnd running pytest on this shows the failure\n❱ pytest tests/test_divide.py::test_divide_unparameterised\n====================================== test session starts =============================================\nplatform linux -- Python 3.11.9, pytest-7.4.4, pluggy-1.5.0\nMatplotlib: 3.8.4\nFreetype: 2.6.1\nrootdir: /mnt/work/git/hub/ns-rse/pytest-examples/main\nconfigfile: pyproject.toml\nplugins: durations-1.2.0, xdist-3.5.0, pytest_tmp_files-0.0.2, mpl-0.17.0, lazy-fixture-0.6.3, cov-5.0.0\ncollected 1 item\n\ntests/test_divide.py x                                                                            [100%]\n\n\n---------- coverage: platform linux, python 3.11.9-final-0 -----------\nName                        Stmts   Miss  Cover\n-----------------------------------------------\npytest_examples/divide.py      16      6    62%\npytest_examples/shapes.py       5      5     0%\n-----------------------------------------------\nTOTAL                          21     11    48%\n\n====================================== short test summary info =========================================\nXFAIL tests/test_divide.py::test_divide_unparameterised - demonstrate expected failure\n====================================== 1 skipped in 0.59s =============================================="
  },
  {
    "objectID": "posts/pytest-xfail/index.html#conditional-skippingfailing",
    "href": "posts/pytest-xfail/index.html#conditional-skippingfailing",
    "title": "Pytest Fail and Skip",
    "section": "Conditional Skipping/Failing",
    "text": "Conditional Skipping/Failing\nThe pytest.mark.skipif() and pytest.mark.xfail() fixtures both have the argument condition which allows you to use a Boolean (i.e. a statement that evaluates to True or False) to determine whether they are used. Any Python expression that can be evaluated to True or False can be used and for backwards compatibility strings can still be used. If condition argument is used in pytest.mark.xfail() then the reason argument must also be given indicating why the test is being skipped/is expected to failed.\nHere we fail the test only if the Python version is 3.10.*. Note the need to import sys and the use of sys.version_info[:2] to extract a tuple of the major and minor Python version).\nimport sys\n\n@pytest.mark.xfail(sys.version_info[:2] == (3, 10), reason=\"Skip under Python 3.10\"))\ndef test_divide_unparameterised() -&gt; None:\n    \"\"\"Test the divide function.\"\"\"\n    assert divide(10, 5) == 3"
  },
  {
    "objectID": "posts/pytest-xfail/index.html#skippingfailing-parameterised-tests",
    "href": "posts/pytest-xfail/index.html#skippingfailing-parameterised-tests",
    "title": "Pytest Fail and Skip",
    "section": "Skipping/Failing Parameterised Tests",
    "text": "Skipping/Failing Parameterised Tests\nIn many instances you can parameterise tests, and you can use the fixtures we’ve covered against the whole test. But what if you want to skip not all of the parameterised tests but only specific ones? This is possible because as covered previously you can use pytest.param() function to define your parameters and give them id=\"some text\" to help identify them. pytest.param() also has a marks= option which allows you to add pytest.mark.* to just that set of parameters and so we can add pytest.mark.xfail() or pytest.mark.skip[if]() to specific sets of parameters.\nInstead of placing the fixture before the test so that it applies to all functions, you use the pytest.param() for each set of parameters and add pytest.mark.xfails() (or other variants) as arguments to the marks option.\nHere we mark the test with id of zero division error with marks=pytest.mark.xfail as we know that a division by zero test will fail and so that set of parameters should be skipped.\n@pytest.mark.parametrize(\n    (\"a\", \"b\", \"expected\"),\n    [\n        pytest.param(10, 5, 2, id=\"ten divided by five\"),\n        pytest.param(9, 3, 3, id=\"nine divided by three\"),\n        pytest.param(5, 2, 2.5, id=\"five divided by two\"),\n        pytest.param(0, 100, 0, id=\"zero divided by one hundred\"),\n        pytest.param(\n            10, 0, ZeroDivisionError, id=\"zero division error\", marks=pytest.mark.xfail(reason=\"Expected to fail\")),\n    ],\n)\ndef test_divide(a: float | int, b: float | int, expected: float) -&gt; None:\n    \"\"\"Test the divide function.\"\"\"\n    assert divide(a, b) == expected\n❱ pytest tests/test_divide.py::test_divide\n====================================== test session starts =============================================\n\nplatform linux -- Python 3.11.9, pytest-7.4.4, pluggy-1.5.0\nMatplotlib: 3.8.4\nFreetype: 2.6.1\nrootdir: /mnt/work/git/hub/ns-rse/pytest-examples/main\nconfigfile: pyproject.toml\nplugins: durations-1.2.0, xdist-3.5.0, pytest_tmp_files-0.0.2, mpl-0.17.0, lazy-fixture-0.6.3, cov-5.0.0\ncollected 5 items\n\ntests/test_divide.py ....x                                                                        [100%]\n\n---------- coverage: platform linux, python 3.11.9-final-0 -----------\nName                        Stmts   Miss  Cover\n-----------------------------------------------\npytest_examples/divide.py      16      3    81%\npytest_examples/shapes.py       5      5     0%\n-----------------------------------------------\nTOTAL                          21      8    62%\n\n====================================== short test summary info =========================================\nXFAIL tests/test_divide.py::test_divide[zero division error] - Expected to fail\n====================================== 4 passed, 1 xfailed in 0.37s ====================================\nThe condition/reason arguments to both pytest.mark.skipif() and pytest.mark.xfail() functions are still valid and can be used to conditionally mark specific sets of parameters to be skipped or indicate if they will fail under certain conditions.\nTo exclude the test with id of five divided by two under Python 3.10 we would do the following (again note the need to import sys and its use in the cond positional argument).\nimport sys\n\n...\n\n@pytest.mark.parametrize(\n    (\"a\", \"b\", \"expected\"),\n    [\n        pytest.param(10, 5, 2, id=\"ten divided by five\"),\n        pytest.param(9, 3, 3, id=\"nine divided by three\"),\n        pytest.param(5, 2, 2.5, id=\"five divided by two\", marks=pytest.mark.xfail(sys.version_info[:2] == (3, 10),\n                                                                                  reason=\"Skip under Python 3.10\")),\n        pytest.param(0, 100, 0, id=\"zero divided by one hundred\"),\n        pytest.param(10, 0, ZeroDivisionError, id=\"zero division error\", marks=pytest.mark.xfail),\n    ],\n)\ndef test_divide(a: float | int, b: float | int, expected: float) -&gt; None:\n    \"\"\"Test the divide function.\"\"\"\n    assert divide(a, b) == expected"
  },
  {
    "objectID": "posts/pytest-xfail/index.html#summary",
    "href": "posts/pytest-xfail/index.html#summary",
    "title": "Pytest Fail and Skip",
    "section": "Summary",
    "text": "Summary\nPytest has features which help support test development and allow specific tests to fail or be skipped completely which helps with both test development and with Continuous Integration where test results can vary depending on platform and package versions.\nThis post stems from a suggestion made by @jni@jni@fosstodon.org during some work I have been contributing to the skan package. Thanks to Juan for the prompt/pointer."
  },
  {
    "objectID": "posts/pytest-xfail/index.html#links",
    "href": "posts/pytest-xfail/index.html#links",
    "title": "Pytest Fail and Skip",
    "section": "Links",
    "text": "Links\n\npytest\n@pytest.mark.skip\n@pytest.mark.skipif\n@pytest.mark.xfail\nPytest Parameterisation\n\n\nBlog Posts\n\nPytest With Eric | An Ultimate Guide to using Pytest Skip Test and XFail"
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html",
    "href": "posts/pre-commit-hooks/index.html",
    "title": "Pre-Commit : Useful Hooks",
    "section": "",
    "text": "I’m a big fan of pre-commit and have written about it before (see posts on pre-commit, pre-commit CI and pre-commit updating). This post discusses some of the hooks that I use and how to configure them."
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#python-linting",
    "href": "posts/pre-commit-hooks/index.html#python-linting",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Python Linting",
    "text": "Python Linting\n\nRuff\nruff is a Python linter written in Rust which means its considerably faster than many native linters. It aims for parity with Flake8 and covers a lot of the linting that PyLint undertakes too. Its configured via pyproject.toml which makes incorporating it into your Python Package simple.\nrepos:\n  - repo: https://github.com/charliermarsh/ruff-pre-commit\n    rev: v0.0.191\n    hooks:\n      - id: ruff\nConfiguration is, as noted, via pyproject.toml and you may find the post on Python Packaging worth reading to understand more on this.\n[tool.ruff]\nexclude = []\n# per-file-ignores = []\nline-length = 120\ntarget-version = \"py310\"\n\n# Allow autofix for all enabled rules (when `--fix`) is provided.\nfixable = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"R\", \"S\", \"W\", \"U\"]\nunfixable = []\n\n\nBlack\nBlack is an opinionated formatter for Python that is PEP8 compliant. By using black to format your code you end up with a consistent style across the code base and commit changes end up being minimal. This helps speed up code-review of pull-requests.\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 22.10.0\n    hooks:\n      - id: black\n        types: [python]\nConfiguration is, as noted, via pyproject.toml and you may find the post on Python Packaging worth reading to understand more on this.\n[tool.black]\nline-length = 120\ntarget-version = [\"py38\", \"py39\", \"py310\"]\ninclude = \"\\\\.pyi?$\"\n\n\npydocstyle\nYou can check your docstrings are correctly written using the pydocstyle hook.\nIts pretty straight-forward to use and accepts arguments so you can pass all the command line options you might want to use into the hook when it runs. It supports three different doc string styles, pep257, numpy and google.\n    - repo  https://github.com/pycqa/pydocstyle\n        rev: 6.3.0  # pick a git hash / tag to point to\n        hooks:\n        - id: pydocstyle\n        args:\n        - --convention=numpy\n        # Optionally ignore rules\n        - --ignore=D101,D2\nAlternatively you can add configuration options to your projects pyproject.toml under a [tool.pydocstyle] section.\n[tool.pydocstyle]\nconvention = \"numpy\"\nignore = [\n  \"D101\",\n  \"D2\"\n]"
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#markdown-linting",
    "href": "posts/pre-commit-hooks/index.html#markdown-linting",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Markdown Linting",
    "text": "Markdown Linting\nmarkdownlint-cli2 is a useful and highly configurable hook for linting Markdown and CommonMark. I wanted to use it on this blog though which is written using Quarto and therefore uses PandocMarkdown with files that have extension .qmd. I therefore enable the hook in .pre-commit-config.yaml with a configuration file specified\nrepos:\n- repo: https://github.com/DavidAnson/markdownlint-cli2\n  rev: v0.6.0\n  hooks:\n    - id: markdownlint-cli2\n      args: [.markdownlin-cli2.yaml]\n..and add a sample configuration file (e.g. .mardownlint-cli2.yaml although other formats such as JSON can be used) is shown below and markdownlint-cli2 picks this up automatically.\n# Configuration\nconfig:\n  # MD013 - line-length\n  line_length:\n    line_length: 120\n    code_blocks: false\n    tables: false\n  html:\n    allowed_elements:\n      - div\n\n# Globs\nglobs:\n  - \"**/*.qmd\"\n  - \"*.qmd\"\n\n# Fix any fixable errors\nfix: false"
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#emacs-lisp",
    "href": "posts/pre-commit-hooks/index.html#emacs-lisp",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Emacs Lisp",
    "text": "Emacs Lisp\nAs I use Emacs I have recourse to write some Emacs Lisp and so its useful to applying formatting to my .el files before committing them. lisp-format does the job nicely.\nrepos:\n  - repo: https://github.com/eschulte/lisp-format\n    rev: 088c8f78ca41204b44f2636275517ac09a2de6a9\n    hooks:\n      - id: lisp-format\n        name: formatter of lisp code\n        description: Run lisp-format against lisp files\n        language: script\n        files: \\.(lisp|cl|asd|scm|el)$\n        entry: lisp-format -i"
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#conclusion",
    "href": "posts/pre-commit-hooks/index.html#conclusion",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Conclusion",
    "text": "Conclusion\nThere are a lot of hooks out there to be used with pre-commit and incorporated into your Continuous Integration pipeline with pre-commit.ci. Which you find useful will depend to a large extent on the languages that you are using for any given project. Here I’ve focused mainly on common tools for Python Packages, Markdown and Lisp but you can find hooks for Docker, Ansible, Rust, Go, JavaScript, C++ and many more, there is even gitlint which lints your commit messages! Checkout the long list of available hooks and try some out."
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#links",
    "href": "posts/pre-commit-hooks/index.html#links",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Links",
    "text": "Links\n\nRelated pre-commit posts\n\npre-commit\npre-commit CI\npre-commit updating\n\n\n\nPre-commit hooks\n\npre-commit (pre-commit hooks)\npre-commit.ci\nruff\nmarkdownlint-cli2\nBlack\nlisp-format\ngitlint"
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html",
    "href": "posts/virtualenvwrapper/index.html",
    "title": "virtualenvwrapper",
    "section": "",
    "text": "If you use Python heavily you will likely be familiar with Virtual Environments. These provide isolated installs of specific packages that take precedence over any packages installed at the system level. There are lots of tools and frameworks for working with virtual environments such as venv, virtualenv and Conda. This post introduces and shows some of the features of virtualenvwrapper."
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html#virtualenvwrapper",
    "href": "posts/virtualenvwrapper/index.html#virtualenvwrapper",
    "title": "virtualenvwrapper",
    "section": "virtualenvwrapper",
    "text": "virtualenvwrapper\nvirtualenvwrapper is…\n\na set of extensions to Ian Bicking’s virtualenv tool. The extensions include wrappers for creating and deleting virtual environments and otherwise managing your development workflow, making it easier to work on more than one project at a time without introducing conflicts in their dependencies.\n\nIt has the following main features…\n\n\nOrganizes all of your virtual environments in one place.\nWrappers for managing your virtual environments (create, delete, copy).\nUse a single command to switch between environments.\nTab completion for commands that take a virtual environment as argument.\nUser-configurable hooks for all operations (see Per-User Customization).\nPlugin system for creating more sharable extensions (see Extending Virtualenvwrapper)."
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html#installation",
    "href": "posts/virtualenvwrapper/index.html#installation",
    "title": "virtualenvwrapper",
    "section": "Installation",
    "text": "Installation\nMany systems have virtualenvwrapper available in their package manager.\nemerge -av virtualenvwrapper          # Gentoo\npacman -Syu python-virtualenvwrapper  # Arch\napt-get install virtualenvwrapper     # Debian\nOnce installed you need to set two key variables $WORKON_HOME and $PROJECT_HOME and ensure the virtualenvwrapper.sh is sourced on starting a shell. $WORKON_HOME is where your virtual environments will be created and stored, whilst $PROJECT_HOME is where projects will be created if you choose to use the helper functions for making projects. Set these to what you want, my options are below. To find out where virtualenvwrapper.sh is installed on your system use which virtualenvwrapper.sh. Once you’ve decided substitute the values in the following of your .bashrc (Bash)or .zshrc (ZSH) depending on which shell you use.\nexport WORKON_HOME=${HOME}/.virtualenvs\nexport PROJECT_HOME=${HOME}/work/git/\nsource /usr/bin/virtualenvwrapper.sh"
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html#creating-a-virtual-environment",
    "href": "posts/virtualenvwrapper/index.html#creating-a-virtual-environment",
    "title": "virtualenvwrapper",
    "section": "Creating a Virtual Environment",
    "text": "Creating a Virtual Environment\nThis is straight-forward.\nmkvirtualenv &lt;env_name&gt;\n\npostmkvirtualenv\nBut what if there are some tools that you want to install each and every time you create a virtual environment, regardless of the project you are working on? For example I use the jedi-language-server and want to have various packages such as ipython, pytest and various extensions, linters such as ruff and pylint available by default in every environment you create. Fortunately there is a simple hook that can be run after the creation of a new environment. The file ~/.virtualenvs/postmkvirtualenv is sourced and run after having run mkvirtualenv and so any commands in there are executed as it is essentially a Bash script.\nIf you maintain a dotfiles directory and have a file that lists the packages you want installed under ~/dotfiles/python/venv_minimal_requirements.txt then you can have the files listed here installed when creating a new virtual environment by appending the command pip install -r ~/dotfiles/python/venv_minimal_requirements.txt to the ~/.virtualenvs/postmkvirtualenv file.\necho \"pip install -r ~/dotfiles/python/venv_minimal_requirements.txt\" &gt;&gt; ~/.virtualenvs/postmkvirtualenv"
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html#project-directories",
    "href": "posts/virtualenvwrapper/index.html#project-directories",
    "title": "virtualenvwrapper",
    "section": "Project Directories",
    "text": "Project Directories\nTypically code for a project resides in its own directory and this can be automatically bound to the virtual environment using the mkproject command instead of mkvirtualenv. The project directory is stored in the $PROJECT_HOME path you will have configured during installation. You can then create a project and a virtual environment with…\nmkproject new_project\n\nSwitching to Project Directories\nYou can switch to a projects directory automatically on activating a particular virtual environment using setvirtualenvproject from the project directory when a specific environment is activated. Make sure you are in the project directory for the corresponding project!. It adds an entry to the ~/.virtualenv/&lt;env_name&gt;/.project file that reflects the directory associated with the environment. Then when you activate the directory via workon &lt;env_name&gt; it will automatically change to the project directory."
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html#deactivating-and-removing-virtual-environments",
    "href": "posts/virtualenvwrapper/index.html#deactivating-and-removing-virtual-environments",
    "title": "virtualenvwrapper",
    "section": "Deactivating and Removing Virtual Environments",
    "text": "Deactivating and Removing Virtual Environments\nIts straight-forward to deactivate the current virtual environment just type deactivate. Similarly you can remove a virtual environment with rmvirtualenv &lt;env_name&gt;.\nOne neat option if you want to keep a virtual environment but install all packages anew is the ability to remove all third-party packages in the current virtual environment using wipeenv."
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html#temporary-virtual-environments",
    "href": "posts/virtualenvwrapper/index.html#temporary-virtual-environments",
    "title": "virtualenvwrapper",
    "section": "Temporary Virtual Environments",
    "text": "Temporary Virtual Environments\nSometimes you just want to try something out quickly in a clean Virtual Environment, if for example you are reviewing a Pull Request. virtualenvwrapper can help here as it has the mktmpenv. There are two options here -c|--cd or -n|--no-cd which changes directory post-activation or doesn’t respectively. The environment gets a unique name and will be deleted automatically when it is deactivated."
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html#drawbacks",
    "href": "posts/virtualenvwrapper/index.html#drawbacks",
    "title": "virtualenvwrapper",
    "section": "Drawbacks",
    "text": "Drawbacks\nThere are a couple of drawbacks I’ve found to using using virtualenvwrapper.\nThe first is that mkproject doesn’t allow nesting of project directories, you have to specify a single directory and it will be created under the $PROJECT_HOME directory with the associated environment name. This doesn’t work for me as I use the structure ~/work/git as the base but then have sub-directories based on the Git Forge (GitHub/GitLab/Codeberg) the repository is associated with and further nesting to reflect the user/organisation within as I have both a personal and work accounts. E.g. ~/work/git/hub/ns-rse/ns-rse.github.io which is the source for this site and associated with my work account (ns-rse) or ~/work/git/lab/nshephard/tcx2gpx which is a project of mine (tcx2gpx) hosted on GitLab. This means that if I wanted to create a project with mkproject based on $PROJECT_HOME being /work/git following this structure I would specify mkproject git/lab/new_project and whilst the directory is created, the virtual environment is created as git/lab/new_project which is truncated to git and you can’t workon git because the activation scripts are nested deeper under git/lab/new_project. Further each environment I created would then conflict. I could probably work around this by creating symbolic links but in practice I just use mkvirtualenv and setvirtualenvproject after I git clone work.\nThis is a problem specifically of my own creation though, something other users might find causes greater friction is that virtualenvwrapper doesn’t support creating and keeping the virtual environments within the project directory itself. This is never something that I’ve wanted to do myself though as I find it tidier to keep them all in one place and easier to find and remove obsolete environments."
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html#conclusion",
    "href": "posts/virtualenvwrapper/index.html#conclusion",
    "title": "virtualenvwrapper",
    "section": "Conclusion",
    "text": "Conclusion\nI’ve used virtualenvwrapper for years and its a nice light-weight alternative to other solutions of using Pythons Virtual Environments such as venv or Conda. It has some limitations but its worth giving it a whirl as there are lots of useful helper functions and hooks that smooth the process of creating, using and switching between virtual environments."
  },
  {
    "objectID": "posts/virtualenvwrapper/index.html#links",
    "href": "posts/virtualenvwrapper/index.html#links",
    "title": "virtualenvwrapper",
    "section": "Links",
    "text": "Links\n\nvirtualenvwrapper documentation"
  },
  {
    "objectID": "posts/pre-commit-r/index.html",
    "href": "posts/pre-commit-r/index.html",
    "title": "Pre-commit and R Packaging",
    "section": "",
    "text": "This post is aimed at getting you up and running with the R precommit Package. This shouldn’t be confused with the Python pre-commit Package although as you might suspect they are closely related.\nThe R package (precommit) consists of a number of R specific hooks that are run by pre-commit before commits are made and check various aspects of your code for compliance with certain style and coding standards (mostly aspects of R packages and I’ll be posting more on R packaging in due course).\nA major part of this post is about getting things setup on Windows, I’ve only given a light overview of some of the hooks and common problems encountered as I’ve gone about using and learning R packaging because not everyone uses Windows.\nMost of the work on the R package is by Lorenz Walthert, if you find it useful consider sponsoring his work, these things take a lot of time and effort and whilst they can be used for free are worth supporting."
  },
  {
    "objectID": "posts/pre-commit-r/index.html#pre-commit",
    "href": "posts/pre-commit-r/index.html#pre-commit",
    "title": "Pre-commit and R Packaging",
    "section": "pre-commit",
    "text": "pre-commit\nI love using pre-commit in my development pipelines and have blogged about it a few times already. It saves so much hassle (once you are used to it) not just for yourself but also your collaborators who are reviewing your Pull Requests. The R precommit package comes with a set of hooks that can be enabled and configured individually. I’ve recently and reason to start making and R package and as I’ve not used R much for a few years and this was my first time attempting to develop a package, I decided to use the hooks to impose the various style standards and checks that are expected.\nI opted to enable all of the hooks. I’ve not covered them all here in detail (yet) but describe some of them below and show how to use some additional hooks from the usethis package too.\n\ncodemetar\nThere is a hook for checking the Codemeta, which is in JSON-LD format is created correctly. The R package codemetar facilitates creating this and pulls metadata from the DESCRIPTION, README.Rmd and other aspects of your package to format them in JSON Codemeta. It comes with a handy function to write the file for you, so after installing you can just run codemetar::write_codemeta() which will create the codemeta.json for you. Remember to run this each and every time you update and of the files from which the metadata is created (although keep an eye on #491 which suggests updating automatically)\n\n\nroxygenize\nRoxygen2 is a package for making the documentation to go with your package, it does this by parsing the documentation strings (“docstrings” for short) that you adorn your functions with that describe the arguments and show example usages. This hook requires additional configuration in .pre-commit-config.yaml as you have to install your package dependencies. Fortunately there is a helper function in the precommit package so you can just run precommit::snippet_generate(\"additional-deps-roxygenize\") and it will output the YAML that you need to add to your .pre-commit-config.yaml. It might look something like the following.\n    hooks:\n    - id: no-debug-statement\n    - id: roxygenize\n      additional_dependencies:\n        -    data.table\n        -    dplyr\n        -    dtplyr\n        -    duckdb\n        -    IMD\n        -    lubridate\n        -    stringr\n\n\nstyle-files\nThe style-files hook runs the styler package against your code to ensure it follows the tidyverse style guide by default, although it can be configured to use a custom style guide of your own creation.\n\n\nlintr\nThe lintr package lints your code automatically. It can be configured by adding a .lintr configuration file to your repository, a simple example is shown below. Note the indented closing parenthesis is important you get a complaint about that and any other formatting issues.\nlinters: linters_with_defaults(\n         line_length_linter(120),\n         object_name_linter = NULL,\n         object_usage_linter = NULL\n  )\n\n\nspell-check\nThis is a useful hook that checks your spelling and adds unusual words to a custom dictionary inst/WORDLIST.\n\n\ndeps-in-desc\nThis hook ensures that all dependencies that are loaded by your package are listed in the DESCRIPTION file so that when the package is installed the necessary dependencies are also pulled in, fairly essential.."
  },
  {
    "objectID": "posts/pre-commit-r/index.html#usethis-package",
    "href": "posts/pre-commit-r/index.html#usethis-package",
    "title": "Pre-commit and R Packaging",
    "section": "usethis package",
    "text": "usethis package\nThe usethis package is a compliment to the devtools package that has a lot of very useful helper functions. Some of these enable additional pre-commit hooks whilst others enable GitHub actions, which are part of Continuous Integration pipelines and I would highly recommend enabling them.\n\nREADME.Rmd\nThe user_readme_rmd() function automatically generates a README.Rmd template and will also create a pre-commit hook that keeps it synchronised with README.md whenever you update it. This is useful because the later, plain-markdown, file is automatically rendered by GitHub/GitLab/Codeberg as your repositories front-page.\n\n\nuse_github_action()\nInvoking use_github_action() within your package repository will prompt you for the type of action you wish to add to it. There are, as of writing, three options.\n    &gt; use_github_action()\n    Which action do you want to add? (0 to exit)\n    (See &lt;https://github.com/r-lib/actions/tree/v2/examples&gt; for other options)\n\n    1: check-standard: Run `R CMD check` on Linux, macOS, and Windows\n    2: test-coverage: Compute test coverage and report to https://about.codecov.io\n    3: pr-commands: Add /document and /style commands for pull requests\nSelecting one will write a file to /.github/workflows/&lt;FILENAME&gt;.yaml and then print out code to add a badge to your repository.\nSelection: 1\n    ✔ Adding '*.html' to '.github/.gitignore'\n    ✔ Creating '.github/workflows/'\n    ✔ Saving 'r-lib/actions/examples/check-standard.yaml@v2' to '.github/workflows/R-CMD-check.yaml'\n    • Learn more at &lt;https://github.com/r-lib/actions/blob/v2/examples/README.md&gt;.\n    • Copy and paste the following lines into 'README.Rmd':\n      &lt;!-- badges: start --&gt;\n      [![R-CMD-check](https://github.com/CUREd-Plus/cuRed/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/CUREd-Plus/cuRed/actions/workflows/R-CMD-check.yaml)\n      &lt;!-- badges: end --&gt;\n      [Copied to clipboard]\n\n\nBadges\nMost of the GitHub Action functions described above include output that can be copy and pasted into README.Rmd to include badges in your GitHub front page. Again the usethis has you covered and can generate the necessary code for the different badges it supports."
  },
  {
    "objectID": "posts/pre-commit-r/index.html#gotchas",
    "href": "posts/pre-commit-r/index.html#gotchas",
    "title": "Pre-commit and R Packaging",
    "section": "Gotchas",
    "text": "Gotchas\nWhen starting out I found that I regularly didn’t pass the pre-commit hooks first time. This can be jarring and confusing to start with but its not something to worry about, they are there to ensure your code and package meet the standards required. If you ever come to submit to CRAN you will be grateful to have adhered to these standards.\nBelow I detail common “gotchas” I encountered when developing the package, what they mean and how to resolve them.\n\nThe following spelling errors were found:\nThe spell-check hook will fail if you’ve introduced new words that aren’t in standard dictionaries with messages similar to the those shown below. Sometimes these will be new words, sometimes they might be catching typos you have made. In the example below famiy should be family so you need to correct the source of the typo (and you’re told where this is, in this case it was line 27 of CITATION.cff), or if the new word should be added to the dictionary you will have to stage the updated inst/WORDLIST file for inclusion in your commit.\nspell-check..............................................................Failed\n- hook id: spell-check\n- exit code: 1\n- files were modified by this hook\n\nℹ Using R 4.3.1 (lockfile was generated with R 4.2.1)\nℹ Using R 4.3.1 (lockfile was generated with R 4.2.1)\nThe following spelling errors were found:\n  WORD    FOUND IN\nfamiy   CITATION.cff:27\nAll spelling errors found were copied to inst/WORDLIST assuming they were not spelling errors and will be ignored in the future. Please  review the above list and for each word that is an actual typo:\n - fix it in the source code.\n - remove it again manually from inst/WORDLIST to make sure it's not\n   ignored in the future.\n Then, try committing again.\nError: Spell check failed\nExecution halted\n\n\n! codemeta.json is out of date\nIf you modify the DESCRIPTION or CITATION.cff then the codemeta-description-updated hook will fail with error messages similar to the following.\ncodemeta-description-updated.............................................Failed\n- hook id: codemeta-description-updated\n- exit code: 1\n\nℹ Using R 4.3.1 (lockfile was generated with R 4.2.1)\nℹ Using R 4.3.1 (lockfile was generated with R 4.2.1)\nError:\n! codemeta.json is out of date; please re-run codemetar::write_codemeta().\nBacktrace:\n    ▆\n 1. └─rlang::abort(\"codemeta.json is out of date; please re-run codemetar::write_codemeta().\")\nExecution halted\nThis means yo need to update the codemeta.json with\ncodemetar::write_codemeta()\n\n\nWarning: Undocumented code objects:\nIf this error arises its because there is a .Rd file missing. You can generate these by ensuring you have the appropriate docstring definition prior to your function and then use the roxygen2::reoxygenise() function to generate the documentation automatically. Don’t forget to git stage and git commit the files to your repository, pushing if needed (e.g. a Continuous Integration pipeline is failing)."
  },
  {
    "objectID": "posts/pre-commit-r/index.html#windows",
    "href": "posts/pre-commit-r/index.html#windows",
    "title": "Pre-commit and R Packaging",
    "section": "Windows",
    "text": "Windows\nI haven’t used Windows for about 23 years but I often have colleagues who do and that was the case with the R package that I have started developing so I needed to get all members of the team up and running with the precommit R package/pipeline.\nWindows doesn’t come with Python by default, but pre-commit is written in Python and so an environment is required in order to run the above pre-commit hooks. There are many options for this, including using Windows Subsystem for Linux (WSL). I opted to try the solution provided in the precommit vignette. This shows how to use the reticulate package which acts as a glue between R and Python, to handle installing a Miniconda environment and setting up precommit/pre-commit.\nThe following runs you through the things you need to install (R, RStudio, GitBash), setting up GitHub with SSH keys and enabling precommit for your R package locally.\n\nInstall R\nWhen installing the defaults are fine, request admin permissions if required.\n\n\nInstall Rstudio\nDefaults are fine, request admin permissions if required.\n\n\nInstall GitBash\nDuring installation you’ll be asked a number of questions, if you’re unsure how to respond to any of them the following provides guidance.\n\nText Editor - Configure with your choice of editor, obviously you’ll want to have Emacs available and select that! 😉\nAdjust your PATH environment - At the bare minimum go with the Recommended option and allow Git from the command line and also from 3rd-party software. Optionally I would recommend the third option of Use Git and optional UNIX tools from the Command Prompt, particularly if you are either a) familiar with UNIX commands or b) not at all familiar with them (as you won’t have to re-learn the Windows commands, just learn the Bash commands they are more widely applicable).\nUse Bundled SSH library - Use the bundled SSH library.\nUse Bundled OpenSSL library - Use the bundled OpenSSL library.\nCheckout Windows-style, commit Unix-style line endings - This is fine, it just changes the internal representation of the carriage return to be more universal.\nUse MinTTY - The default terminal of MSYS2 is fine and more functional than the Windows’ default console window.\nDefault Merge behaviour - The default (fast-forward or merge) this is fine.\nChoose a credential helper - Select None here, we will let RStudio manage these.\nConfigure Extra Options - Defaults are fine.\nConfiguring experimental options - No need to enable any of these.\n\n\nConfigure Git\nStart a GitBash shell and configure your email address and name.\ngit config --global user.email \"you@example.com\"\ngit config --global user.name \"Your Name\"\n\n\n\nConfigure RStudio/GitHub with SSH keys\n\nStart RStudio\nCreate SSH key - Navigate to Tools &gt; General Options &gt; Git/SVN &gt; Create SSH Key and under SSH key type select the default (ED25519) this is a very secure elliptic curve algorithm and is supported by GitHub. Use a secure password (i.e. long), do not change the location it is created at.\nOnce created select View public key and use Ctrl + c to copy this to your clipboard.\nNavigate to GitHub and login then click on your avatar in the top right and select Settings &gt; SSH and GPG keys &gt; New SSH Key.\nGive the key a name and paste into the box below where indicated/instructed then click on Add SSH key.\n\n\n\nClone Repository\nIts likely that you will have an existing repository that you wish to work on with this pipeline, if so you will have to clone it locally so you can work on it with the precommit pipeline. The following assumes you have added your SSH key to your GitHub account as described above.\n\nNavigate to the repository you wish to clone (e.g. https://github.com/CUREd-Plus/cuRed/) and click on the Code button then select SSH under the Local tab in the box that appears.\nClick on the box that has two small squares to the right of some text to copy the URL to clipboard.\nReturn to RStudio and start a new project with File &gt; New Project &gt; Version Control &gt; Git and paste the URL into the Repository URL. Select a location to clone to under Create project as subdirectory of:, e.g. c:/Users/&lt;username&gt;/work/cuRed (replacing &lt;username&gt; with your username).\nIf prompted for password enter it. If asked to answer Yes\\/No answer Yes and then if prompted to Store password for this session answer Yes.\nYou should now have cloned the repository and have a project to work on.\n\n\n\nInstall pre-commit\nAs mentioned above pre-commit refers to two things, primarily it is the Python package pre-commit that does all the work of running Linting, Tests etc. before making commits. It also refers to an R package precommit (note the omission of the hyphen -) that works with the Python package to enable use of various R packages that carry out such checks. Because it is a Python package it needs a Python Virtual Environment to run. This may sound unfamiliar but don’t worry the R precommit package and documentation guides you through doing so, what follows is a rehash of the official documentation.\n\nInstall precommit and reticulate\nFrom RStudio install the remotes and reticulate package, then install the most recent version of precommit directly from GitHub.\ninstall.packages(c(\"remotes\", \"reticulate\"))\nremotes::install_github(\"lorenzwalthert/precommit\")\n\n\nInstall Miniconda environment\nYou can now use reticulate to install a Miniconda virtual environment framework for R to run Python packages (i.e. pre-commit).\noptions(timeout=600)\nreticulate::install_miniconda()\n\n\nInstall pre-commit framework\nThis step now installs the Python package pre-commit within a new Miniconda virtual environment (by default r-precommit). There will be a fair bit of output here as all the dependencies in Python for pre-commit are downloaded.\nprecommit::install_precommit()\nprecommit::autoupdate()\n\n\nUse precommit with the existing project\nYou should have cloned the repository you wish to enable precommit to use (see above). You now need to enable precommit for this local copy of the repository. This will place a script in .git/hooks/pre-commit that says which Miniconda environment to use (r-precommit) and will activate this whenever a commit is made, the install_hooks = TRUE ensures that the R specific hooks and their required environments are installed (under \\~/.config/pre-commit/).\nMake sure you have opened the .Rproj file in RStudio, this ensures you are within the project directory that you want to install precommit to (alternatively used setwd()).\nprecommit::use_precommit(install_hooks = TRUE)"
  },
  {
    "objectID": "posts/pre-commit-r/index.html#links",
    "href": "posts/pre-commit-r/index.html#links",
    "title": "Pre-commit and R Packaging",
    "section": "Links",
    "text": "Links\n\nR Packages book by Hadley Wickham and Jenny Bryan\nHappy Git and GitHub for the useR by Jenny Bryan\n\n\nR Packages\n\ndevtools\nusethis\nroxygen2\nR precommit\n(GitHub | lorenzwalthert/precommit check the Issues, can be useful for troubleshooting.\n\n\n\nPython\n\nMiniconda\n\n\n\nPre-commit\n\npre-commit\npre-commit.ci\nns-rse | pre-commit blog posts I’ve made about pre-commit."
  },
  {
    "objectID": "posts/cli-alternatives/index.html",
    "href": "posts/cli-alternatives/index.html",
    "title": "Linux Command Line Alternatives",
    "section": "",
    "text": "The command line is my second home when sat at a computer (Emacs is my first ;-) and the UNIX Philosophy is the key to the huge amount of highly productive tools that are available under UNIX, GNU/Linux, BSD, OSX, PowerShell etc.\nMany of these tools work and have done for many years, but there are some new alternatives that are coming through that build and modernise on these tools without breaking the core functionality. Here I detail some of the tools and why you might want to use them. Each tool has a brief introduction with some example output shown and then some aliases listed that you can drop into ~/.bash_aliases or ~/.oh-my-zsh/custom/aliases to use on your system."
  },
  {
    "objectID": "posts/cli-alternatives/index.html#alternatives",
    "href": "posts/cli-alternatives/index.html#alternatives",
    "title": "Linux Command Line Alternatives",
    "section": "Alternatives",
    "text": "Alternatives\n\nbat\nbat is “A cat(1) clone with wings.”. It automatically uses syntax highlighting and integrates with git if a file is version controlled to show changes and lots more. You can pipe input to it, including from e.g. curl -s https://server.com/some_file\n\nExample - bat\n❱ bat pyproject.toml\n───────┬──────────────────────────────────────────────\n       │ File: pyproject.toml\n───────┼──────────────────────────────────────────────\n   1   │ [build-system]\n   2   │ requires = [\n   3   │   \"setuptools\",\n   4   │   \"versioneer==0.26\",\n   5   │   \"wheel\"]\n   6   │ build-backend = \"setuptools.build_meta\"\n   7   │\n   8   │ [tool.black]\n   9   │ line-length = 120\n  10   │ target-version = ['py38', 'py39', 'py310']\n  11   │ include = '\\.pyi?$'\n───────┴──────────────────────────────────────────────\n\n\nConfiguration\nYou can generate a default configuration file with\nbat --generate-config-file\nThis will be saved at ~/.config/bat/config and you can edit it as desired.\n\n\n\ncheat\ncheat is actually a web-service that returns short “cheats” for command line programmes which will often cover many use cases and save you having to read the rather dry man pages for functions.\n\nExample - cheat\n❱ cheat cheat\n cheat:cheat\n# To see example usage of a program:\ncheat &lt;command&gt;\n\n# To edit a cheatsheet\ncheat -e &lt;command&gt;\n\n# To list available cheatsheets\ncheat -l\n\n# To search available cheatsheets\ncheat -s &lt;command&gt;\n\n# To get the current `cheat' version\ncheat -v\n\n tldr:cheat\n# cheat\n# Create and view interactive cheat sheets on the command-line.\n# More information: &lt;https://github.com/cheat/cheat&gt;.\n\n# Show example usage of a command:\ncheat command\n\n# Edit the cheat sheet for a command:\ncheat -e command\n\n# List the available cheat sheets:\ncheat -l\n\n# Search available the cheat sheets for a specified command name:\ncheat -s command\n\n# Get the current cheat version:\ncheat -v\n\n\nAliases - cheat\nYou don’t need to install anything to use this, instead define an alias for your shell (e.g. in ~/.bashrc/~/.zshrc/~/.oh-my-zsh/custom/aliases.zsh).\n## Linux commands https://github.com/chubin/cheat.sheets\ncheat () {\n    curl cheat.sh/\"$@\"\n}\n\n\n\ndifftastic\ndifftastic (GitHub) is an alternative to the default GNU diff packaged with most systems. It is “aware” of some 30 or so programming languages and will show diffs side-by-side rather than the traditional linear manner. It integrates easily with Git so when you git diff it uses difft to show the differences.\nHighly recommended, but don’t take my word for it, give it a whirl yourself.\n\n\nduf\nduf is a nice alternative to the traditional du and df commands which report disk usage and file/directory usage respectively.\n\nExamples\n❱ tldr duf\n\n  duf\n\n  Disk Usage/Free Utility.\n  More information: https://github.com/muesli/duf.\n\n  - List accessible devices:\n    duf\n\n  - List everything (such as pseudo, duplicate or inaccessible file systems):\n    duf --all\n\n  - Only show specified devices or mount points:\n    duf path/to/directory1 path/to/directory2 ...\n\n  - Sort the output by a specified criteria:\n  duf --sort size|used|avail|usage\n\n❱ duf\n╭──────────────────────────────────────────────────────────────────────────────────────────────╮\n│ 4 local devices                                                                              │\n├────────────┬────────┬───────┬────────┬───────────────────────────────┬──────┬────────────────┤\n│ MOUNTED ON │   SIZE │  USED │  AVAIL │              USE%             │ TYPE │ FILESYSTEM     │\n├────────────┼────────┼───────┼────────┼───────────────────────────────┼──────┼────────────────┤\n│ /          │  19.5G │  9.5G │   9.0G │ [#########...........]  48.9% │ ext4 │ /dev/mmcblk0p2 │\n│ /boot      │ 199.8M │ 38.8M │ 161.0M │ [###.................]  19.4% │ vfat │ /dev/mmcblk0p1 │\n│ /home      │   9.3G │  3.7G │   5.0G │ [########............]  40.4% │ ext4 │ /dev/mmcblk0p3 │\n│ /mnt/usb   │   4.5T │  3.2T │   1.1T │ [##############......]  71.3% │ ext4 │ /dev/sda1      │\n╰────────────┴────────┴───────┴────────┴───────────────────────────────┴──────┴────────────────╯\n╭───────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ 6 special devices                                                                                 │\n├────────────────┬────────┬────────┬────────┬───────────────────────────────┬──────────┬────────────┤\n│ MOUNTED ON     │   SIZE │   USED │  AVAIL │              USE%             │ TYPE     │ FILESYSTEM │\n├────────────────┼────────┼────────┼────────┼───────────────────────────────┼──────────┼────────────┤\n│ /dev           │   3.7G │     0B │   3.7G │                               │ devtmpfs │ dev        │\n│ /dev/shm       │   3.9G │     0B │   3.9G │                               │ tmpfs    │ tmpfs      │\n│ /run           │   3.9G │ 812.0K │   3.9G │ [....................]   0.0% │ tmpfs    │ run        │\n│ /run/user/1001 │ 789.3M │  20.0K │ 789.3M │ [....................]   0.0% │ tmpfs    │ tmpfs      │\n│ /run/user/966  │ 789.3M │  24.0K │ 789.2M │ [....................]   0.0% │ tmpfs    │ tmpfs      │\n│ /tmp           │   3.9G │   4.0K │   3.9G │ [....................]   0.0% │ tmpfs    │ tmpfs      │\n╰────────────────┴────────┴────────┴────────┴───────────────────────────────┴──────────┴────────────╯\n\n\n\nfd\nfd is an alternative to find that is easier to use. It is “opinionated” (i.e. decisions have been made about default options that you may not agree with) but purportedly covers ~80% of use cases. It works directly with regular expressions.\n\nExample - fd\n❱ tldr fd\n\n  fd\n\n  An alternative to `find`.\n  Aims to be faster and easier to use than `find`.\n  More information: https://github.com/sharkdp/fd.\n\n  - Recursively find files matching the given pattern in the current directory:\n    fd pattern\n\n  - Find files that begin with \"foo\":\n    fd '^foo'\n\n  - Find files with a specific extension:\n    fd --extension txt\n\n  - Find files in a specific directory:\n    fd pattern path/to/directory\n\n  - Include ignored and hidden files in the search:\n    fd --hidden --no-ignore pattern\n\n  - Execute a command on each search result returned:\n    fd pattern --exec command\n\n\n\njq\njq is to JSON (JavaScript Object Notation) what awk/grep/sed is to text files. It allows parsing, searching and selecting of JSON files, which if you’ve not encountered them before take a bit of getting used to.\n\nExample - jq\nDetails of using jq are really beyond the scope of this short article, like awk its almost a language in itself.\n❱ tldr jq\n\n  jq\n\n  A command-line JSON processor that uses a domain-specific language.\n  More information: https://stedolan.github.io/jq/manual/.\n\n  - Execute a specific expression (print a colored and formatted json):\n    cat path/to/file.json | jq '.'\n\n  - Execute a specific script:\n    cat path/to/file.json | jq --from-file path/to/script.jq\n\n  - Pass specific arguments:\n    cat path/to/file.json | jq --arg \"name1\" \"value1\" --arg \"name2\" \"value2\" ... '. + $ARGS.named'\n\n  - Print specific keys:\n    cat path/to/file.json | jq '.key1, .key2, ...'\n\n  - Print specific array items:\n    cat path/to/file.json | jq '.[index1], .[index2], ...'\n\n  - Print all array items/object keys:\n    cat path/to/file.json | jq '.[]'\n\n  - Add/remove specific keys:\n    cat path/to/file.json | jq '. +|- {\"key1\": \"value1\", \"key2\": \"value2\", ...}'\n\n\n\nlsd\nlsd is lsDeluxe and is very similar to exa but with a few additions such as icons.\n\nExample - lsd\n❱ l\n.rw-r--r-- neil neil  144 B  Sun Aug 14 19:56:53 2022  #.gitlab-ci.yml#\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:21:25 2022  .\ndrwxrwxr-x root users 4.0 KB Tue Aug 30 20:46:37 2022  ..\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:21:56 2022  .git\ndrwxr-xr-x neil neil  4.0 KB Sun Aug 14 21:51:03 2022  .github\n.rw-r--r-- neil neil  613 B  Sun Aug 14 21:44:38 2022  .gitignore\n.rw-r--r-- neil neil  151 B  Sun Aug 14 19:56:13 2022  .gitlab-ci.yml\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:21:25 2022  .quarto\n.rw-r--r-- neil neil  386 B  Thu Sep 15 22:05:23 2022  _quarto.yaml\n.rw-r--r-- neil neil  263 B  Sun Aug 14 10:59:13 2022  _quarto.yml~\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:05:24 2022  _site\n.rw-r--r-- neil neil  1.1 KB Thu Sep 15 22:05:23 2022  about.qmd\n.rw-r--r-- neil neil  455 B  Sun Aug 14 11:02:13 2022  about.qmd~\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:05:23 2022  img\n.rw-r--r-- neil neil  185 B  Sun Aug 14 22:22:04 2022  index.qmd\n.rw-r--r-- neil neil  191 B  Sun Aug 14 10:59:13 2022  index.qmd~\n.rw-r--r-- neil neil   34 KB Sun Aug 14 21:14:38 2022  LICENSE\n.rw-r--r-- neil neil  1.7 KB Thu Sep 15 22:05:23 2022  links.qmd\n.rw-r--r-- neil neil  237 B  Thu Sep 15 21:46:30 2022  links.qmd~\ndrwxr-xr-x neil neil  4.0 KB Wed Sep 14 20:24:25 2022  posts\n.rw-r--r-- neil neil  378 B  Thu Aug 25 23:20:16 2022  README.md\n.rw-r--r-- neil neil   13 B  Sun Aug 14 21:58:38 2022  requirements.txt\n.rw-r--r-- neil neil   17 B  Sun Aug 14 21:24:35 2022  styles.css\ndrwxr-xr-x neil neil  4.0 KB Thu Aug 25 23:20:16 2022  www\n\n\nAliases - lsd\nalias ls='lsd'\nalias l='ls -lha'\nalias lla='ls -la'\nalias lt='ls --tree'\n\n\n\ntldr\ntldr is very similar to cheat in that it shows short, simple examples of using a command. There are a number of different clients written in C, Node and Python as well as a few others. It depends on jq so you will have to install that if you want to use yq.\n\nExample - tldr\n❱ tldr tldr\n\n  tldr\n\n  Display simple help pages for command-line tools from the tldr-pages project.\n  More information: https://tldr.sh.\n\n  - Print the tldr page for a specific command (hint: this is how you got here!):\n    tldr command\n\n  - Print the tldr page for a specific subcommand:\n    tldr command-subcommand\n\n  - Print the tldr page for a command for a specific [p]latform:\n    tldr -p android|linux|osx|sunos|windows command\n\n  - [u]pdate the local cache of tldr pages:\n    tldr -u\n\n\n\nyq\nyq is to YAML (YAML Ain’t Markup Language) what jq is to JSON. Written in Python it allows fast and efficient parsing, searching and selecting of YAML files.\n\nExample - yq\n❱ tldr yq\n\n  yq\n\n  A lightweight and portable command-line YAML processor.\n  More information: https://mikefarah.gitbook.io/yq/.\n\n  - Output a YAML file, in pretty-print format (v4+):\n    yq eval path/to/file.yaml\n\n  - Output a YAML file, in pretty-print format (v3):\n    yq read path/to/file.yaml --colors\n\n  - Output the first element in a YAML file that contains only an array (v4+):\n    yq eval '.[0]' path/to/file.yaml\n\n  - Output the first element in a YAML file that contains only an array (v3):\n    yq read path/to/file.yaml '[0]'\n\n  - Set (or overwrite) a key to a value in a file (v4+):\n    yq eval '.key = \"value\"' --inplace path/to/file.yaml\n\n  - Set (or overwrite) a key to a value in a file (v3):\n    yq write --inplace path/to/file.yaml 'key' 'value'\n\n  - Merge two files and print to stdout (v4+):\n    yq eval-all 'select(filename == \"path/to/file1.yaml\") * select(filename == \"path/to/file2.yaml\")' path/to/file1.yaml path/to/file2.yaml\n\n  - Merge two files and print to stdout (v3):\n    yq merge path/to/file1.yaml path/to/file2.yaml --colors"
  },
  {
    "objectID": "posts/cli-alternatives/index.html#installation",
    "href": "posts/cli-alternatives/index.html#installation",
    "title": "Linux Command Line Alternatives",
    "section": "Installation",
    "text": "Installation\nMost of these programmes will be available in your systems package manager, if they are not you should consult the project page directly for install instructions.\n\nLinux\n# Gentoo\nemerge -av bat duf fd jq lsd tldr yq\n\n# Arch\npacman -Syu bat duf fd jq lsd tldr yq\n\n# Ubuntu\nsudo apt-install bat duf fd jq lsd tldr yq\n\n\nOSX\nbrew install bat duf fd jq lsd tldr yq\n\n\nWindows\nWARNING None of these have been tested I do not have access to a Windows system running PowerShell. They use Scoop a command-line installer for Windows.\nscoop install lsd"
  },
  {
    "objectID": "posts/cli-alternatives/index.html#links",
    "href": "posts/cli-alternatives/index.html#links",
    "title": "Linux Command Line Alternatives",
    "section": "Links",
    "text": "Links\n\nbat\ncheat\nduf\nexa\nfd\njq\nlsd\ntldr\nyq"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html",
    "href": "posts/pre-commit-ci/index.html",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "",
    "text": "NB If you’ve not read it already I would recommend reading my previous post on using pre-commit as the contents described herein assume that you are already using pre-commit in your development.\nHaving pre-commit setup locally to run before making commits is great. Typically code lives in a “forge” such as GitHub or GitLab and as pre-commit is run on each commit you shouldn’t have any problems when you come to git push your code to the remote origin repository (i.e. the repository hosted on GitHub/GitLab) as all pre-commit checks will have to have passed before this will take place.\nBut what if for some reason you disabled pre-commit just to make some changes rather than addressing the failed linting or test? Or if you work on an open-source project and someone else contributes how can you ensure that their contributed code meets the code-style chosen by the project and that all tests pass in light of the changes that are being introduced?"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#continuous-integration-continuous-delivery-cicd",
    "href": "posts/pre-commit-ci/index.html#continuous-integration-continuous-delivery-cicd",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "Continuous Integration / Continuous Delivery (CI/CD)",
    "text": "Continuous Integration / Continuous Delivery (CI/CD)\nThe solution to this is Continuous Integration/Continuous Delivery (CI/CD) which runs various hooks on GitHub/GitLab etc. in response to specific tasks/actions that occur on the repository. The exact name or system used depends on the forge, on GitHub these are GitHub Actions (see also Actions Marketplace) whilst on GitLab uses Pipelines. There are even standalone systems which integrate with both such as the popular Jenkins.\nBy employing pre-commit as part of your CI/CD pipeline you ensure code meets the standards (linting, tests etc.) you wish contributions to meet before it is merged into your main/master branch`\nThese work by running processes under certain conditions, for example on a push to the main branch or a tag that begins with v, and they might run processes such as running the test suite for your project to ensure all tests pass, build web-pages or build the package for deployment to a repository (e.g. PyPI). They are really useful and flexible systems and can be leveraged to run pre-commit on your code when Pull Requests (PR) are made to ensure the PR passes the various hooks. Ultimately a PR results in a commit to master/main and so its logically consistent that Pull Requests should pass pre-commit prior to being merged.\nUnder any system you could write your own hook to run pre-commit but there is an even easier and more efficient solution if you use GitHub in the form of pre-commit.ci."
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#github-and-pre-commit.ci",
    "href": "posts/pre-commit-ci/index.html#github-and-pre-commit.ci",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "GitHub and pre-commit.ci",
    "text": "GitHub and pre-commit.ci\nCurrently pre-commit.ci only supports GitHub although support of other systems is in the pipeline. pre-commit.ci doesn’t need any configuration beyond your already existing .pre-commit-config.yaml (see Pre-commit : Protecting Your Future Self). Where a pre-commit hook corrects formatting issues as is the case with some of the defaults such as trailing-whitespace or check-yaml, or if you are using Python linters such as black or ruff which fix errors, pre-commit.ci can commit these changes and push them back to the Pull Request automatically. In a similar vein it will also routinely update the rev used in your .pre-commit-config.yaml, commit the change and push it back to your repository.\nIt is also really fast because pre-commit.ci keeps the virtual environments that are used in tests cached whereas if you wrote your own action to run this the GitHub runner that is spun up to run GitHub Actions would have to download all of these each time the action is run is they are not persistent.\nUse of pre-commit.ci is free for open-source repositories and there are paid options for private or organisation repositories.\n\nBenefits of pre-commit.ci\n\nSupports GitHub but more to come in the future.\nZero configuration, just need .pre-commit-config.yaml.\nCorrects & commits formatting issues automatically without need for developer to reformat.\nAutomatically updates .pre-commit-config.yaml for you (e.g. new rev).\nFaster than your own GitHub Action.\nFree for open source repositories (paid for version for private/organisation repositories).\n\n\n\nConfiguration (.pre-commit-config.yaml)\nWhilst not required it is possible to configure the behaviour of pre-commit.ci by adding a ci: section to your .pre-commit-config.yaml. The fields are fairly self-explanatory as the example below shows. Its possible to toggle whether to autofix_prs and to set the autofix_commit_msg. The autoupdate_schedule can be set to weekly, monthly or quarterly along with a custom autoupdate_commit_msg. Finally you can optionally disable some hooks from being run only in pre-commit.ci.\nci:\n  autofix_prs: true\n  autofix_commit_msg: '[pre-commit.ci] Fixing issues with pre-commit'\n  autoupdate_schedule: weekly\n  autoupdate_commit_msg: '[pre-commit.ci] pre-commit automatically updated revs.'\n  skip: [pylint] # Optionally list ids of hooks to skip on CI\n\n\nSetup\nSetup is relatively straight-forward, head to https://pre-commit.ci and sign-in with your GitHub account and grant pre-commit.ci access to your account.\n\n\n\nPre-commit CI\n\n\nOnce you have granted access you can choose which repositories pre-commit.ci has access to. It is possible to grant access to all repositories but I would recommend doing so on a per-repository basis so you know and are in control of what is happening across your repositories. If you have administration rights to organisation repositories these should be listed in the “Select repositories” pull-down menu.\n\n\n\nGranting pre-commit.ci access to GitHub\n\n\n\n\npre-commit.ci jobs\nWhen logged into pre-commit.ci using your GitHub account you are presented with a page similar to the following which lists the accounts and any organisations that you have authorised pre-commit.ci to access.\n\n\n\nPre-commit.ci account access\n\n\nYou can follow the links through to view the history of jobs run by pre-commit.ci and whether they pass or fail. The page shows the current status and provides both Markdown and reStructured Text code for adding badges to your source documents (e.g. the Markdown badge can be added to your repositories top-level README.md and the badge will be displayed on GitHub)\n\n\n\nPre-commit.ci jobs pass\n\n\nYou can click through and see the results of a given run and when they pass they look similar to the output you would have seen when making commits locally.\n\n\n\nPre-commit.ci jobs pass\n\n\nBut sometimes things will fail as shown below where the trailing-whitespace hook failed and the file was modified. But since pre-commit.ci corrects and pushes such changes automatically you can see at the bottom that these changes were pushed to the Pull Request from which the originated.\n\n\n\nPre-commit.ci jobs fail"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#gitlab",
    "href": "posts/pre-commit-ci/index.html#gitlab",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "GitLab",
    "text": "GitLab\nAs pre-commit.ci doesn’t (yet) support GitLab integrating pre-commit into your GitLab Pipeline is a little more involved. What follows is based on the excellent post on StackOverflow describing how to achieve this integration.\nYou should already have a valid .pre-commit-config.yaml in place (if not work through Pre-commit : Protecting your future self (blog-post)). To enable pre-commit on your GitLab Pipeline you need to to have a pipeline in place. This is a file in the root of your repository called .gitlab-ci.yml. You need to add the following to this file…\nvariables:\n  # since we're not using merge request pipelines in this example,\n  # we will configure the pre-commit job to run on branch pipelines only.\n  # If you ARE using merge request pipelines, you can omit this section\n  PRE_COMMIT_DEDUPLICATE_MR_AND_BRANCH: false\n  PRE_COMMIT_AUTO_FIX_BRANCH_ONLY: true\n\ninclude:\n  - remote: 'https://gitlab.com/yesolutions/gitlab-ci-templates/raw/main/templates/pre-commit-autofix.yaml'\nThis uses the pre-commit-autofix.yaml from yesolutions to run pre-commit and as the configuration shows automatically apply fixes pre-commit makes to your code. There are more options available for configuring this pipeline and they are documented here.\nBecause you are allowing a third-party pipeline to access your repository when pushing the changes pre-commit makes back to your repository for this to work you must create a project access token. Under the repositories Settings &gt; Access Tokens you can create a new token with an expiry date. You must then create a CI/CD variable called PRE_COMMIT_ACCESS_TOKEN with this token as a value.\nOnce you have done this your CI/CD pipeline should show at the very start the .pre stage…\n\n\n\nGitLab pre-commit pipeline.\n\n\n…and you can click through on this to see the details of the pipeline. Note that it takes a while to run as it has to download and initialise all of the environments for each configured hook unlike pre-commit.ci (this is akin to writing your own GitHub Action to run pre-commit which would also have to download and initialise the environments).\n\n\n\nSuccess! GitLab pre-commit hooks pass!"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#summary",
    "href": "posts/pre-commit-ci/index.html#summary",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "Summary",
    "text": "Summary\nThis article has covered\n\nWhy to integrate pre-commit into your Continuous Integration/Delivery pipeline.\nWhat the pre-commit.ci service is and the benefits it provides.\nHow to integrate pre-commit.ci with GitHub repositories.\nHow to integrate pre-commit with GitLab repositories.\n\nBy automating linting and testing in this manner you improve and shorten the feedback loop for developers and contributors which frees up more time and focus on the code itself."
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#links",
    "href": "posts/pre-commit-ci/index.html#links",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "Links",
    "text": "Links\n\nPre-commit : Protecting your future self (blog-post)- pre-requisite reading if you are not already using pre-commit\nPre-commit : Protecting your future self (slides) - slides from a talk given at Research Computing at the University of Leeds that extended the above blog post to cover the material in this post (hit s to see the “speaker notes” 😉).\npre-commit\npre-commit.ci\nHow to use pre-commit to automatically correct commits and merge requests with GitLab CI - Stack Overflow"
  },
  {
    "objectID": "posts/pennine-way/index.html",
    "href": "posts/pennine-way/index.html",
    "title": "Pennine Way 2024",
    "section": "",
    "text": "This summer I decided to hike the iconic Pennine Way a 431 kilometre (261 miles) national trail stretching between Kirk Yetholm in the North and Edale in the South. Traditionally the route is hiked from South to North but as I live in Sheffield and have hiked around Kinder, Bleaklow and Blackhill I decided to hike from North to South as even if I didn’t complete the route I would get to see places I wasn’t familiar with and it was also cheaper to buy an advance rail ticket than have to purchase one on the day of return as I had no idea how long it would take.\nNB - In order to provide some relevance to the computing theme of this blog I include examples of how to plot GPX points in both R using the leaflet R package and python using the folium package make using the leaflet JavaScript framework very simple."
  },
  {
    "objectID": "posts/pennine-way/index.html#planning",
    "href": "posts/pennine-way/index.html#planning",
    "title": "Pennine Way 2024",
    "section": "Planning",
    "text": "Planning\nA number of years ago I attempted to hike the Pennine Way from Edale and having set off from Edale and reached Crowden in good time decided I could combine the first two days and continued to Standedge. This was a mistake as I wasn’t used to hiking with such weight and its a rough section up over Laddow Rocks. Half way there my knee didn’t like this and I hobbled the rest of the way and called my wife to come and collect me, heading home with my tail between my legs.\nI didn’t want to repeat this but I had no idea how long this would take me so booked two and a half weeks off of work allowing me 18 days to complete. I use OpenStreetMap and the excellent BRouter WebUI to plan days of upto 35km hiking although one day came in at 44km which I was wary of as I typically don’t enjoy hiking more than 30km without carrying significant weight so didn’t want to push myself too hard. But I figured I could be flexible and go slower if needed, and it wouldn’t be a problem if I was faster I could just drink more beer!\nA kit list of what I took is at the end along with details of what I used and didn’t, but I took everything I needed for camping along with clothes, a stove for boiling water, six dehydrated meals, maps, battery pack and a guide which I would have to read backwards as it covered the route from South to North over 20 days."
  },
  {
    "objectID": "posts/pennine-way/index.html#tracking",
    "href": "posts/pennine-way/index.html#tracking",
    "title": "Pennine Way 2024",
    "section": "Tracking",
    "text": "Tracking\nI use OpenTracks to privately record GPX tracks of my hiking, running and cycling as it stores them only on the device on which they are recorded, although you can of course copy them elsewhere and use them as you like. I did this each day I hiked recording almost complete traces for the whole trip. I had one short drop out and missed restarting after having a meal but after that often left it running whilst stopped for lunch or dinner sto avoid forgetting to restart.\nI uploaded each track to an instance of FitTrackee that I host myself which provided a neat way of reviewing the statistics for each days hiking which I have exported and saved to CSV and used in the tables and figures of this write-up.\nDetails and more information about these and other Open Source tools for self-research that protect the users privacy from the pernicious intrusions made by Big Tech can be found on the Personal Science Wiki."
  },
  {
    "objectID": "posts/pennine-way/index.html#pictures",
    "href": "posts/pennine-way/index.html#pictures",
    "title": "Pennine Way 2024",
    "section": "Pictures",
    "text": "Pictures\nI carried my phone, a Motorola GT100 and a Sony RX100 Mk IV with me along the route. I take a lot of pictures, most are crap but some are half decent and I’ve included in this post the better ones or those that accompany the story well. You can find the full set of pictures on my Pennine Way Flickr album."
  },
  {
    "objectID": "posts/pennine-way/index.html#using-gpx-tracks",
    "href": "posts/pennine-way/index.html#using-gpx-tracks",
    "title": "Pennine Way 2024",
    "section": "Using GPX Tracks",
    "text": "Using GPX Tracks\nI’ve opted to provide solutions to plotting GPX tracks in both R and Python since I use both in my day to day work. Each code chunk outputs a map and the code can be viewed if you want to learn how to create maps using either language.\nThis requires…\n\nFinding files with the .gpx extension.\nLoading each file GPX file.\nExtracting the GPS points (latitude and longitude), speed, elevation and date/time.\nPlotting the points on a Leaflet map.\n\nTo simplify this process I define functions to carry out the task (rather than copying and pasting the code for each day). Both R and Python have libraries for working with the Leaflet library. Currently I’ve got slightly further with the Python library than the R but I intend to update that and will write a separate blog post on mapping with each of the libraries. Things to do on the maps…\n\nConsistent zoom level on both.\nCorrectly colouring points by speed or elevation in R.\nStart and End markers.\nAdd pop-ups on hovering over any point to show the metrics (lat/lon, date/time, speed, elevation, accuracy).\nPlot the whole journey as one.\n\n\nRPython\n\n\nThe R library is named after the Javascript library, i.e. leaflet\n\n## https://rstudio.github.io/leaflet/\n## https://www.r-bloggers.com/2022/10/r-and-gpx-how-to-read-and-visualize-gpx-files-in-r/\n## https://mastodon.social/@yabellini@fosstodon.org/113181640784237866\nlibrary(htmlwidgets)\nlibrary(leaflet)\nlibrary(gpx)\n## List GPX files\ngpx_files &lt;- list.files(pattern = \"\\\\.gpx$\", recursive = TRUE)\n## Remove the 'pennine_way' gpx as we will concatenate the others\ngpx_files &lt;- head(gpx_files, -1)\n## Strip the common path and extract the date\ndates &lt;- stringr::str_split(gpx_files, \"/\", simplify = TRUE)[,2]\ndates &lt;- stringr::str_split(dates, \"_\", simplify = TRUE)[,1]\n\ngps &lt;- lapply(gpx_files, gpx::read_gpx)\nnames(gps) &lt;- dates\n#gps &lt;- lapply(gps, function(df) df[[1]]$tracks)\n\n#' Convert GPS points to data frame\n#'\n#' @params\n#'\n#' gps list GPS track read by gpx::read_gpx().\n#' columns list Column names.\n#'\n#' @returns Data Frame of GPS points.\ngps_to_df &lt;- function(gps, columns = c(\"elevation\", \"time\", \"lat\", \"lng\", \"speed\", \"accuracy\", \"segment_id\")) {\n    df &lt;- as.data.frame(gps$tracks[[1]])\n    colnames(df) &lt;- columns\n    df &lt;- df |&gt;\n          dplyr::mutate(speed = as.numeric(speed),\n                        elevation = as.numeric(elevation),\n                        accuracy = as.numeric(accuracy),\n                        lat = as.numeric(lat),\n                        lng = as.numeric(lng),\n                        accuracy = as.numeric(accuracy),\n                        time = lubridate::as_datetime(time),\n          )\n    df\n}\ngps_df &lt;- lapply(gps, gps_to_df)\nnames(gps_df) &lt;- dates\n\n#' Plot a gps dataframe\n#'\n#' @params\n#'\n#' gps_df list List of GPS Dataframes.\n#' date str Date of track to plot\n#' color str Variable to colour points as, default 'speed' but 'elevation' also possible\n#'\n#' @returns Leaflet map\nplot_day &lt;- function(gps_df, date, color = \"speed\") {\n    ## Palette not working...yet!\n    ## pal &lt;- leaflet::colorNumeric(palette = c(\"green\", \"red\"),\n    ##                             domain = gps_df[date][[color]])\n    leaflet::leaflet() |&gt;\n        leaflet::addTiles() |&gt;\n        leaflet::addPolylines(data = gps_df[date][[1]],\n                              lat = ~lat,\n                              lng = ~lng,\n                              ## color = ~pal(color)\n                              )\n}\n\n\n\nThe gpxplotter package provides wrappers around folium for plotting GPX traces, but I opted to do this manually so I could learn more about Folium (I used to use it in a previous job, just need to remember what I used to do!).\n\n# https://towardsdatascience.com/build-interactive-gps-activity-maps-from-gpx-files-using-folium-cf9eebba1fe7\nfrom pathlib import Path\nimport pandas as pd\nimport folium\nimport gpxpy\ngpx_files = sorted(Path(\".\").glob(\"**/*.gpx\"))\n\ndef gpx_to_df(filename: Path | str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load a GPX track and extract data.\n\n    Parameters\n    ----------\n    filename: Path | str\n        Path to a .gpx file.\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    gpx = gpxpy.parse(filename.open())\n    gpx_segments = list(gpx.tracks[0].segments)\n    data = []\n    for segment in gpx_segments:\n        for point_id, point in enumerate(segment.points):\n            data.append([point.latitude,\n                         point.longitude,\n                         point.elevation,\n                         point.time,\n                         segment.get_speed(point_id)\n                       ])\n    columns = [\"latitude\", \"longitude\", \"elevation\", \"time\", \"speed\"]\n    return pd.DataFrame(data, columns = columns)\n\ndef tidy_speed(df: pd.DataFrame, speed: str = \"speed\", threshold: float = 3.5) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove extreme speed values from a GPS dataframe.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        Pandas DataFrame of GPS points.\n    speed: str\n        Variable holding speed, default is 'speed' and shouldn't need changing.\n    threshold: float\n        Threshold for defining extreme speed\n\n    Returns\n    -------\n    pd.DataFrame\n        Pandas Dataframe with values of 'speed' &gt; 'threshold' set to 'threshold'.\n    \"\"\"\n    df.loc[df[speed] &gt; threshold, speed] = threshold\n    return df\n\ndef extract_lat_lon(gps_df: pd.DataFrame, lat: str = \"latitude\", lon: str = \"longitude\") -&gt; list[tuple]:\n    \"\"\"\n    Extract latitude and longitude from GPX data frame.\n\n    Parameters\n    ----------\n    gps_df: pd.DataFrame\n        Pandas Dataframe with latitude and longitude columns.\n    lat: str\n        Name of latitude column.\n    lon: str\n        Name of longitude column.\n\n    Returns\n    -------\n    list[tuple]\n        Returns a list of tuples each of which is the latitude and longitude.\n    \"\"\"\n    _array = gps_df[[lat, lon]].to_numpy()\n    return [tuple(point) for point in _array]\n\ndef plot_gpx_df(gps_df: list[tuple],\n                lat: str = \"latitude\",\n                lon: str = \"longitude\",\n                colors: str = \"speed\",\n                colormap: str = \"viridis\",\n                speed_threshold: float = 3.5,\n                default_tile: str = \"openstreetmap\") -&gt; folium.Map:\n    \"\"\"\n    Plot gps points on a map.\n\n    Parameters\n    ----------\n    gps_df: list[tuple]\n        A list of tuples of GPS points\n    lat: str\n        Name of latitude column.\n    lon: str\n        Name of longitude column.\n    colors: str\n        What to plot the colour of the line as, options are 'elevation' and 'speed' (default).\n    colormap: str\n        Colormap to use when plotting points.\n    speed_threshold: float\n        Speed threshold for defining and resetting extreme values, default is '3.5'.\n    default_tile: str\n        Default map tile to use\n    \"\"\"\n    if speed_threshold &gt; 0.0:\n        gps_df = tidy_speed(gps_df, threshold=speed_threshold)\n    map = folium.Map(location=[gps_df[lat].mean(),\n                               gps_df[lon].mean()],\n                     tiles = None)\n\n    folium.TileLayer(\"openstreetmap\", name = \"OpenStreetMap\").add_to(map)\n    folium.TileLayer(\"http://tile.stamen.com/terrain/{z}/{x}/{y}.jpg\",\n                     attr=\"terrain-bcg\",\n                     name=\"Terrain Map\").add_to(map)\n    # Add layer control (not working)\n    # folium.LayerControl(collapsed = True).add_to(map)\n    points = extract_lat_lon(gps_df, lat, lon)\n    folium.ColorLine(points, weight=4, colors=gps_df[colors]).add_to(map)\n    # Calculate bounding box (not working)\n    # south_west = gps_df[[lat, lon]].min().values.tolist()\n    # north_east = gps_df[[lat, lon]].min().values.tolist()\n    # map.fit_bounds([south_west, north_east])\n    return map\n\n## Get a list of all GPX points\ngpx = {Path(_file.stem).stem.split(\"_\")[0]: gpx_to_df(_file) for _file in gpx_files }"
  },
  {
    "objectID": "posts/pennine-way/index.html#day-by-day",
    "href": "posts/pennine-way/index.html#day-by-day",
    "title": "Pennine Way 2024",
    "section": "Day by Day",
    "text": "Day by Day\n\nDay 1 - Sheffield to Kirk Yetholm\nHaving packed over the previous few days I weighed the rucksack with 2 litres of water attached, 17.4kg.\nThis was a walking holiday so I waved goodbye to my wife and daughter around 07:30 feeling fresh and walked to the train station to catch the 08:22 train to Berwick-Upon-Tweed. Being Sheffield this involved a hill but it wasn’t too bad carrying the extra weight of the rucksack. I had poles but hadn’t put them to good use just yet.\nThe train journey was uneventful, but gave me time to sit and read a book on the Kobo I bought with me. I had bought the Kobo for hiking in the Julian Alps of Slovenia the previous year as I didn’t want to carry multiple books. Having recently started reading Johann Harri’s Stolen Focus : Why You Can’t Pay Attention I expected to get through it and had some fiction to read as well (Haruki Murakami’s IQ84). The countryside passed by as did the first couple of chapters and before long I was getting off at Berwick-Upon-Tweed.\nI’d checked the schedule and had a short wait before the bus to Kelso arrived. There was a gentleman trying to straighten one of two buckled wheels on his bike outside the station. Its quite hard to buckle wheels and I figured he’d therefore been in an accident recently so went and asked if he was ok. He said he was but that he’d been knocked off by a car turning across him. He was lucky and only had a few bruises but his bike was not ride-able and he was supposed to be cycling to Wales. He asked me to stand on one side of the wheel whilst he stood on the other to try and straighten it. Unsurprisingly this didn’t work. He then noticed some fence railings and tried levering it straight…unsuccessfully. Not only were both his wheels buckled but the built-in pannier rack that was part of the frame had sheered off so he would have trouble carrying his bags. He wasn’t hurt so I left him to his own devices and explored Castle Vale Park which led down to the River Tweed by the Royal Border Bridge and the remains of Berwick Castle.\n\n\n\nRoyal Border Bridge, Berwick Upon Tweed | Flickr\n\n\nA nice way to kill some time but I didn’t want to miss my bus so headed back through Castle Vale Park to wait at the bus stop. It arrived and set-off on time and I bounced through some nice farmland towards Kelso, spying The Cheviot hills in the distance which I would be hiking up in a few hours.\nDidn’t have masses of time to explore Kelso but grabbed a baguette and had a pint around the town square and purchased a bottle of Traquair Jacobite Ale (8% ABV) to drink before bed (with limited weight I wanted something strong!).\nAnother bus journey and I arrived in Kirk Yetholm. I’d clocked someone who looked like they were hiking and as we got of asked where he was heading. He said he was supposed to have been doing an Ultrar-marathon through the area but had injured himself a few days before the start so was taking active recovery by walking and bussing his way along the route. He kindly offered to snap a picture of me next to the Pennine Way map on the bus-stop, bid me farewell and headed off whilst I went for another pint in The Border, the pub that marks the “end” (or start) of the Pennine Way.\n\n\n\nThe Border Inn | Flickr\n\n\nHad a nice chat with a local who had lived in Sheffield in the past and was joined by what seemed the proprietor of the pub who said me I should sign the log-book behind the bar even though I was leaving. I duly did so to log the start date somewhere and considered purchasing a t-shirt but a) it was in the wrong direction and b) I hadn’t even started, let alone finished hiking at that point. One of them kindly offered to snap a picture of me in front of the pub’s display of the end of The Way.\n\n\n\nBeer at The Border | Having a pint at The Border Inn in Kirk Yetholm | Flickr\n\n\n\n\nDay 1 - 2024-08-14 Kirk Yetholm to Clennell Street\nBeer drunk I said farewell, shouldered my pack and started the GPS tracker, setting off up the lane in the heat of the afternoon. Not ideal conditions for hiking but I had a schedule to follow.\n\n\n\nAbove Kirk Yetholm\n\n\n\n\n\nFollow The Path\n\n\nThere are two options to this section of The Way, a high option going over various hills and low option for when weather is foul. I didn’t have that excuse and had been planning on stopping for the night at the Auchope Refuge Hut which was on the high route so up the hills I went. These undulated but the path was good and I made good time arriving at the hut in around three hours.\n\n\n\nCollege Burn\n\n\n\n\n\nAuchope Refuge\n\n\nWith a few more hours of daylight left I decided to push on and find somewhere else to wild camp as my legs weren’t tired and I had plenty of time, even though this involved a steep climb from Auchope Rigg up most of Cairn Hill. I decided in doing so not to do the dog-leg extension of The Way across the flat-ish moor to the summit of The Cheviot, the hill after which the area is named as it added an extra 4km to the journey and would have taken about an hour and I was now on the lookout for somewhere to stop.\nHeading downhill there weren’t many options forthcoming, lots of heather with a path through it but as things levelled out I came to a cross-roads with a path marked Clennell Street with a sign indicating that motor vehicles were not allowed between 1st April and 1st May. Quite how anything but motorbikes could get up there I’ve no idea, but there was a flat patch of grass so I donned a long-sleeve top to keep the midges away and setup the tent and bed for the night. That done I had my dinner of some ciabatta and pate along with the bottle of Traquair Jacobite I’d carried along and as the sunset I snuggled down for the first night under canvas feeling pleased to have gone a little further than planned and had no problems hiking up the hills with the weight of my pack.\n\n\n\nClennell Street | Flickr\n\n\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-14\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-14\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 2 - Clennell Street to Bryness\nWoke early around 05:00 as the tent was flapping in the wind and it was raining earlier than forecast. After listening to the rain for a while I got up at 05:30 and after eating some more pate and bread donned my waterproof jacket, shorts and sandals, packed the tent up and set off in the clouds for Windy Gyle which lived up to its name.\n\n\n\nWet Weather Walking Gear\n\n\nThere is a large pile of stones on the summit which is an ancient monument which should not be distubed, not much to look at because of the low cloud so I continued the descent, which roughly follows the bordere between Scotland and England, glad of my choice of sandals and shorts because it was very boggy in places.\nI came across Yearning Saddle Refuge Hut and took the opportunity to get out of the wind and rain and eat one of the blueberry snack bars I’d carried. A few pieces of food had been left for people who might need them along with lots of reminders to take your rubbish with you, which I had already done bringing my empty beer bottle with me.\n\n\n\nYearning Saddle Mountain Refuge\n\n\n\n\n\nYearning Saddle Lamb Hill\n\n\nNavigation was straight-forward as the trail was well marked with regular way markers and sign-posts although there was a split with an “alternative” route sign-posted. I opted to stick with the original which followed a “permissive by-way”.\n\n\n\nA brief lull in the rain\n\n\nPassed a couple of herds of feral goats and met a guy heading north who was on day 46 of hiking from Lands End to John O’Groats which was pretty impressive. He said that he’d broken a walking pole yesterday and would have to make a detour to replace it which reminded me that in The Border the previous day I had seen a rack for “Free walking poles” with a single pair hanging so I suggested he could grab try grabbing them from there since the Scotish National Trail, Scotlands equivalent of the Pennine Way, starts at The Border and I figured he’d be passing through. Unfortunately his planned route didn’t follow The Way to Kirk Yetholm but he was hopeful he’d be able to pick another pole up somewhere else.\nAs I got nearer Bryness I started to see more people, there was one guy off to have a meal in one of the mountain refuges. He had a long beard which made me wonder whether this was a good insulator against the rain or whether it helped water ingress into the jacket, not something I’d ever consider with regards to water-proofing. A family group of four heading north were next and then a couple who were finishing off their journey along The Way having left Edale 16 days previously. They were very friendly and reeled off many of the highlights I would pass, warning that it would likely be windy on Cross Fell.\n\n\n\n1990\n\n\nI arrived at the Forest View Inn where I had booked to camp for the night around 13:00 having covered the ~25 km in around six hours but they didn’t open for check-in until 16:00. Fortuantely the conservatory was accsessible and South facing so I was able to warm up and start drying my soaked clothes. My pack had remained dry thanks to the Deuter rain cover my wife had got me as a birthday present. I was both surprised and pleased with this as previous rain covers I have had were not very effective at all.\nJust before 16:00 a cat appeared at the back door to the conservatory and shortly after Oli appeared and checked me in (i.e. took my food order for the evening, I opted for Tortellini Pasta bake). Oli and Laura were a lovely welcoming couple who took over the Forest View Inn bed and breakfast, formerly a Youth Hostel, around four years ago having got fed up with desk jobs in Manchester. Oli had previously lived in Sheffield on Eccellsal Road and done some climbing in his time, but was more of a runner these days. He’d attempted both the North and Southern Spine Race Challenger twice and completed each once.\n\n\n\nForest View Inn\n\n\nI took the first of several beers that evening and pitched my tent in the back garden, spreading my wet things out over a picnic bench to dry in the sun, ignoring the fact there was a drying room which is great but fresh air is much more preferable.\nOther guests started to arrive, first was a couple of ladies who were reconnaissancing The Way from Alston to Kirk Yetholm as one of them, Nicola, would be attempting the Winter Spine Race for the second time in January 2025 having had to drop out (literally) when she collapsed outside of Alston due to infections in her feet where the skin had all detached due to a poor choice of overly tight waterproof socks. Her friend Rachel was no slouch on the racing front either as they had got to know each other whilst running the Dragons Back race which traverses the length of Wales taking in all the 3000 ft peaks. Rachel had not only completed the race, most drop out at some stage, but had done so after falling on the first day and fracturing her wrist and a rib. Tough and crazy women!\nOli had popped out to retrieve some guests who had stayed the previous night and were doing the remaining section of The Way over two days, returning to Forest View Inn in-between and returning to finish off the trek to Kirk Yetholm the following day. This included the family I’d met and a couple I’d passed and a guy from Sheffield who was on his own and finishing off the trail having started in Edale.\nDinner was communal and very sociable and revolved heavily around the ludicrious experiences Nicola and Rachel had running ultra-marathons. I had another beer after dinner but headed to bed early around 21:00 shortly after the sun had set but not before picking up a vegetable pasty and paying for my beer and food.\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-15\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-15\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 3 - Bryness to Hadrians Wall\nSlept well but woke early again around 05:00 despite there not being any wind or rain. I didn’t hang about too long, took another shower and was packed and off by 06:50. Ducked into St Francis’ Church and chatted to a guy who had spent the night in there. He was hiking around Kielder Forest stopping wherever he fancied.\n\n\n\nSt Francis’ Church, Bryness\n\n\nWishing him a good time I got back on track and The Way then followed good solid forestry trails through pine forests for about 15-16km before breaking out onto moorland which I reached in about three hours.\n\n\n\nForest Giants\n\n\nThe moorland wasn’t too bad, mainly as it was mostly paved so the few boggy bits weren’t a problem. I stepped over what I thought were some carnivorous plants but was in too much of a hurry to go back and photograph them to check which I regret now as I didn’t see any more.\n\n\n\nSturdy Post\n\n\nDidn’t pass many people until I was near Bellingham, this was to become something of a theme due to my early starts, but met a few people heading North along The Way. One guy complemented me on how fast I had been coming down the hill (the angle helped!) and a very jolly Welsh lady wanted to snap a picture with me to add to her collection she was posting to Farcebook. There was another solo hiker who had come all the way from Edale who looked fit and happy.\n\n\n\nComing or Going\n\n\nI reached Bellingham around 13:30 having covered 26km in just over five hours and opted for pizza and beer at The Cheviot Hotel which was delicious and grabbed some beers for later in the day before setting off on The Way again.\n\n\n\nLunchtime Beer in Bellingham\n\n\nI felt good and noticed I’d had a voice in my head telling me to “engage the core powerhouse”, “activate the glutes when stepping forward” and “keep the shoulders back and stomach in” which was Sam Webster, my pilates instructor common mantra during the classes I attend. This seemed like good advice as slouching and lolloping forward would only end in injury so I resolved to make sure I was walking up-right and fully engaging my body whilst walking for the rest of the hike. Listening to music for the first time since starting hiking I realised this also helped with the rhythm of walking as did focusing on my breathing (related to actively engaging core and body).\nThe hiking from Bellingham was fairly mundane, crossing agricultural land before entering another managed forest and following good tracks through these. Around 17:00 I started keeping an eye out for somewhere to camp, knowing that I wouldn’t make it to the next town I’d planned to stop at, Greenhead, as it was too far away. After leaving the forest The Way crossed some moorland and in a shallow valley and in the middle was a walled off area with a few trees. This looked ideal as the walls provided good shelter from the wind which was still hitting me head on from the South, but on reflection I decided not to stop here because if the wind dropped it would have been midge hell.\n\n\n\nGood Spot to Camp\n\n\nI continued until I reached Cragend by Greenlee Moss which gave a brilliant view across the lake as the sun was setting. There was a convenient stone wall which provided shelter from the wind and after clearing some cow and sheep shit out of the way I set myself up for the night and set about rehydrating one of the meals I had with me.\n\n\n\nStone Wall for shelter\n\n\nI’d not really given it much thought as they were small/far away but the fact I’d had to clear the area of cow shit to pitch my tent should have rung a bell in my head. It hadn’t, but the loud snort of an approaching bell did so I grabbed my pouch of rehydrating food and put the wall between the bull and myself. It had wandered up with a couple of calves who started sniffing around my stove which I’d left out and I gently shooed them away from the safety of the wall. Fortunately the bull was taking more of an interest in a female the other side of the fence. After a stand-off of about five minutes they all lost interest in me (or perhaps the lack of food which I’d eaten) and wandered off which was my opportunity to quickly take the tent down and move onto a safer spot, recalling the devestation cows had wrecked on some neighbours tents whilst I was in Peru 2012.\n\n\n\nCompany\n\n\nIt was starting to get dark as I headed over to Rapishaw Gap on Hadrians Wall and heading towards Milking gap I started scoping for a flat place to pitch my tent, regretting not going up the other side of Rapishaw Gap where there looked like a nice sheltered spot, but I didn’t have to go far until I found a spot and re-pitched my tent. As I was finishing off I was startled by a guy passing by who said it looked like a nice spot to camp. He’d been out running but had lost track of time watching the sunset but didn’t have too far to go to get back.\n\n\n\nA night on Hadrians Wall\n\n\nI was tired after around 50 km of hiking so crawled into the tent and fell asleep quickly.\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-16\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-16\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 4 - Hadrians Wall to Garigill\nWoke early and stuck my head out of the tent to a beautiful sunrise along the line of the wall.\n\n\n\nWorth Getting Out of Bed\n\n\n\n\n\nSunrise on Hadrians Wall\n\n\nDidn’t hang around too long, collapsed the tent and packed everything away fairly quickly to avoid getting in trouble for camping on (next to) an ancient monument. I don’t eat breakfast so off we went on the undulations of Hadrians Wall.\n\n\n\nHome for a week\n\n\nQuickly reached the famous Sycamore Gap passing a couple of climbers who were also camping/bivvying on the wall to make the walk-in to the crag extremely short.\n\n\n\nSycamore Gap\n\n\nBeing up and walking early is lovely, its quiet and there aren’t many people about so it was nice avoiding whatever crowds were to arrive later in the day. There were a few like minded people about, one lady commenting the exact same thing, another walking from Cawfields to Sycamore Gap who had waved from across a gap. The countryside is really quite a friendly place, especially for those out on their own.\n\n\n\nSteel Rigg\n\n\nToilets at Cawfields Lake were very welcome and there was also fresh water available which was useful for the coming day. Hadrians Wall petters out here and crosses some farmland where I met a couple of guys heading North on The Way and encouraged them onwards as they didn’t have too far to go compared to how far they had already come, which served to remind me how much further I still had to go!\n\n\n\nMilecastle, Cawfields, Hadrians Wall\n\n\nPassing through Walltown Quarry which has been beautifully transformed into a small nature reserve I stopped in the shop and grabbed a can of Sprite to drink to keep my energy up as I had decided not to detour into Greenhead in search of food (this turned out to be a mistake) and instead carried on passing Thirlwall Castle, although as I had a long way to go I only snapped a picture from outside and didn’t explore the grounds. As you cross the railway line shortly after Thirlwall View had a little honesty tuck shop for hikers of The Way to re-supply. I didn’t have any change so only took pictures rather than some food. Such tuck shops were to crop up a number of times along The Way and it was lovely to see how generous and helpful people could be towards strangers.\n\n\n\nJuniper Cottage, Greenhead\n\n\nI survived a game of Frogger on the A69 but was mildly alarmed by the sign on the gate which warned of biological hazards, although they only seemed to apply to those who undertaking work on telephone masts! The next few kilometers were fairly mundane going gently uphill through grazed fields and low lying moorland. After a couple of hours though and not having had any food I was starting to get tired, my feet were hurting and my pace slowed. It became hard work going up hill and for the first time in four days I wasn’t really enjoying myself. I stopped to cool my feet in a stream which helped a bit but then had more mundane terrain skirting the edge of arable land that led me up the valley.\n\n\n\nLive Shooting & Bio-Security Area\n\n\n\n\n\nCooling the feet\n\n\nI hoped there would be somewhere to get a snack in Slaggyford but alas not but as I was planning to get to Garigill for the night and had to pass through Alston so figured I’d be able to eat on the way through. Looking at the map I decided that I wasn’t going to do what appeared to be a fairly mundane loop out across a hillside and instead head along the road into Alston.\n\n\n\nSlaggyford Viaduct\n\n\nJust after joining the road the smell of chips frying wafted up the road from The Nook and I knew instantly where I would be eating. They had burger night on so I ordered a delicious chicken burger which came with a healthy portion of chipps and quaffed a couple of cans of lemon Fanta. Feeling tired I asked staff if there was anywhere to camp in Alston, they didn’t think so but asked another customer who was local who confirmed this but advised to “just camp on the village green, no one will care and lots of people do it”. I thanked the staff and locals for advice and taking their advice shouldered the pack and crossed the adjacent field to reach a track that ran along the South Tynedale Railway which was a nice compact and flat trail into Alston and took me from Northumberland into Cumberland.\n\n\n\nCumberland | Northumberland\n\n\nFeeling invigorated by the food I stopped in the Alston House Hotel for a pint and after grabbing some supplies (viz. beer, sweets and lunch for the next day) set off up hill to my original target for the day, Garigill. I’d read about a diversion because the Dryburn Bridge was closed due to flood damage which took me a long an alternative trail on the north side of the river which didn’t look as though it had seen much traffic and involved hacking through shoulder height ferns in the fading light.\nAs I entered Garigill I met a lady out walking her dog who asked if I was intending on camping behind the village hall which I was. She advised me I might want to look elsewhere as there was a wedding reception in the hall that evening and suggested a spot down by the river which would be quieter. Again I was surprised at how welcoming and helpful people were to those trekking The Way and thanked her for her advice. I was too tired to go looking for alternatives though so when I got to the village hall spoke to a couple of guys outside having cigarettes saying I had been hoping to camp out the back that evening. They said I should just go through and pitch up and that I could even join the party and have some curry which was very generous but I was satiated and tired and just wanted to go to bed so I went through and pitched my tent in the far corner under a tree, had a beer and showed some curious children from the wedding what my tent was like then went to bed. A short while later I was woken though as some of the kids decided to use my tent as a target for throwing plastic bottles and cans at. I didn’t bother rising to the bate and stayed quiet in my tent. The music stopped around 23:00 and I heard some arguing (alcohol and families often result in high emotions) but at least the shower of plastic bottles and cans had stopped and I soon drifted off to sleep.\n\n\n\nWedding Reception in Garigill\n\n\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-17\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-17\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 5 - Garigill to High Cup Nick\nSlept until about 06:30 as I had a relatively short day planned going up and over Cross Fell, down into Dufton and on to High Cup Nick. I collected the rubbish that had been thrown at my tent and put it in the recycling and chatted to a lady who was tidying up after last nights reception. Turns out it was the bride who hadn’t been able to sleep knowing that she had to tidy up so she’d got up early to get started with the job.\n\n\n\nGarigill\n\n\nI had a shower, after clearing up what broken glass from the night before that I could see on the floor and managed to avoid cutting my feet which would have been bad for hiking. Dropped some cash in the honesty box (aka letter box) packed up and set off heading up Cross Fell. Poked my head inside the church and resisted the temptation to ring the bell.\n\n\n\nYou Can Ring my Bell\n\n\n\n\n\nDufton 15 miles\n\n\nThe path was a good compact track, no doubt for the grouse shooting. After winding my way up the hill side the beautiful Gregs Hut came into view and I poked my head in to have a look around before continuing. Will definitely have to come back and spend at least one night here in the future.\n\n\n\nGreg’s Hut\n\n\nA short steep section led to the plateau of Cross Fell with its excellent stone shelter from the wind and another trig point. There was a party of three with a couple of dogs who had hiked up from Dufton for the day when I arrived and we chatted briefly. I had a beer before snapping a panorama and heading off and over Great Dun Fell with its listening station on the summit like a golf ball poised to be driven south over the A66.\n\n\n\nFrom Cross Fell\n\n\n\n\n\nCross Fell Summit\n\n\n\n\n\nCross Fell Panorama\n\n\n\n\n\nListening for…\n\n\n\n\n\nUphill Slog\n\n\nThe path then headed across another fell before down into Dufton. I met a guy who was heading North on The Way with a very large rucksack and an umbrella dangling from the bottom. I’d considered taking an umbrella myself to keep the worst of heavy rain off the top of me but decided against it as most rain in the UK comes in sideways.\n\n\n\nFloating Flag Stones\n\n\n\n\n\nDown to Dufton\n\n\nI had been planning on eating at the The Stag Inn. Arriving just after 16:00 I had to wait until 17:00 until food was served and was told I couldn’t look at the menu in advance as there was a party of eight booked in and they were taking priority so I would have to order after them. In the end the party (two families) turned up late and by 17:15 hadn’t decided what they wanted to eat so I was allowed to order ahead of them (as had a number of other people who were eating but hadn’t been waiting as long as I had). Not the friendliest of service I came across but the food was ok.\n\n\n\nThe Stagg Inn, Dufton\n\n\nIt was getting late and in my rush to set off and get to High Cup Nick I left without my walking poles. I didn’t get far before realising this so dashed back to the pub to grab them and started again at a decent pace to climb back up the hill, passing a pair who had walked from Middleton in Teesdale that day heading North along The Way. The 6km and ~300m of ascent (most of which are in the first 2-3km) were accomplished quickly (just over an hour) and I found myself coming onto the rim of the amazing High Cup Nick, a valley with a dolerite rim with limestone above and below the layer of dolerite. As I neared the head I kept my eyes open for a flat spot to camp, not wanting to pitch up at the very top where it was boggier and there would be more midges if the wind dropped (needn’t have worried, the wind barely stopped the whole trip!).\nI snapped a few pictures in the fading light then setup my tent for the night and went to sleep almost instantly.\n\n\n\nHigh Cup Nick(Panoramic)\n\n\n\n\n\nWild Camping above High Cup Nick\n\n\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-18\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-18\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 6 - High Cup Nick to Middleton in Teesdale\nWet, misty and windy this morning, I’d made the right decision heading up the previous night to see High Cup Nick in all its glory as I couldn’t see much of it this morning. Packed the tent up and was off quickly, looking forward to a short day of 28km almost exclusively downhill along the edge of the Maize Beck then the River Tees to Middleton in Teeside.\n\n\n\nWrapped Up\n\n\n\n\n\nMaize Beck\n\n\nCrossed a bridge that had been erected in memory of Ken Wilson and followed a good compact track downhill passing through Birksdale Farm and shortly after coming on to the fantastic view of the confluence of Maize Beack and River Tees. The trail swings by underneath the dam of Cow Green Reservoir and down the side of Cauldron Snout which was in full flow and very impressive.\n\n\n\nMaize Beck and Cauldron Snout\n\n\n\n\n\nCauldron Snout\n\n\n\n\n\nRiver Tees\n\n\nThe Way then follows the River Tees all the way to Middleton on Tees side. Heading down this section I met a lady heading North along The Way, the first solo female hiker I’d met. She had set off early and was keen to reach Dufton before the forecast rain passed through, just as I was keen to reach Middleton.\nAfter passing through Cronkley farm I came across what seemed like a Juniper forest with lots of small stout evergreens that looked to me like Juniper, something I’ve encountered on cliff faces over the years climbing but never seen a forest of. Force Garth Quarry was passed and very soon I arrived above High Force Waterfall which was quite spectacular. Snapped a few shots from above then went round to the viewing area from the South to get some shots, waiting to get into position as there were a lot of people about, including a school/youth group.\n\n\n\nTop of High Force\n\n\n\n\n\nGoing Down\n\n\n\n\n\nHigh Force\n\n\nPlodding along down hill the sound of the water bubbling over waterfalls was too tempting and I opted to cool my feet off in the cold water. It was delightful and I wish I had done it earlier. If you can stand it for long enough it rejunivates the feet, taking down swelling (the longer you stay in the more swelling goes down) and is just a delightful contrast to pounding the floor.\n\n\n\nVeins in the Rock\n\n\n\n\n\nJump!\n\n\nLow Force came very soon after with the Wynch Bridge which strongly advised not to have more than one person on it at any given time which I and others duly respected.\n\n\n\nLow Force\n\n\nA little further along the trail I met a couple out for a short walk who asked if I was walking the Pennine Way (the big rucksack and poles are a bit of a giveaway) we chatted and it transpired they were from Sheffield and occasionally drank in The Brothers Arms had just passed a pair who were also hiking from North to South who were also from Sheffield and who one of them knew. This was a first! I’d almost caught up some other Wayfarers heading South! I said farewell and as my feet were sore though so I decided to cool them off in the River Tees. Refreshed I resolved to catch up the pair from Sheffield and a short while later I came across two people with rucksacks who had stopped for a short break. It was the two Wayfarers who were heading South, I said I was heading the same way too as I lived in Sheffield so it was like walking home (knowing full well this was likely to be the pair from Sheffield). They were indeed from Sheffield and we came round to where abouts we lived, I said Meersbrook and one of them asked if I drank in The Brothers Arms which I do and he then asked where I lived. Transpires one of them, Martin who was 70, lives at number 59 on the same road as me! His friend had walked the Pennine Way many years ago when he was 17 and Martin had tried a year after lockdown but had aborted so they had joined forces to complete the way again. We posed for a picture and resolved to catch up when we were both back in Sheffield.\nIt was only a short distance to Middleton on Teesdale where I noticed Ozzys Pizza on the way into town but headed straight to the pub for a couple of mid-afternoon pints in The Forresters and wrote up some over-due notes in my diary and charged by phone and camera. A guy, Dave, playing darts and pools asked if I needed somewhere to camp for the night and said it would be fine to pitch up in the yard of his portaloo company. I thanked him for the offer but politely declined saying I wanted to have a shower, very friendly pub and people. Suitably refreshed I swung by the Co-Op for some beers and sweets and headed back out of town to Daleview where I met Andy and Alex who were heading North on The Way. Andy knew lots about The Pennine Way as he volunteered to work on The Spine Race and mentioned the Middleton Tandoori which piqued my interest as I love curry. Unfortunately this was only a transient restaurant that was setup to feed participants and volunteers. Andy proudly told me he’d eaten 12 portions of Chicken Korma one year but seemed put out that this coming year they had moved the station and said that he would not be able to indulge to quite such an extent. Alex was closer to my age, probably a bit younger and we agreed to head into town to grab a pizza for dinner that evening from Ozzy’s which was delicious (we invited Andy but he passed, favouring his own food).\nAlex was a software engineer with a degree in computer science but was also learning about horticulture, initially to grow produce but more recently had got interested in ornamental plants. We popped back into town for pizza at Ozzy’s which was delicious and then along to The Forresters afterwards for another pint and to charge devices before heading back to the campsite for the night.\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-19\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-19\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 7 - Middleton in Teesdale to Thwaite\nBig day as I was hoping to get to Hawes in one big push stopping for lunch at the Tan Hill Inn, England’s highest pub. It was unfortunately drizzling when I woke up early but I forced myself out of the tent and packed it away, noticing that Andy had the foresight to bring a cloth to wipe his tent down and get most of the moisture off of it\nI’d made an organisational error and didn’t get snacks (i.e. sweets to chomp on whilst hiking) from the Co-op the previous night after pizza and beers with Alex so after packing everything up I hiked into town to grab some more Haribo Starmix before setting off on The Way in light drizzle. Gentle arable land rolled past with a couple of tuck shops passed on the way, one of which had Irn-Bru cans for sale which I took advantage of for an energy boost later on. Grassholme Reservoir was quickly passed.\n\n\n\nTuck Shop\n\n\n\n\n\nShell of a House\n\n\nComing over Beck Head there was a barn owl circling around the field, it seemed to be struggling to gain height and escape in the strong wind that was blowing as it circled the area multiple times. I thought I glimpsed someone walking ahead of me in the same direction with a red bag but lost sight of them quickly even though I should have had a clear view of them as I dropped down into the valley with Blackton Reservoir as the path went up the hillside opposite. I soon caught up with them though as they had stopped for a break just past Clove Lodge. It was a lady called Jolyene from Ireland who was a locum vet in Cornwall who was doing The Way in sections with her dog (who’s name I forget). We fell into step and talked for a while, losing the path as we headed up Peatbrig Hill. She was looking to redress the balance in her life and spend more time doing the things she enjoyed, running (when the body allowed), travelling with friends and family and her main passion surfing. Around Race Yate she stopped to give her dog some water and said she couldn’t keep pace.\nI carried on dropping down to cross the A66 which was a little weird as its somewhere I drive along when heading to Keswick. It was quite noisy due to the traffic, a complete contrast to the quiet hills and dales I’d been walking over the last week and even the villages I passed through. Just south of the A66 was a small farm and a sign for “God’s Bridge” a natural limestone bridge over the River Greta. There wasn’t much water flowing but it was very cool. If I hadn’t been in such a rush I might have investigated more, but I still had a long way to go to get to Hawes.\n\n\n\nA66 Underpass\n\n\n\n\n\nGod’s Bridge\n\n\nFollowing the track up from River Greta I met a father and son who were on day six or seven heading North on The Way. We had a brief chat and they asked how I was getting on in sandals and shorts which I’d opted for that day as it had been raining when I set off and I knew I had the bog that leads up to Tan Hill to contend with. I was quite happy walking in sandals and felt they worked well.\nThe Way then followed a good shooting track for a few kilometers up the slope towards Tan Hill but eventually petered out. As I left the trail I met two older ladies who were also following The Way North but had taken the sensible option of having their kit sherpa’d ahead each day so they had the luxury of walking with lighter day sacks. Not a bad idea at all I thought as I checked how heavy my pack felt.\n\n\n\nShooting Track\n\n\n\n\n\nTan Hill\n\n\nThere were a series of good wooden sign-posts with white painted tops that marked the way through the bog to Tan Hill summit and the highest pub in England. Near the top I saw a couple in the middle of the moor and bog disagreeing about which way to go as they tried to get back on the path. As I met them it was clear they weren’t british and I advised them to follow the marker posts and ignore the smaller plastic ones which I think they had mistakenly started following and had taken them off-track as these were markers for grouse feeders.\n\n\n\nCaution Deep Hole\n\n\nOn arrival at the Tan Hill Inn I popped in to ask if it would be ok to pitch my tent so it could dry in the wind and sun whilst I had lunch and duly did so before settling down to an expensive pint and scampi and chips which was good but probably a bit over-priced too in my opinion.\n\n\n\nDrying the Tent\n\n\nNo time for a second pint (the queue at the bar precluded this) so I packed up the now dry tent and set off on a good track towards East Stronsdale, gradually losing height. Upper East Gill Force was passed and The Way then headed up an interestingly paved path up Kisdon hill before skirting around over jumbled limestone blocks.\n\n\n\nUpper East Gill Force\n\n\n\n\n\nOld Mining Tracks\n\n\nTraversing Kisdon Hill gave some stunning views into Swaledale, unfortunately at this point a heavy shower passed over but as it was late in the day I had the delight of a rainbow over Black Hill.\n\n\n\nRainbow over Black Hill\n\n\n\n\n\nAbove Muker\n\n\nDropping down into Thwaite there were more black clouds looming over Dam Hill and Great Shunner Fell so I decided to not push on to Hawes that evening and instead stop in the campsite I’d seen in the valley. Not before having a beer though so I popped into an empty Kearton House only to be told that they weren’t open and the only reason the door was unlocked was because the solicitors had just visited. I was advsed I might be able to get a beer at Usha Campsite where I was heading as they had a small shop so I walked the kilometre or so down the road only to find I was too late in the day as the shop, and reception in general, had closed an hour and a half earlier.\nPitched the tent in a field in the lee of a wall and large tree after carefully looking for dead branches that might come down in high wind and deciding it was relatively unlikely to happen.\n\n\n\nUsha Gap Campsite\n\n\nA quick shower then boiled some water to make dinner. There were a number of Duke of Edinbrough participants who were hiking around the area and I chatted to a few of the as I ate my rehydrated meal in the warmth and comfort of the facilities. They were bemoaning having to hike 20km or so, but to be fair they were having to carry all of their food, unlike mysel who was making good use of all of the eateries I was passing.\nPretty tired so had an early night, but not before a few more showers passed through which helped me feel vindicated in my decision not to push on to Hawes that evening.\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-20\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-20\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 8 - Thwaite to Horton in Ribblesdale\nUp early (there is a theme here!) and packing the tent away I left before most of the Duke of Edinbrough hikers were up. Reception wasn’t open so I went to post my £10 through the letter box just as a lady was sorting reception for the day. She was very grateful and wished me luck on my days hiking.\n\n\n\nSunrise from Usha Gap Campsite\n\n\nI found a path along Muker Beck back to Thwaite which was a lot more pleasant the following the road I’d come down the previous evening and had soon passed through Thwaite and started heading up Dam Hill on a good compact farm track. This turned to flagstones as the path joined the open moorland and I had a good target to reach of a prominent cairn.\n\n\n\nSunlit Swaledale\n\n\nI find setting myself short targets of reaching feature X or the other side of the valley in a certain number of minutes a good motivational way to keep moving quickly. Within no time I found I’d reached the cairn and paused to snap some pictures looking back down Swaledale before continuing over Great Shunner Fell.\n\n\n\nFrom Dam Hill Cairn\n\n\n\n\n\nSwaledale from Great Shunner Fell\n\n\nThere were great views to the North West but some ominous grey clouds were looming on the horizon so I didn’t hang around too long admiring the view, instead dropping down quickly to reach Hawes for lunch. This section was longer than I expected and only served to reassure me I’d made the right decision the previous night not to tackle it in the fading light.\n\n\n\nLots of Moorland\n\n\nPassing through Hardraw I was tempted to duck into The Green Dragon for a pint but mindful of the grey skies I kept on going to Hawes instead where there were more options for lunch. In hindsight I wish I had stopped as it is home to England’s highest waterfall, Hardraw Force which I didn’t know about, although you have to pay to access it which would likely have put me off.\nA shower passed as I made my way to Hawes which is a lovely village in Wensleydale. None of the pubs opened until mid-day so instead I opted for lunch in The Wensleydale Pantry which turned out to be an excellent choice as I had a delicious Thai vegetable Penang curry.\n\n\n\nHawes\n\n\n\n\n\nSpoilt for Choice\n\n\nReplenished I grabbed some sweets and a can of beer and set off out the west of the village pausing outside of St Margaret’s Church to put on waterproof trousers as it had started to drizzle.\n\n\n\nSt Margaret, Hawes\n\n\nThis was just a passing shower and I soon stopped to remove the trousers as I was sweating. Onwards and upwards passing near the summit of Ten End (584m) I joined the Cam Road with excellent views over the valley to the west. By now it had started drizzling again but the wall by the side of Cam Road provided some shelter (the rain was coming in from the side rather than above) and I didn’t think much of it as I passed by Dodd Fell Hill (668m). As I started dropping down into Ribblesdale though I realised I was quite wet now and the rain showed no signs of letting up. There was no point in putting waterproofs on now though so I plodded on downhill.\n\n\n\nI’m going uphill again\n\n\nThere was a random caravan just off of Cam High Road and as I continued a cyclist passed me coming up the road. We said hello but he looked like he was in a hurry and carried on. I noticed his cycling shorts had a big hole in them on the right thigh suggesting he’d come off earlier in the day. I left Cam High Road following Cam Road, crossing Cam Beck and Calf Holes, skirting farmland.\n\n\n\nHere comes the rain\n\n\nI was thoroughly soaked by now and finding it pretty miserable for the second time of the hike and resolved to find somewhere dry to stay for the night in Horton in Ribblesdale. Eventually I arrived in town and immediately sort shelter in The Crown where there were a couple of guys who had just completed the Yorkshire Three Peaks and were waiting for the friends to join them. After finishing my pint I set about trying to find a B&B to stay in and there appeared to be one on the far side of town so I trekked over in the rain. There weren’t many cars outside which made me hopeful they’d have space but after knocking on the door (twice) a lady answered and said they didn’t have a room that evening. I asked if she could recommend anywhere and was advised to try Settle which I could get to by train so I walked back to the train station which was closed but found a small area out of the rain to check what was available online. The cheapest place was around £100-120 which was way outside my budget and didn’t include the cost of the train, nor the hassle of being so far off route the next morning so I resigned myself to a wet night in the tent and wandered back towards The Crown checking out the National Trust toilet block and grass as a possible place to pitch before returning to the pub and ordering a bowl of chips and another pint to console myself. I then did what I should have done in the first instance which was ask the barman if there was anywhere to stay in town and he advised me that there was another pub The Golden Lion Hotel just past the campsite (which I hadn’t actually located yet!) and that they had a bunkhouse and rooms but that they often shut early if they were quiet. My chips arrived and I couldn’t finish them quick enough before shouldering my pack and heading down the road to check out this lifeline. A Ukrainian woman was behind the bar but had only been working there five days so wasn’t sure about how the bunkhouse worked but they had space. With the assistance of what I presume was the landlord I was checked in and given the code to access the room. No one else was there so I draped my wet clothes and kit out over the beds in the hope they would dry a little overnight (there wasn’t any real heating and no dry room as far as I could tell).\nAfter a quick shower I went back down to the bar where I got chatting to Roger who was heading North on The Way and had only found the pub and bunkhouse after he had pitched his tent next door at the campsite. He didn’t seem too phased by the wet weather and told me he’d walked The Pennine Way many years ago when he was 17, but that he couldn’t remember any of it. Whilst we were chatting a guy came into the bar decked out in cycling gear looking to stay in the bunkhouse, I thought he was cycle touring and, like me, needed a place to shelter for the night. Having just been checked in I helped show him where it was and entered the code to get in even though he had been chatting to the bar staff in what sounded like her native language.\nTime was called at the bar relatively early, around 21:30 so I said farewell to Roger and wished him luck on the rest of his hike and retired to the bunkhouse where I got to know Daniil Sadomskij who wasn’t just cycle touring, he was competing in the GBDuro 2024 and was on his fifth day of cycling from Lands End to Cape Wrath via Wales, The Pennine Bridleway. It sounded brutal and would explain the cyclists I passed earlier in the day. Dani was currently placed fourth but it wasn’t about winning for him as his main motivation was to see the country as he’d been living in the UK for eight years working as a chef in a London restaurant. He had an excellent pair of Lidl shorts and was eating some cold, congealed chips he’d picked up earlier in the day. This didn’t look very appetising so I offered him one of my dehydrated meals which he gratefully accepted and after boiling the kettle and a short wait he had some warm pasta and salmon to help him recover from the days exertions and fuel him for the next days.\nHe was a really nice guy, engaged to be married to a Welsh lady and enthusiastic about cycling. We were both tired though and planned to get up early, although he said he was going to set his alarm for 04:00 and make an early start which didn’t bother me as I could doze a bit afterwards.\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-21\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-21\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 9 - Horton in Ribblesdale to Gargrave\nStill raining when I was woken early by Daniil’s alarm. Knowing he’d set it early I didn’t bother to look at the time and dozed instead listening to it going off repeatedly. When I finally looked at the time it was 06:50 so I got up and let Daniil know that he’d missed his early start. He wasn’t too bothered, saying that if the body needs rest it needs rest. We both slowly packed up our kit which was still slightly damp and he sensibly did some stretching before posing for a picture. We wished each other good luck and he and jumped on his bike and set off, rejoining the Pennine Bridleway heading ultimately to Cape Wrath.\n\n\n\nDaniil\n\n\nI was packed and ready to head out into the wind and rain shortly after and with nothing to keep me hanging around brace myself for the onslaught and headed out. Not sure there was much that could have prepared me for the battering the wind and rain gave me as I headed up Pen y Ghent, it was brutal! The rain was heavy and coming in horizontal as I gained height and I passed a couple who I’d seen in the car park of the pub who had decided not to continue which seemed wise, they were in shorts and pretty basic trainers. The higher I got the stronger the wind blew and I chatted to a guy who said he’d been out wild camping for the night (nutter!). On the final path up to the summit plateau the wind really kicked in though and I was very grateful of having my walking poles to help stabilise me. Even then I was still struggling to walk in a straight line and remain upright! The plateau itself provided some respite as the wind wasn’t hitting the side of the hill and being driven upwards and there was a shelter (viz. wall) near the summit trig point.\n\n\n\nPen-y-Ghent Summit\n\n\nA couple who had come up the way I was due to descend arrived and said it was pretty windy and wet on the path. There was no point hanging around so I got on with descending out of the horrendous wind. The rocks were wet, as predicted several days ago, but weren’t that bad to negotiate (hands were employed for safety). The loss of altitude was accompanied by some easing off of the wind, but not by much. As I skirted round Cow Hill though the weather seemed to ease a bit, with both the rain and wind dying down a bit and the cloud base lifting a little. I unfortunately had to head up and over Fountains Fell (668m) so up I went again. The weather was definitely improving as it wasn’t anywhere near as windy or wet as Pen y Ghent had been although the views remained non-existent.\n\n\n\nFountains Fell\n\n\nSoon I was dropping down and passing through farmland and making my way to skirt the long way round Malham Tarn where the wind was whipping across and making quite a few waves. Somewhere to return to in the future as it was very picturesque (like so many other places on the hike!).\n\n\n\nMalham Tarn\n\n\n\n\n\nThe River from Malham Tarn\n\n\nThe sun was out in force by now so I stripped down to shorts and hung my waterproofs from my arm to dry in the wind which worked pretty quickly as I could pack them away before starting the descent to Malham Cove. I passed a couple of young ladies out for a walk as I descended from Malham Moor and skirted under Watlowes. They seemed impressed by my progress and said they would have to try hiking The Way which I thoroughly recommended they do. The limestone pavement above Malham Cove was impressive and had attracted those fit enough to climb the stairs up from the bottom of the Cove.\n\n\n\nAbove Malham Cove\n\n\n\n\n\nMalham Cove\n\n\nI was getting hungry by now so after a few pictures I descended the steps, picking up an empty Red Bull can along the way, and passed many people out for the day as I made a beeline for The Lester Arms where I had a mushroom and black bean burger with wedges and beer battered onion rings. Chatted to a few others who were having a drink and explained how useful I found walking poles to a couple, as the lady had had operations on her knees which were weak as a consequence.\n\n\n\nLunch at The Lister Arms, Malham\n\n\nSatiated and watered I set off for Gargrave, tracking the River Aire as it wound its way down the dale. This took me most of the way, but I parted company and was taken over a couple of hills via some fields with cows in which weren’t bothered by my presence.\n\n\n\nMalham Cove from afar\n\n\n\n\n\nGoooogley Eyes\n\n\nSoon I arrived at Gargrave and headed to the Co-op to get a few beers for the evening before back-tracking to Eshton Road Caravan Park where a lovely old gentleman pointed me to a small enclosed field to camp for the night. A beer whilst pitching the tent close to the wall so it was sheltered from the storm that was forecast to come through overnight before heading back into town to eat at The Frying Yorkshireman which was excellent both in terms of food and service. Very friendly, big portions of delicious food and a boot shaped glass for the beer. If you’re ever in Gargrave and need to eat head here you won’t be disappointed. Back to the tent for another beer (cheaper than the pub!) and an early night.\n\n\n\nThe Frying Yorkshireman\n\n\nChatted to a friendly guy on the campsite who I’d seen eating fish and chips opposite the chippy whilst I snapped pictures of the antiquated pharmacy and then got an early night.\n\n\n\nGargrave Pharmacy\n\n\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-22\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-22\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 10 - Gargrave to Colden\nWoken early around 05:00 by the wind and rain of Storm Lilian passing through, it was pretty windy and on checking the satellite imagery it was due to rain heavily between 06:00 and 07:00 and decided to stay in bed and ride that out rather than take the tent down in the rain. Forecast was spot on and my tent was almost flattened by some very strong gusts about 06:30. Fortunately its design (an older version of the Wild Country Zephyros 1) with a single pole meant it sprung back up pretty quickly and I weathered the storm ok. On rising it was clear that some of the other campers hadn’t fared so well. There was a camper van with the air-beam awning collapsed, but that was because they had been savvy enough to do it themselves before the wind took it away. Unfortunately the awning to the caravan of the guy I’d chatted to the previous night was nowhere to be seen.\nAfter packing my kit up I set off to the Co-op to get supplies for the day but as I crossed the road I realised I hadn’t seen my wallet so checked my pockets and those of my rucksack and then realised I’d left it on the floor of the tent and packed it away inside! Wasted five minutes retrieving it from inside whilst next to the canal and packing it all back up again. This proved a bit of a waste of time as on arrival at the Co-op the lights were out and the door was shut with a sign on the window saying there had been a power cut and they weren’t open. I managed to get the attention of a member of staff and asked where the nearest place shop was. After establishing that I couldn’t hear a word she was saying through the shut door she cracked it open and advised me it was 10 minutes drive down the road in Skipton, clearly not paying any attention to the rucksack on my back and walking poles I was carrying!\nI resigned myself to having to make a detour later on and set off for the day crossing open farmland and winding my way through a few country lanes where I encountered a tree that had been felled by the wind. Fortuantely it only slowed me down for a minute or so as I negotiated the branches and soon I found myself walking along a section of the canal where I came across a cool two-storey bridge.\n\n\n\nTree Down\n\n\nIn Thornton in Craven I asked a gentleman who was rescuing his bin where the nearest shop was. Unfortunately there weren’t any shops in Thornton, the nearest was Earby which was off-route, but I needed some food for the day so crossed a couple of fields, noticing that the Colne and Broughton/Skipton Road was closed due to another tree that had come down and there was a queue of lorries backing up into Earby as they had no way of turning around on the narrow lanes. Earby had the usual Co-op and supplies for the day were purchased along with a “Yorkshire Slice” from a local bakery where I instantly regretted not buying my lunch from there instead of the Co-op.\n\n\n\nYorkshire Dales\n\n\nI soon rejoined The Way and headed up to Pinhaw Beacon where there was a memorial to those who had died in the COVID19 Pandemic written to a Shakespeare sonnet. I was saddened to see some idiots had vandalised the tribute to the NHS at the end. Some people have no idea how fortunate they are to have free, world leading, health-care in this country.\n\n\n\nCoronavirus Sonnet\n\n\nAfter dropping down into Lothersdale and out again I met a guy who was walking from Dover to Cape Wrath and was currently on day 20. He was a talker and regaled going through London rather than round it camping near some football stadium and many other facets of his 20 days to get to that point. Mindful of the time I wished him well and carried on as I still had a long way to go.\nI’d hoped there would be a pub near The Way in Ikornshaw but alas no so up Green Hill I went. As I reached the edge of the moors there appeared to be a collection of extreme sheds dotted along the edge. Perhaps these were for game keepers but there were a lot of them. There was a guy working outside of one and he took the time to calm his dog as I passed but I didn’t think to ask him what the sheds were for or why there were so many.\nThe moorland opened up and I met a lady who was out for a run, we had a brief chat about how nice a day it was and she mentioned she had just passed another person who was hiking the Pennine Way and heading South, this was a surprise and I decided to quicken my pace and see if I could catch them up. On the top of the moor I met two ladies who were heading north independently, the second was suffering from a very sore knee which she suspected was due to the weight of her pack. I advised getting some poles if possible to help and carried on.\n\n\n\nShooting Cabin\n\n\nI dropped down and passing Ponden Reservoir then up and over to Within Heights where an abandoned farm was a possible an inspiration for Emily Bronte’s Wuthering Heights, a book I’ve never read and likely never will.\nGood flagstones led to Walshaw Dean Reservoirs which were skirted round and Lower Gorple Reservoir passed quickly.\n\n\n\nWalshaw Dean Lower Reservoir\n\n\nTraversing the moor of Standing Stone Hill was straight forward and I soon dropped down into Colden where I easily found the The New Delight Inn with the Hebden Bridge Camping adjacent. I wandered into the busy pub which had a lovely community vibe, lots of people milling around chatting and was asked if I was looking to camp for the night (I can’t think what gave that away!) and quickly directed to a lady sat with friends who happily took my £20 note and gave me a warm £10 she had extracted from her bra. The setup was simple as there was a small field next to the car park, flat at the bottom with toilets outside the pub and one person already camped there.\nHeading over I dumped the pack and said hello to the other camper who happened to be the person the runner had met earlier in the day. His name was Mike and he was busy enjoying his dinner and avoiding the midges. We had a brief chat about how long we had been hiking and I left him to his dinner to pitch my tent before it got dark and return to the pub to get food and beer. The pub didn’t actually serve food, beyond the usual bar snacks of crisps and nuts which would often do me fine but tonight I needed something more substantial and was in luck as there was a takeaway in Hebden Bridge that delivered. Connected to the WiFi I placed my order and settled down with a pint to write up my notes for the past few days as I’d fallen behind. I was unfortunately quite tired, the light wasn’t great and I didn’t have my glasses which I find I’m needing more frequently these days, but I got some notes written before the pizza arrived which was delicious.\n\n\n\nTakeaway Pizza Delivered\n\n\nI had a big day planned as I wanted to pass Standing Edge where there wasn’t much accommodation, and push through to Crowden in a single big day which would leave me with “just” Bleaklow and Kinder to pass to get to Edale and so suitably satiated I headed to bed early. Thankfully the kids playing outside the pub whilst their parents drank and chatted didn’t use my tent for target practice this time.\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-23\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-23\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 11 - Colden to Crowden\nUp early, again, but not as early as Mike who said hello as he set off, he thought I might catch him up though, having done so over the past few days. Tent away and I hit the trail again, heading down a lovely path above Colden Water. I wasn’t paying attention though and made a minor navigational error missing the turning and only realised 500m down the hill in the woods adding an unnecessary kilometre to the days hiking and having to turn around and head back up hill to find The Way which followed a very narrow, overgrown path between dry stone walls of fields before dropping steeply down into Calder Valley.\n\n\n\nStoodley Pike from afar\n\n\nI had been thinking about getting some food here but as I approached the River Calder I saw a heron in the middle of the river and completely forgot to look for food as I snapped as many pictures as I could before it flew off.\n\n\n\nHeron\n\n\nHappy to have got some half decent shots, given the lack of zoom on my camera, I started the long slog out of the valley to Stoodley Pike. This section had looked steep but the path followed farm tracks and wound its way through woods and some fields before the final steep section onto the edge of the moor.\n\n\n\nLand Rover\n\n\nBefore this last steep bit I paused to sort out my clothing as it looked like it was about to start raining and spied what I thought was Mike passing Stoodley Pike. Its an impressive monument and I took the time to climb the 39 steps that take you to the balcony and snap some overcast shots in each of the ordinal directions.\n\n\n\nStoodley Pike\n\n\n\n\n\nFrom Stoodley Pike\n\n\nContinuing along The Way I met a group out for an early hike approaching from the opposite direction and followed the flagstones across the moor to Warland Reservoir which was looking a bit empty.\n\n\n\nWarland Reservoir\n\n\nI was gaining on Mike now and was able to go quickly as the track was flat and my legs were strong from the past ten days of hiking. Blackstone Edge Reservoir and The White House came into view and were quickly passed and as I started to head up to Blackstone Edge I caught up with Mike who had paused to check his navigation and we walked and chatted as we went up to the edge. He said I should carry on as I was faster but after checking out the largest buttress for routes I was happy to walk and chat with him which made a change from the last ten days of being with my own thoughts.\n\n\n\nThe White House\n\n\n\n\n\nBlackstone Edge Trig Point\n\n\nMike was easy to chat to, he is retired but worked on oil rigs off the coast of Norfolk after responding to an advert in a paper for mechanically minded people to transfer into the area. He’d worked his way up through the different levels, for a time having his own company and had lived in the US for a period with a lady he was with for a while. On returning to work after the COVID pandemic he decided he didn’t want to do that any more and after seeing a financial consultant he realised he could take early retirement and who would blame him. He enjoyed getting out walking and seeing the countryside with his dog Rufus who he clearly loved a lot and was pleased to be fit and able to enjoy his retirement unlike many of his friends who were still working or had heart conditions or other physical ailments which prevented them from doing so.\nWe soon passed over the M62 and both found the noise and bustle quite jarring compared to the peace and solitude we’d experienced hiking over fells and moorland but quickly left it behind and returned to peace and quiet. As we neared a road the possibility of lunch loomed in the form of a small cafe in a container by the side of the road but unfortunately they were closed. We were both sadden to see there appeared to be some people living in tents around this layby and the rise in homelessness is another poor reflection on the previous governments performance.\n\n\n\nCrossing the M62\n\n\n\n\n\nThe Most Easterly Point in the County of Lancashire\n\n\n\n\n\nMike\n\n\nA couple of kilometeres and we reached Standedege above Marsden where I had abandoned a previous attempt to walk the Pennine Way from South to North a number of years ago and stopped on a convenient rock for lunch. Mike had a wrap and some cheese whilst I broke out my stove and reheated my penultimate dehydrated meal for lunch. The stove is super efficient and I was soon tucking into the hotpot mash which was tasty but as always with these meals there is a tiny bit that never gets the water it needs. Mike set off before me and 15 minutes later I’d packed everything away and hit the path again myself.\nI noticed the Great Western Inn had a number of caravans outside it and this must have been the pub that someone had told me was a cheap campsite to stay at, but I was pushing on to Crowden so I had a relatively easy day into Edale on my last day. This first section to was nice going over moorland, passing between Black Moss and Swellands reservoir before dropping down and crossing Wessenden Brook and passing along the side of Wessenden and Wessenden Head reservoir. I noticed a small waterfall on the opposite side of the brook.\n\n\n\nStandedge\n\n\n\n\n\nWessenden Brook\n\n\n\n\n\nMiddle Wessenden\n\n\nAs I reached the top of Wessenden Head there was a Canadian couple who were trying to work out which way the Pennine Way went so I pointed them in the right direction. I was then asked by a lady with a large group of other “Instagrammers” who had just tumbled out of cars from the nearby road “Is the waterfall nearby?” I tried to explain that I had passed a waterfall but that it was 30-40 minutes down the track behind me and I didn’t know which waterfall they were looking for. They seemed to think I was referring to the road they had just left and not the track behind me. I gave up trying to answer their question as it seemed futile. Crossing the A635 felt like being on home territory having gone along it a few times earlier in the year to go climbing at the Standing Stones. Good paving slabs led over Dean Clough and up to Black Hill then across open moorland with a very boggy section where the paving slabs had disappeared to the head of Crowden Great Brook. This involved weaving back and forth across the brook before the path rose up the hillside to go over Laddow Rocks. Normally I would have a poke around and a small climb but was finding this section tough going as the heather made for a very narrow path and using poles was fairly pointless. Pretty uneven rocks down to Crowden, but I soon passed through the garden of Crowden Outdoor Education Centre where one of my friends from the Peak Climbing Club worked. Had it been a weekday I would have stopped to say hello but it was early evening on a Saturday so he wasn’t around.\n\n\n\nBlack Hill Trig Point\n\n\n\n\n\nFrom Laddow\n\n\n\n\n\nBleaklow in Sun\n\n\nChecked in at Crowden campsite, the most expensive of the trip at £13, and said hello to Mike who had just finished pitching his tent and to a father and son who had hiked over from Marsden and had been hoping to wild camp on Black Hill or above Laddow before continuing to Kinder Scout for the night and then onto Hathersage. They were trying out their kit in prepertion of hiking part of the Kungsleden Trail which I’d never heard of but sounds interesting, albeit almost twice as long as the Pennine Way, although considerably flatter by the sounds of it.\nMike and I decided to order some takeaways as it was our last night on the trail. I had another pizza with chips and he had a burger with chips. The food arrived as it got dark and we chatted over dinner before an early night as we were both tired from the long day and had Bleaklow and Kinder the following day.\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-24\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-24\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\nDay 12 - Crowden to Edale\n28km to cover before 15:00, my self-imposed finish time to ensure I can claim to have completed The Way in 12 days so up early. Unconventionally for me I had one of my remaining two meals for breakfast, pasta and mushrooms, to see me through as there are no shops or pubs between Crowden and Edale. I packed the tent up damp from the previous nights rain, not caring as I was heading home today and could dry it once there. I said goodbye to Mike who was in no rush after the previous days efforts and thanked him for his company the previous day. Wished the father and son pair good luck on their the rest of their trip over-nighting on Kinder and then heading to Hathersage and their planned Arctic trip.\nClear blue skies and warm sun helped me set a decent pace as I headed down to the dam of Torside Researvoir and crossed over to the steep climb up onto the Bleaklow plateau.\n\n\n\nTorside Reservoir\n\n\n\n\n\nTo Bleaklow Head\n\n\n\n\n\nTorside Reservoir\n\n\nHalf-way up the steep section whilst snapping pictures with the Sony for some bizarre reason it stopped taking pictures, it would focus and do everything except actually take and save the picture. I wasted about 15-20 minutes trying to sort it before deciding it would have to wait and besides I had a 12-month warranty on it courtesy of Harrison Cameras excellent second-hand policy, so continued up the remaining section of the trail that runs round the edge of Torside Clough. As I crossed over the clough at the top I had another go at fixing the camera and resetting all settings seemed to sort it out. Will have to keep an eye on this though and consider returning it if the problem recurs.\nBleaklow Head was easy to find, but the wind had picked up a bit and it was starting to feel chilly when exposed.\n\n\n\nBleaklow Head\n\n\nI had forgotten what a pain it was navigating through the groughs on Bleaklow, compounded by the fact it had rained recently and/or overnight so many had running water in them and navigating round them slowed me down a bit, but soon the number of people I was passing, most going to look at the remains of the plane crash, increased and I soon reached the head of the Alport River.\n\n\n\nAlport River Head\n\n\nHaving walked from here to Kinder a few weeks previously when training I knew exactly where to go. Not that its hard, a massive path and paving slabs lead you to the crossing of the A57. A few kilometres of paving slab followed before swinging south and heading up the North tip of Kinder Scout. It was pretty windy on the edge of the plateau but there were plenty of people about, presumably going to see the inverted waterfall phenomenon which occurs when the wind is high as the water falling off of Kinder Downfall gets blown back up in an endless cycle, although being the end of summer it wasn’t as impressive as it can be in autumn.\n\n\n\nKinder North\n\n\n\n\n\nKinder Reservoir\n\n\nGrateful of the poles I traversed the edge of the plateau passing behind the Upfall and heading South towards Edale Rocks and Jacobs Ladder. Lots of people were out on the hills which is great to see but something of a shock given how quiet it had been along most of The Way. I had my eye on the clock to make sure I got to Edale before 15:00 so didn’t hang around and apart from offering a bit of advice to people who looked lost and talking to a couple who had been keeping pace and talking loudly about how to stay fit (they were impressed with having walked +400km in 12 days) I kept moving and got to Barber Booth.\n\n\n\nJacobs Ladder\n\n\n\n\n\nEdale 1 1/4 miles\n\n\n\n\n\nGreat Ridge Edale Valley\n\n\n\n\n\nThe last Stretch\n\n\nI’d forgotten The Way has one very small bit of uphill to return to Edale here but it passed quickly and I was soon bouncing down through the last field to take the track that goes along a small brook into Edale. With no fanfare I exited the path and was done, I had reached The Nags Head, the time was 14:51 so I had walked there from Kirk Yetholm in pretty much 12 days of hiking and was delighted, if a little tired.\nThree members of Edale Mountain Rescue were out collecting donations to fund their excellent work so I approached them and asked if one of them would mind snapping a couple of pictures of me. “Only because you’ve got a Podsacs rucksack” said one of them. My riposte was simple “Does hiking the Pennine Way in 12 days qualify for a picture?”. The gentleman was more than happy to oblige me and suggested we go over to the wall where there was a plaque as well as snapping pictures next to the pub. I ducked inside to grab a pint and asked if there was a log book to sign which there was, much smaller than the one at Kirk Yetholm but I recorded my walk and took my pint (and a pint of water) outside to chat to the guys from Mountain Rescue and check train times.\n\n\n\nThey got the sign wrong\n\n\n\n\n\nPropping up The Nags Head\n\n\nHydrated I wandered down to the train station to catch the 15:33 train which was typically delayed and did some stretching of my tired legs and was rudely reminded of why I enjoy being in quiet places because some drunk idiot started making fun of me for doing “yoga”. Train arrived late and whisked me back to Sheffield and I was grateful of the free ride as no ticket inspector passed through the carriage I was in. My wife and daughter were out so I walked home, not that I’d have accepted a lift anyway, stopping for a pint in a local pub (The Brothers Arms) before heading home.\nHaving been away for 12 days there were tons of leaves on our drive and in the street so before heading in I put my pack down and grabbed my broom and set about clearing them. After five minutes or so the front door opened and my wife and daughter asked if I was going to actually come in. I felt bad when I did as my daughter had made a lovely finishing banner for me to break through with “Welcome Home Daddy” in rainbow letters. She had apparently been peeking out the window keeping an eye out for me.\n\nRPython\n\n\n\n\nCode\nplot_day(gps_df, \"2024-08-25\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gpx_df(gps_df=gpx[\"2024-08-25\"], colors=\"speed\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/pennine-way/index.html#reflections",
    "href": "posts/pennine-way/index.html#reflections",
    "title": "Pennine Way 2024",
    "section": "Reflections",
    "text": "Reflections\nThe Pennine Way passes through a lot of rugged, remote and beautiful countryside and I would highly recommend hiking it in some manner at some point in your life.\n\nHiking multiple days is really good fun.\nComfortable shoes are essential.\nSandals and shorts are great for wet weather and bogs. I used a pair of Teva’s and have bought another spare pair.\nGrass is the best thing for walking on, rubble the worst.\nThere are lots of really nice friendly people in the countryside. Get out of the town or city you live in and go and meet them and take the spirit back to the city.\nAlways say hello to the people you pass.\nBackpacking-induced Paresthesias (see also here) is weird and lasted for at least four weeks.\nRain isn’t that bad really, but combined with wind can be somewhat tiring.\nHead winds are annoying, particularly if strong.\nWalking poles are amazing for reducing stress on your knees and allowing you to go faster for longer.\nYou don’t have to go a long way to meet your neighbours!"
  },
  {
    "objectID": "posts/pennine-way/index.html#summary-table",
    "href": "posts/pennine-way/index.html#summary-table",
    "title": "Pennine Way 2024",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nDate\nStart\nFinish\nDistance (km)\nAscent (m)\nDescent (m)\nDuration (hⓂ️s)\nPauses (hⓂ️s)\nTotal Time (hⓂ️s)\nMean Speed (km/h)\nMax Speed (km/h)\n\n\n\n\n1\n2024-08-14 Wed\nKirk Yetholm\nClennell Street\n18.45\n1129.24\n705.14\n4:45:11\n0:1:18\n4:46:29\n3.88\n5.7\n\n\n2\n2024-08-15 Thu\nClennell Street\nBryness\n23.554\n736.15\n1052.45\n6:00:26\n0:31:57\n6:32:24\n3.92\n5.93\n\n\n3\n2024-08-16 Fri\nBryness\nHadrians Wall\n47.904\n1573.63\n1503.53\n11:12:51\n3:43:42\n14:56:34\n4.27\n6.15\n\n\n4\n2024-08-17 Sat\nHadrians Wall\nGarigill\n49.112\n1907.28\n1871.28\n13:23:58\n1:44:56\n15:08:54\n3.67\n6.14\n\n\n5\n2024-08-18 Sun\nGarigill\nHigh Cup Nick\n33.094\n1593.73\n1381.03\n8:38:28\n2:53:02\n11:31:31\n3.83\n6.51\n\n\n6\n2024-08-19 Mon\nHigh Cup Nick\nMiddleton in Teesdale\n28.341\n840.14\n1190.44\n7:14:33\n0:8:54\n7:23:27\n3.91\n6.59\n\n\n7\n2024-08-20 Tue\nMiddleton in Teesdale\nThwaite\n42.736\n1704.63\n1684.63\n10:23:16\n2:27:48\n12:51:04\n4.11\n6.07\n\n\n8\n2024-08-21 Wed\nThwaite\nHorton in Ribblesdale\n40.691\n1547.21\n1619.61\n9:30:41\n2:43:40\n12:14:22\n4.28\n6.24\n\n\n9\n2024-08-22 Thu\nHorton in Ribblesdale\nGargrave\n38.811\n1759.36\n1908.96\n10:17:26\n2:21:08\n12:38:34\n3.77\n6.43\n\n\n10\n2024-08-23 Fri\nGargrave\nColden\n47.980\n2302.5\n2121.5\n11:22:01\n0:24:09\n11:46:10\n4.22\n6.77\n\n\n11\n2024-08-24 Sat\nColden\nCrowden\n49.205\n2512.51\n2582.31\n11:12:44\n1:03:51\n12:16:35\n4.39\n7.05\n\n\n12\n2024-08-25 Sun\nCrowden\nEdale\n28.535\n1323.47\n1305.07\n6:58:46\n0:20:43\n7:19:29\n4.09\n6.62\n\n\nTotal\n\n\n\n448.413\n18929.85\n18925.95\n\n\n\n4.0283333\n7.05"
  },
  {
    "objectID": "posts/git-ssh/index.html",
    "href": "posts/git-ssh/index.html",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "",
    "text": "How to configure individual Git repositories to use specific SSH keys. This is useful if you have more than one account on a forge, for example a personal and work account."
  },
  {
    "objectID": "posts/git-ssh/index.html#background",
    "href": "posts/git-ssh/index.html#background",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Background",
    "text": "Background\nTypically when pushing and pulling changes to a forge such as GitHub, GitLab or Codeberg you use an SSH (Secure SHell) key to authenticate that you have permission to access the repository."
  },
  {
    "objectID": "posts/git-ssh/index.html#ssh-keys",
    "href": "posts/git-ssh/index.html#ssh-keys",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "SSH Keys",
    "text": "SSH Keys\n\nConcept\nSSH keys are, in conjunction with “keychains”, used to save you having to enter a password each time you connect from one computer to another. They are generated on your computer and consist of two parts, a private key which remains on your computer and a public key which you place on remote computers you wish to connect to. There is a password associated with your key which is required to “unlock” your private key on your computer. Only an unlocked private key will match with a public key. Think of the public key as the lock on your front door, and the private key the key you carry on your traditional, physical, keychain/keyring. Only when the two match will things be unlocked, although you have to unlock your private key when you want to use it just as you have to get your keys out of your pocket (although “keychains” help with this).\n\n\nGeneration\nThere are different algorithms for generating SSH key pairs. DSA is no longer considered secure and RSA keys should have at least 2048-bits if not 4096-bits. A good choice these days is to use an elliptic curve based key such as ed25519 as they are shorter and faster. For more on why you should use this key see the article Upgrade your SSH keys!.\nTo generate a key use the following command entering a secure (i.e. long) password.\nssh-keygen -o -a 100 -t ed25519\nYou will be prompted for a filename to save your keys to, so you should know where to find them (the default is ~/.ssh/id_ed25519[.pub]). You have a private key ~/.ssh/id_ed25519 and a public ~/.ssh/id_ed25519.pub and we will use this to set up authentication on your Git Forge.\n\n\nForge Configuration\nUnder your account settings on your chosen Git Forge navigate to Settings &gt; SSH and GPG Keys and select Add New Key on (GitHub). On GitLab navigate to Preferences &gt; SSH Keys GitLab), this page allows you to add a new key.\nYou need to copy and paste your public key into the Key box on these pages and give it a name (typically the hostname of your computer is a good choice). To view your public key simply use cat and copy and paste it. You can optionally choose to set an expiration date for your key which is good practice but means you have to generate new keys in the future.\ncat ~/.ssh/id_ed25519.pub"
  },
  {
    "objectID": "posts/git-ssh/index.html#git-global-ssh-configuration",
    "href": "posts/git-ssh/index.html#git-global-ssh-configuration",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Git Global SSH Configuration",
    "text": "Git Global SSH Configuration\nTypically your global configuration for which key to use is set in ~/.ssh/config with an entry similar to the below.\nHost github.com\n     User git\n     Port 22\n     PreferredAuthentications publickey\n     IdentityFile ~/.ssh/id_ed25519\nHere it uses the User name git on port 22. The preferred authentication method is using a publickey and the private key used is stored locally at ~/.ssh/id_ed25519.\nWhen asked to connect to a forge using SSH (e.g. git pull or git push) will look through the ~/.ssh/config file to see if there is a configuration section that matches the target and if so use the configuration defined there-in. You will then be prompted for your SSH private key password.\n\nWhat are Keychains?\nYou may be wondering how an SSH key makes your life easier, you are still prompted to enter a password when trying to interact with a Git Forge, or use it in a more traditional manner to connect over SSH to another server. This is where the magic of a “keychain” steps in to make your life easier, you still have to enter a password but only once to add your SSH key to the “keychain”. Typically keychains are front-ends for interacting with and managing SSH agent. The name is apt since you add your SSH key to the keychain once, typically on log-in, and are asked for your password to unlock it and then stores it in the SSH agent. Then each time SSH requires an SSH key it retrieves it from the keychain rather than prompting you for a password.\nThere are many different implementations of keychain such as the Funtoo Keychain Project, Seahorse the GNOME GUI management tool,"
  },
  {
    "objectID": "posts/git-ssh/index.html#git-per-repository-configuration",
    "href": "posts/git-ssh/index.html#git-per-repository-configuration",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Git Per Repository Configuration",
    "text": "Git Per Repository Configuration\nWe now get to the meat of this post, how to configure individual repositories to use specific SSH keys. This may be desirable if you have two accounts on the same forge e.g. both on GitHub.com or both on GitLab.com? As of Git 2.10.0 you can configure each repository to use a specific key (source). At the command line…\ncd a/git/repository\ngit config core.sshCommand \"ssh -i ~/.ssh/work_ed25519 -F /dev/null\"\ngit config --local user.name \"Username\"\ngit config --local user.email \"repos@username.com\"\nThis adds the following to the repositories configuration which is stored under .git/config and you can of course enter this directly to the configuration file yourself.\n[core]\n    sshCommand = ssh -i ~/.ssh/work_ed25519 -F /dev/null\n[user]\n    name = Username\n    email = repos@username.com\nWhat is this doing? Well it’s instructing Git to run ssh using the private key file (with the -i flag to specify the identity_file) that is located at ~/.ssh/work_ed25519. Providing you have…\n\nAlready uploaded the public key (work_ed25519.pub) to your GitHub account.\nStored this key in a Keychain as described above.\n\n…you shouldn’t be prompted for a password.\nYou can now configure, on a repository basis, which SSH key is used by Git when pushing/pulling changes from the remote origin (typically a forge such as GitHub, GitLab, Codeberg or so forth). If however you have multiple projects you wish to setup with an alternative SSH key configuration it can be tedious to configure each repository. Thankfully Git &gt;= 2.13 introduced Conditional includes to the configuration."
  },
  {
    "objectID": "posts/git-ssh/index.html#conditional-includes",
    "href": "posts/git-ssh/index.html#conditional-includes",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Conditional Includes",
    "text": "Conditional Includes\nYou global configuration is stored in ~/.gitconfig and defines key variables such user and name, the default editor and many other options, including a customised sshCommand as was added above to a local .git/config file.\nGit 2.13 introduced the aforementioned Conditional includes which works “_by setting a includeIf.&lt;condition&gt;.path variable to the name of the file to be included.”. For our current case-use the &lt;condition&gt; we are interested in is whether the path, which is interpreted as a pattern, is a gitdir then we include what follows.\nFor example, we place all of our work related Git repositories under the ~/work/ directory and wish to use ~/.ssh/work_ed25519 for these and keep all of our personal repositories elsewhere and wish to use our main ~/.ssh/id_ed25519 key for those.\nOut ~/.gitconfig should look like\n[user]\n    name = Your Name\n    email = your.personal@email.com\n\n[includeIf \"gitdir:~/work/\"]    # Directory paths ending in '/** has the globbing wildcard '**' added by default.\n    path = ~/work/.gitconfig_work\n\n[core]\n    sshCommand = ssh -i ~/.ssh/id_ed25519 -F /dev/null\nThen our ~/work/.gitconfig_work can contain the alternative values we wish to use for all repositories under the ~/work/ directory.\n[user]\n    name = Your Name\n    email = your.work@email.com\n\n[core]\n    sshCommand = ssh -i ~/.ssh/work_ed25519 -F /dev/null"
  },
  {
    "objectID": "posts/git-ssh/index.html#commit-verification-with-ssh",
    "href": "posts/git-ssh/index.html#commit-verification-with-ssh",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Commit verification with SSH",
    "text": "Commit verification with SSH\nVerification of commits is a useful security feature, but beyond the scope of this article but as doing so with SSH keys is a recently supported feature on GitHub (see blog SSH commit verification now supported) I felt it worth mentioning."
  },
  {
    "objectID": "posts/git-ssh/index.html#links",
    "href": "posts/git-ssh/index.html#links",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Links",
    "text": "Links\n\nSSH\n\nSSH Academy\nOpenSSH Key Management, Part 1\nOpenSSH Key Management, Part 2\nOpenSSH Key Management, Part 3\n\n\n\nForges\n\nGitHub | Connect with SSH\nGitLab | Use SSH keys to communicate with GitLab\nCodeberg | Adding an SSH key to your account"
  },
  {
    "objectID": "posts/virtualenv-hooks/index.html",
    "href": "posts/virtualenv-hooks/index.html",
    "title": "virtualenvwrapper hooks",
    "section": "",
    "text": "I’ve written previously about virtualenvwrapper which I use to manage my Python Virtual Environments and mentioned the possibility of using hooks but didn’t go into detail."
  },
  {
    "objectID": "posts/virtualenv-hooks/index.html#introduction",
    "href": "posts/virtualenv-hooks/index.html#introduction",
    "title": "virtualenvwrapper hooks",
    "section": "Introduction",
    "text": "Introduction\nJust like the various hooks available in Git, virtualenvwrapper also supports hooks that allow scripts to be run in response to various events. These reside under your $VIRTUALENVWRAPPER_HOOK_DIR which by default is the same as your $WORKON_HOME directory and in a typical standard installation will be ~/.virtualenvs.\nThe available scripts that are recognised are…\n\nget_env_details\ninitialize\npremkvirtualenv\npostmkvirtualenv\nprecpvirtualenv\npostcpvirtualenv\npreactivate\npostactivate\n\nEach of these is a simple shell script and will start with the scripting language to use e.g. #!/usr/bin/bash or #!/usr/bin/zsh depending on your shell. You can then script the actions you wish to take when the script is executed."
  },
  {
    "objectID": "posts/virtualenv-hooks/index.html#install-minimal-requirements",
    "href": "posts/virtualenv-hooks/index.html#install-minimal-requirements",
    "title": "virtualenvwrapper hooks",
    "section": "Install minimal requirements",
    "text": "Install minimal requirements\nI’m a big fan of dotfiles1, mine are hosted on GitLab, it’s a repository of my configuration files and scripts that I use regularly across multiple computers. Because I’m lazy I wrote a couple of requirements.txt files for installing packages in my virtual environments.\n\nrequirements.txt : holds everything I might ever want to use in Python.\n\npython-lsp-requirements.txt\n\nInstall packages for setting up a Python Language Server (which I use from Emacs).\n\n\n\nvenv_minimal_requirements.txt\n\na minimal set of the most common Python packages I am likely to want when creating a new virtual environment.\n\n\n\nBecause I have my dotfiles cloned to the same location on every computer (~/dotfiles) I added the following to the ~/.virtualenvs/postmkvirtualenv 2 which will install all of the packages listed in ~/dotfiles/python/venv_minimal_requirements.txt whenever a create a new virtual environment, whether that is with mkvritualenv or mktmpenv.\npip install --no-cache-dir -r ~/dotfiles/python/venv_minimal_requirements.txt\nThis ensured the latest versions of each packages listed in ~/dotfiles/python/venv_minimal_requirements.txt were downloaded and installed as the --no-cache-dir prevents using cached versions of packages."
  },
  {
    "objectID": "posts/virtualenv-hooks/index.html#a-smarter-script",
    "href": "posts/virtualenv-hooks/index.html#a-smarter-script",
    "title": "virtualenvwrapper hooks",
    "section": "A smarter script",
    "text": "A smarter script\nThis served me well for a time, but occasionally I found I didn’t want to install any packages in a new virtual environment (most often when testing new branches using mktmpenv) and I’d have to remember to comment out the line in the hook file (~/.virtualenvs/postmkvirtualenv) before creating the environment. Typically though I’d forget to do this and would have to halt installation of required packages, deactivate the environment, then comment it out and create a new environment.\nThis quickly became irksome.\nBut ~/.virtualenvs/postmkvirtualenv is just a script and so we can use a bit of scripting knowledge to make it interactive and ask the user if they want to install the packages listed in venv_minimal_requirements.txt. I found a really useful answer on StackOverflow in the How do I prompt for yes/no/cancel input in a Linux shell script that showed several different ways to prompt the user for a response as to whether they want to do something.\nI therefore updated my ~/.virtualenvs/postmkvirtualenv to the following which prompts for a numeric response, 1 for Yes and 2 for No and takes the appropriate action, installing using my original invocation of pip if I want to install packages and enter 1 or installing nothing if I enter 2.\n#!/usr/bin/zsh\n# This hook is sourced after a new virtualenv is activated.\n\n\necho \"Do you wish to install minimal requirements (from venv_minimal_requirements.txt)? \"\nselect yn in \"Yes\" \"No\"; do\n  case $yn in\n    Yes ) pip install --no-cache-dir -r ~/dotfiles/python/venv_minimal_requirements.txt; break;;\n    No ) echo \"No packages installed. install packages with 'pip'.\\n\"; break;;\n  esac\ndone\nNB You may want to tweak the opening shebang if you use the Bash shell."
  },
  {
    "objectID": "posts/virtualenv-hooks/index.html#conclusion",
    "href": "posts/virtualenv-hooks/index.html#conclusion",
    "title": "virtualenvwrapper hooks",
    "section": "Conclusion",
    "text": "Conclusion\nA little bit of shell scripting knowledge can be really powerful when used in conjunction with “hooks”. This is true of virtualenvwrapper as well as of Git and the pre-commit framework."
  },
  {
    "objectID": "posts/virtualenv-hooks/index.html#footnotes",
    "href": "posts/virtualenv-hooks/index.html#footnotes",
    "title": "virtualenvwrapper hooks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is a wealth of information on what you can do with your dotfiles but that is an article in itself and I’m yet to write it. A useful set of different aliases you could use can be found here↩︎\nActually I create the script in ~/dotfiles/python/postmkvirtualenv and made a symbolic link at ~/.virtualenv/postmkvirtualenv that points to it so that whenever I update or improve this script it is updated across my computers.↩︎"
  },
  {
    "objectID": "posts/running-2022/index.html",
    "href": "posts/running-2022/index.html",
    "title": "Running in 2022",
    "section": "",
    "text": "I started running 🏃 in 2019. It never used to appeal but I wanted to get more regular exercise easily from my doorstep as despite commuting by bike for most of my life and cycling between 12-20km daily it wasn’t enough for me. Whilst I’ve always cycled for commuting and cycling was delightful during the pandemic lockdowns I’ve since enjoyed it less and less as it seems an increasing number of motorists seen to have little to no regard for the safety of pedestrians and cyclists. As a consequence I prefer running over cycling these days and get most of my aerobic exercise on two feet rather than two wheels."
  },
  {
    "objectID": "posts/running-2022/index.html#background",
    "href": "posts/running-2022/index.html#background",
    "title": "Running in 2022",
    "section": "Background",
    "text": "Background\nHaving worked as a statistician (of different sorts) over a number of years I also like data and analysing it. I therefore record my runs (and occasional cycles). A long time ago I used to use Endomondo but didn’t like the idea of sharing my personal location data with a random company so went private and now use the privacy respecting OpenTracks1.\nI use Emacs for most of my work, often using Org-mode which offers literate programming via Org-Babel and so my logged runs/cycles/hikes/climbs are summarised captured into Org files and I’ve written a literate document to process these in R and output to HTML which I host on my VPS. I can view my progress online.\nAt some point early in 2022 I decided to set myself an arbitrary goal of running at least 1200km in the year. It seemed a nice round number at 100km/month. This post summarises the data collected during that period and serves as a learning exercise for me to refresh/improve my R and Quarto knowledge. I used to use R when I worked as a Medical Statistician but for the last five years or so I’ve mainly used Python for work reasons. I’m keen to use Quarto as its a very neat literate programming framework (this blog is written in Quarto). The code chunks are hidden by default but can be easily expanded if you wish to look at them. I’ve summarised some of the features I’ve learnt about Quarto and I’ve explained how I use Emacs Org-mode to capture my runs."
  },
  {
    "objectID": "posts/running-2022/index.html#data",
    "href": "posts/running-2022/index.html#data",
    "title": "Running in 2022",
    "section": "Data",
    "text": "Data\nMy data is captured using org-capture in an ASCII text file (for more on this setup see below), but its wrapped in an org-mode table. For the purpose of this post I have imported, filtered and redacted some of the data saved this to a .RData file having so that it works with the GitHub Action that produces the blog. The following code chunk determines where to load the file from. If I’m working on the file locally the original data is loaded and parsed before saving to .RData. In GitHub pages, the Action (CI/CD) pipeline won’t have access to the original instead it load the .RData file and its good to go with generating graphs and renders correctly.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| label: load-data\nlibrary(dplyr)\nlibrary(ggdark)\nlibrary(ggplot2)\nlibrary(ggridges)\nlibrary(hms)\nlibrary(knitr)\nlibrary(lubridate)\nlibrary(orgutils)\nlibrary(plotly)\nlibrary(readr)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(stringr)\n\ncurrent_env &lt;- Sys.getenv()\ncurrent_env[\"hostname\"] &lt;- Sys.info()[\"nodename\"]\n\nif(current_env[\"hostname\"] == \"kimura\") {\n    # If at home we load and parse the data\n    remove_days &lt;- c(\"Mon \" = \"\", \"Tue \" = \"\", \"Wed \" = \"\", \"Thu \" = \"\", \"Fri \" = \"\", \"Sat \" = \"\", \"Sun \" = \"\")\n    training_dir &lt;- paste(current_env[\"HOME\"], \"org/training/\", sep=\"/\")\n    running_log &lt;- paste(training_dir, \"log/running.org\", sep=\"\")\n\n    data &lt;- orgutils::readOrg(running_log, table.name=\"running-log\")\n    data &lt;- dplyr::tibble(data)\n    data &lt;- data |&gt;\n        mutate(distance = as.double(stringr::str_replace(Distance, \"km\", \"\")),\n               date = stringr::str_replace(Date, \"&lt;\", \"\"),\n               date = stringr::str_replace(date, \"&gt;\", \"\"),\n               date = stringr::str_replace(date, \"\\\\[\", \"\"),\n               date = stringr::str_replace(date, \"\\\\]\", \"\"),\n               date = stringr::str_replace_all(date, remove_days),\n               year_month_day = stringr::str_extract(date, \"[0-9]+-[0-9]+-[0-9]+\"),\n               time = stringr::str_replace(Time, \"min \\\\+ \", \" \"),\n               time = stringr::str_replace(time, \"s\", \"\")) |&gt;\n        dplyr::filter(year_month_day&gt;= ymd(\"2022-01-01\")) |&gt;\n        tidyr::separate(time, c(\"min\", \"sec\")) |&gt;\n        mutate(date = lubridate::ymd_hm(date),\n               year_month = floor_date(date, \"month\"),\n               year_week = floor_date(date, \"week\"),\n               year_day = lubridate::wday(date, week_start=1, label=TRUE, abbr=FALSE),\n               logged_at = hms::as_hms(date),\n               min = as.integer(min),\n               sec = as.integer(sec),\n               hour = floor(min / 60),\n               min = min - (hour * 60),\n               time = lubridate::hms(paste(hour, min, sec, sep=\":\")),\n               time_sec = lubridate::period_to_seconds(time),\n               time_min = lubridate::period_to_seconds(time) / 60,\n               pace = as.numeric(time) / (60 * distance)) |&gt;\n        dplyr::select(-c(Date, Route, Time, Pace, year_month_day, hour, min, sec))\n    # readr::write_csv(data, file=\"running_2022.csv\")\n    save(data, file=\"data.RData\")\n\n} else {\n    # Otherwise we load parsed data.\n    # data &lt;- readr::read_csv(file=\"running_2022.csv\", col_names = TRUE)\n    load(\"data.RData\")\n}\n\nsummary_data &lt;- data  |&gt;\n     dplyr::select(distance, pace, time)  |&gt;\n     dplyr::summarise(across(c(distance, pace, time),\n                             list(sum=sum,\n                                  mean=mean,\n                                  sd=sd,\n                                  median=median,\n##                                  quantile=quantile,\n                                  min=min,\n                                  max=max),\n                             .names = \"{.col}_{.fn}\"))"
  },
  {
    "objectID": "posts/running-2022/index.html#summary",
    "href": "posts/running-2022/index.html#summary",
    "title": "Running in 2022",
    "section": "Summary",
    "text": "Summary\nIn total in 2022 I went out running 146 times and covered a total distance of 1377.2km. Individual runs ranged from 6.43km to 32.62km (mean : 9.4328767km; standard deviation : 3.2130937km; median : 8.69km). The mean pace across all runs was 5.2192349min/km (standard deviation 0.265666; fastest : 4.754306min/km; slowest 7.1990599min/km). The total time I spent running was 4513 (mean : 30.9109589; standard deviation 1249.9327749; median : 30).\nThe longest run I did was the Edale Skyline ( 32.62km in 0.9833333mins) although I started in Hope and went anti-clockwise rather than the “race” circuit of starting in Edale and ascending Ringing Roger then going clockwise. This meant I had slightly less ascent to do. However by around 28-29km coming off of Kinder and passing Hope Cross my legs (well the tendons going into my groin!) were complaining a lot and so I basically walked the remainder up to Win Hill and very tentatively jogged back down into Hope. This data point is a bit of an outlier and so is excluded from some plots. For some reason this value isn’t correctly shown in the table below.\nTODO - Calculate the inter-quartile range, tricky with across() used above? 🤔\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| label: summary-data\n##| tbl-cap-location: top\n##| tbl-cap: Summary statistics for run distances, pace and time.\n\nsummary_data |&gt;\n    reshape2::melt()  |&gt;\n    tidyr::separate(variable, c(\"Metric\", \"Statistic\")) |&gt;\n    dplyr::mutate(Metric = dplyr::recode(Metric, distance=\"Distance (km)\", time=\"Time (min)\", pace=\"Pace (min/km)\"),\n                  Statistic = dplyr::recode(Statistic, sum=\"Total\", mean=\"Mean\", sd=\"SD\", min=\"Min.\", max=\"Max.\",\n                                       median=\"Median\"))  |&gt;\n    tidyr::pivot_wider(names_from=Metric, values_from=value) |&gt;\n    knitr::kable(digits=3)\n\n\n\n\nStatistic\nDistance (km)\nPace (min/km)\nTime (min)\n\n\n\n\nTotal\n1377.200\n762.008\n4513.000\n\n\nMean\n9.433\n5.219\n30.911\n\n\nSD\n3.213\n0.266\n1249.933\n\n\nMedian\n8.690\n5.193\n30.000\n\n\nMin.\n6.430\n4.754\n0.000\n\n\nMax.\n32.620\n7.199\n59.000\n\n\n\n\n\nFor some reason\n\nNumber of Runs\nHow often I go running varies depending on the distance I’ve done recently and more importantly whether I’m carrying an injury of some sort.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| label: number-of-runs\n##| fig-cap-location: top\n##| fig-cap: \"Runs per by Month and Week\"\n##| fig-subcap:\n##|    - \"Runs by month\"\n##|    - \"Runs by week\"\n##| fig-alt: \"A plot of runs per month/week in 2022.\"\nmonth &lt;- data |&gt; ggplot(aes(year_month)) +\n    geom_bar() +\n    dark_theme_bw() +\n    ylab(\"Runs\") +\n    xlab(\"Month/Year\") +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(month)\n\n\n\n\nweek &lt;- data |&gt; ggplot(aes(year_week)) +\n    geom_bar() +\n    dark_theme_bw() +\n    ylab(\"Runs\") +\n    xlab(\"Week\") +\n    scale_x_datetime(breaks = date_breaks(\"2 weeks\")) + ##, labels=date_format(\"%w\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(week)\n\n\n\n\n\n–\n\n\nDistance\nI prefer longer runs, I often feel I’m only getting going and into a rhythm after 3-5km and therefore struggle to find motivation to go out for short runs. That said, whilst I can run &gt; 21km (half-marathon) distances I don’t do so often and past experience tells me that if I run too much I end up injuring myself.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: distance-time-series\n##| fig-cap: \"Distance of Runs.\"\n##| fig-alt: \"A plot of distance of runs in 2022.\"\ndistance_time &lt;- data |&gt;\n    ggplot(aes(date, distance)) +\n    geom_line() +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Date\") +\n    # scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%b\"))\nggplotly(distance_time)\n\n\n\n\n\n\nDistance By Month\nMarch was a fallow month for running as I had a sore knee and a month of work and so I did a lot of cycling (A57 Snake Pass when it was closed, a morning jaunt to Bakewell and back) and DIY around the home and garden (finishing off a patio). I also had to ease off from mid-November and most of December due to a sore thigh.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: true\n##| fig-cap-location: top\n##| label: distance-per-month\n##| fig-cap: \"Distance per month.\"\n##| fig-subcap:\n##|    - \"Bar Chart\"\n##|    - \"Box Plot\"\n##|    - \"Ridge Density\"\n##| fig-alt: \"A plot of distance per month/week in 2022.\"\nbar_distance_month &lt;- data |&gt; ggplot(aes(year_month)) +\n    geom_bar(aes(weight = distance)) +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\")  +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\"))\n    ## theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(bar_distance_month)\n\n\n\n\nbox_distance_month &lt;- data |&gt; ggplot(aes(year_month, distance)) +\n    geom_boxplot(aes(factor(year_month), distance)) +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\")\n    ## scale_x_datetime(breaks = date_breaks(\"1 month\")) + ##, labels=date_format(\"%B\"))\n    ## theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(box_distance_month)\n\n\n\n\nridge_distance_month &lt;- data |&gt; ggplot(aes(x=distance, y=factor(year_month), group=year_month, fill=after_stat(x))) +\n    # geom_density_ridges_gradient(scale=1, gradient_lwd=1.) +\n    geom_density_ridges_gradient(gradient_lwd=1.) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_fill_viridis_c(name=\"Distance (km)\", option=\"C\") +\n    dark_theme_bw() +\n    ylab(\"Year\") +\n    xlab(\"Distance (km)\") ## +\n    ## scale_y_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\"))\n### ggplotly(ridge_distance_month)\nridge_distance_month\n\n\n\n\n\n\n\ndev.off()\n\nnull device \n          1 \n\n\n\n\nDistance By Week\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: distance-per-week\n##| fig-cap: \"Distance per week.\"\n##| fig-subcap:\n##|    - \"Bar Chart\"\n##|    - \"Box Plot\"\n##| fig-alt: \"Plots of distance per month in 2022.\"\nbar_distance_week &lt;- data |&gt; ggplot(aes(year_week)) +\n    geom_bar(aes(weight = distance)) +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\")  +\n    ## scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%w\"))\n    theme(axis.text.x = element_text(angle=30, hjust=1))\n\nggplotly(bar_distance_week)\n\n\n\n\nbox_distance_week &lt;- data |&gt; ggplot(aes(factor(year_week), distance)) +\n    geom_boxplot() +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\") +\n    ## scale_x_datetime(breaks = date_breaks(\"2 weeks\")) + ##, labels=date_format(\"%w\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(box_distance_week)\n\n\n\n\n\n\n\nRuns v Distance\nLets look at how the number of runs per week affects the overall distance covered. Do I keep to a consistent distance if I’m doing less runs by doing longer runs? We can look at that by plotting the number of runs per week against either the total distance or the mean distance for that week. If I do fewer longer runs there should be a downwards trend, with weeks where I only run once having very high values, and weeks where I do multiple runs having very low means.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: runs-distance\n##| fig-cap: \"Runs per week v Distance\"\n##| fig-subcap:\n##|    - Total Distance\n##|    - Mean Distance\n##| fig-alt: \"A plot of number of runs per week and the distanace covered in 2022.\"\ntmp_data &lt;- data |&gt;\n    group_by(year_week) |&gt;\n    summarise(runs = n(),\n              distance_total= sum(distance),\n              distance_mean = mean(distance))\nruns_distance_total&lt;- tmp_data |&gt;\n    ggplot(aes(runs, distance_total)) +\n    geom_point(aes(color=distance_mean, size=distance_mean, alpha=0.45)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Total Distance (km)\") +\n    xlab(\"Runs (km)\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(runs_distance_total, tooltip = c(\"year_week\", \"distance_total\", \"distance_mean\"))\n\n\n\n\nruns_distance_mean &lt;- tmp_data |&gt;\n    ggplot(aes(runs, distance_mean, label=year_week)) +\n    geom_point(aes(color=distance_total, size=distance_total, alpha=0.45)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Mean Distance (km)\") +\n    xlab(\"Runs (km)\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(runs_distance_mean, tooltip = c(\"year_week\", \"distance_total\", \"distance_mean\"))\n\n\n\n\n\n\n\n\nPace\nI like pushing myself and going fast, I perhaps stupidly and against much perceived wisdom, think that if I’m able to hold a conversation then I’m not trying hard enough (in reality I rarely try talking as I always run on my own).\nBefore looking at pace over time its interesting to look at the relationship between distance and time. Obviously longer runs are going to take longer, but is the relationship linear or do I get slower the further I go?\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: distance-v-time\n##| fig-cap: \"Distance v Pace.\"\n##| fig-subcap:\n##|    - All\n##|    - Excluding Outliers\n##| fig-alt: \"Distance v Pace for runs in 2022.\"\ndistance_time_all &lt;- data |&gt;\n    ggplot(aes(distance, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Distance\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(distance_time_all)\n\n\n\n\ndistance_time_excl_outliers &lt;- data |&gt;\n    dplyr::filter(pace &lt; 7) |&gt;\n    ggplot(aes(distance, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Distance\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(distance_time_excl_outliers)\n\n\n\n\n\nThe relationship between distance and pace is interesting, one might expect that the pace decreases with the overall distance, but it depends on the terrain. Most of my shorter runs involved a fair proportion of uphill as I live in the bottom of a valley and my typical circuit takes me up the valley to some extent before turning around and heading back. Longer runs I would typically get out of the valley and run along fairly flat ground before heading back down and I think this is what causes the dip in the above graph (excluding outliers) in the range of 11-14km, but further distances I tire and so my pace drops.\nLooking at pace over time it increases as the year progresses but then runs are getting gradually longer, and I know I changed the route to include more hills. Strangely getting COVID at the end of August didn’t appear to negatively impact my pace although running with an injury late November/December did (I should probably have had a break but wanted to reach my goal so dialled down the frequency and distance).\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: pace-overtime\n##| fig-cap: \"Pace over time by distance.\"\n##| fig-alt: \"A plot of distance per month/week in 2022.\"\nno_outliers &lt;- data |&gt;\n    dplyr::filter(pace &lt; 7)\npace_timeseries&lt;- data |&gt;\n    ggplot(aes(date, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth(method=\"loess\", data=no_outliers) +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Date\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%b\"))\nggplotly(pace_timeseries)\n\n\n\n\n\n\nPace By Month\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: pace-per-month\n##| fig-cap: \"Pace per month.\"\n##| fig-subcap:\n##|    - \"Box Plot\"\n##|    - \"Ridge Density\"\n##| fig-alt: \"A plot of pace per month/week in 2022.\"\nbox_pace_month &lt;- data |&gt; ggplot(aes(factor(year_month), pace)) +\n    geom_boxplot() +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Month/Year\") ## +\n### scale_x_datetime(breaks = date_breaks(\"2 weeks\"), labels=date_format(\"%B\"))\nggplotly(box_pace_month)\n\n\n\n\nridge_pace_month &lt;- data |&gt; ggplot(aes(x=pace, y=factor(year_month), group=year_month, fill=after_stat(x))) +\n    # geom_density_ridges_gradient(scale=1, gradient_lwd=1.) +\n    geom_density_ridges_gradient(scale=1, gradient_lwd=1.) +\n    scale_x_continuous(expand = c(0, 0)) +\n    # scale_y_discrete(expand = expansion(mult = c(0.01, 0.25))) +\n    scale_fill_viridis_c(name=\"Pace (min/km)\", option=\"C\") +\n    dark_theme_bw() +\n    ylab(\"Year\") +\n    xlab(\"Pace (min/km)\") ## +\n###     scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\"))\n### ggplotly(ridge_pace_month)\nridge_pace_month\n\n\n\n\n\n\n\n\n\n\nPace By Week\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: pace-per-week\n##| fig-cap: \"Pace per week.\"\n##| fig-alt: \"Plots of pace per month in 2022.\"\nbox_pace_week &lt;- data |&gt; ggplot(aes(factor(year_week), pace)) +\n    geom_boxplot() +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Month/Year\") +\n    ## scale_x_datetime(breaks = date_breaks(\"2 weeks\"), labels=date_format(\"%w\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(box_pace_week)\n\n\n\n\n\n\n\n\nWhen Do I Run?\nI’m much more a morning person and typically go out running on an empty stomach as I find it quite unpleasant to have a bellyful of food jiggling around inside me. But what days of the week and times do I actually go running? I can answer this with only a low degree of accuracy because whilst I do try to log my runs immediately after having completed them (post-stretching!) I don’t always do so and so the times below reflect the times I logged the run rather than started.\nIn the summer when it gets light early I’ll sometimes go out running at 06:00 but clearly these are not reflected in the logged times.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: when-I-run\n##| fig-cap: \"When I go running.\"\n##| fig-subcap:\n##|    - \"Day\"\n##|    - \"Time\"\n##|    - \"Month v Time\"\n##|    - \"Day v Time\"\n##| fig-alt: \"When I go running\"\nwhat_day_I_run &lt;- data  |&gt; ggplot(aes(year_day)) +\n    geom_bar() +\n    dark_theme_bw() +\n    xlab(\"Day of Week\") +\n    ylab(\"N\")\nggplotly(what_day_I_run)\n\n\n\n\nwhen_I_run &lt;- data |&gt; ggplot(aes(logged_at)) +\n    geom_bar() +\n    dark_theme_bw() +\n    xlab(\"Time of Day\") +\n    ylab(\"N\")\nggplotly(when_I_run)\n\n\n\n\nwhat_time_I_run_by_month &lt;- data |&gt; ggplot(aes(year_month, logged_at)) +\n    geom_point() +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    xlab(\"Month\") +\n    ylab(\"Time of Day\")\nggplotly(what_time_I_run_by_month)\n\n\n\n\nwhat_time_I_run_each_day &lt;- data |&gt; ggplot(aes(year_day, logged_at)) +\n    geom_point() +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    xlab(\"Day of Week\") +\n    ylab(\"Time of Day\")\nggplotly(what_time_I_run_each_day)\n\n\n\n\n\n\nDoes distance differ with time of day?\nIf I go out running in the morning I usually go further because I work five days a week and lunch-time runs have to fit within an hour. As you can see I don’t generally run in the evening as I don’t like exercising with food in my stomach.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: when-I-run-distance\n##| fig-cap: \"Time of Day v Distance.\"\n##| fig-alt: \"Time of day I go running v distance\"\nwhen_I_run_distance &lt;- data |&gt; ggplot(aes(logged_at, distance)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth() +\n    dark_theme_bw() +\n    xlab(\"Time of Day\") +\n    ylab(\"Distance\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") ## +\n    ## scale_x_datetime(breaks = date_breaks(\"1 hour\"), labels=date_format(\"%H\"))\nggplotly(when_I_run_distance)\n\n\n\n\n\n\n\nDoes pace differ with time of day?\nAs I’ve mentioned I somewhat counter-intuitively get a faster mean pace when going further distances. Does this feature come through with the time of day and do I run faster in the morning or at other times. The graph below plots the time of day against the pace with the distance denoted by the size and colour of points.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: when-I-run-pace\n##| fig-cap: \"When I go running v pace.\"\n##| fig-alt: \"Time of day I go running v pace\"\nwhen_I_run_pace &lt;- data |&gt; ggplot(aes(logged_at, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth() +\n    dark_theme_bw() +\n    xlab(\"Time of Day\") +\n    ylab(\"Pace\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") ## +\n    ## scale_x_datetime(breaks = date_breaks(\"1 hour\"), labels=date_format(\"%H\"))\nggplotly(when_I_run_pace)\n\n\n\n\n\nI could go on and on making different types of plots but I think that is sufficient for now."
  },
  {
    "objectID": "posts/running-2022/index.html#capturing-data-in-emacs-org-mode",
    "href": "posts/running-2022/index.html#capturing-data-in-emacs-org-mode",
    "title": "Running in 2022",
    "section": "Capturing Data in Emacs Org-mode",
    "text": "Capturing Data in Emacs Org-mode\nI thought it would be worthwhile including a section on how I capture data. Its not perfect (yet!) because as mentioned I use OpenTracks to log my runs. This saves data to a GPX file on finishing on my phone and I use SyncThing to back these up automatically to my server using a run condition so it only tries to do so when the phone is connected to my home WiFi network and the phone is charging. At some point I will take advantage of this and develop a dashboard that loads the GPX data and allows interactive exploration as well as summarising the data similar to that which I’ve presented here. But life is busy so for now I manually capture the summary statistics using Org-mode and Org Capture templates.\nI have a file ~/path/to/running.org which contains an Org table as shown below.\n##+CAPTION: Running Log\n##+NAME: running-log\n| Date                   | Route                     | Distance | Time         | Pace                           | Notes     |\n|------------------------+---------------------------+----------+--------------+--------------------------------+-----------|\n| [2022-12-25 Sun 10:17] | From here to there.       | 9.17km   | 48min + 06s  | 5 min / km + 14.721919 s / km  | Felt ok   |\n| [2022-12-21 Wed 13:20] | A to B via C              | 9.81km   | 49min + 14s  | 5 min / km + 1.1213048 s / km  | No falls! |\n| [2022-12-17 Sat 09:08] | There and back            | 15.05km  | 79min + 03s  | 5 min / km + 15.149502 s / km  | Not bad   |\n|------------------------+---------------------------+----------+--------------+--------------------------------+-----------|\n##+TBLFM: $5=uconvert($4/$3, (min+s)/km);L\nThe layout is I think self-explanatory and each time I want to log a run I could open the file C-x C-f ~/path/to/running.org, navigate to the top of the file and start entering details manually in a new row. But that is a bit slow and instead I wrote an Org-capture rule to capture runs (and many other things). Writing these was initially a bit tricky (as I’m a slow learner) but I now understand the structure and can quickly add new entries to capture items where I want them to be, saving me time in the long run.\n(use-package org-capture\n         :ensure nil\n         :after org-gtd\n         :config\n         (setq org-default-notes-file (concat org-directory \"/notes.org\"))\n         (setq org-capture-templates\n           '((\"e\" \"Exercise\")\n             (\"er\" \"Logging a run\" table-line (file+olp \"~/path/to/running.org\")\n              \"| %U | %? | km | min + s | | |\" :prepend t)\n             (\"ec\" \"Logging a cycle\" table-line (file+olp \"~/path/to/cycling.org\")\n              \"| %U | %? | km | min + s | | |\" :prepend t)\n             (\"eh\" \"Logging a hike\" table-line (file+olp \"~/path/to/hiking.org\")\n              \"| %U | %? | km | m | min + s| |\" :prepend t)\n             (\"em\" \"Weight & Waist/Hip\" table-line (file+olp \"~/path/to/metrics_weight.org\")\n              \"| %U | %? | | | |\" :prepend t)\n             (\"es\" \"Steps\" table-line (file+olp \"~/path/to/metrics_steps.org\")\n              \"| %t | %? |\" :prepend t)\n             (\"eb\" \"Blood\" table-line (file+olp \"~/path/to/metrics_blood.org\")\n              \"| %U | %? | | | | | |\" :prepend t))))\nTo enter Org-capture its C-c c this brings up a menu for all of the capture templates I’ve defined…\nSelect a capture template\n=========================\n\n[E]  Email\n[a]  Agenda\n[e]  Exercise\n[w]  Work\nExercise is entered by pressing e as defined on the line (\"e\" \"Exercise\"). I then see a sub-menu…\nSelect a capture template\n=========================\n\ne [r]  Logging a run\ne [c]  Logging a cycle\ne [h]  Logging a hike\ne [m]  Weight & Waist/Hip\ne [s]  Steps\ne [b]  Blood\n…and I can choose what activity to log, hit r for a run and a new buffer appears, the date and time is entered automatically because that field is set to be %U in the template. The cursor is located in the Route column because the field content is %? which means user input is required. The Distance, Time and Notes fields are also completed although the Pace field should be left blank since the formula at the bottom of the table (#+TBLFM: $5=uconvert($4/$3, (min+s)/km);L) calculates this automatically on saving.\nCapture buffer, Finish 'C-c C-c', refile 'C-c C-w', abort 'C-c C-k'\n| [2022-12-27 Tue 11:29] | Out for a run | 10.8km | 54min + 12s | | Fun run in the snow. |\nOnce all the fields are completed press C-c C-c to save the changes and the row is added to the table in the file ~/path/to/running.org. This file forms part of my training.org (its pulled into the main document training.org using #+INCLUDE: ~/path/to/running.org), but the data is used and processed using R. How does the data get from the org-formatted table into the R session as a data frame for summarising and using? This is part of the amazing magic that is Org-babel for literate programming. A source code chunk can be defined with a :var option which refers to the table you want to include. In this my source block does some processing of the table to tidy up the dates into something R understands and is shown below.\n##+begin_src R :session *training-R* :eval yes :exports none :var running_table=running-log  :colnames nil :results output silent\n  running_table %&lt;&gt;% mutate(distance = as.double(str_replace(Distance, \"km\", \"\")),\n             time = str_replace(Time, \"min \\\\+ \", \" \"),\n             time = str_replace(time, \"s\", \"\"),\n             Date = str_extract(Date, \"[0-9]+-[0-9]+-[0-9]+\"),\n             date = ymd(Date),\n             year = floor_date(date, \"year\"),\n             year_month = floor_date(date, \"month\"),\n             year_week = floor_date(date, \"week\")) %&gt;%\n      separate(time, c(\"min\", \"sec\")) %&gt;%\n      mutate(min = as.integer(min),\n             sec = as.integer(sec),\n             hour = floor(min / 60),\n             min = min - (hour * 60),\n             # time = chron(time=paste(hour, min, sec, sep=\":\")),\n             time = hms(paste(hour, min, sec, sep=\":\")),\n             pace = as.numeric(time) / (60 * distance)) %&gt;%\n             # pace = Pace) %&gt;%\n      select(-c(Date, Distance, Time, Pace, hour, min, sec))\n##+end_src\nThe key to getting the Org-mode table (which has the #+NAME: running-log) into the R session (which is set to :session *training-R* and is evaluated :eval yes) is the option :var running_table=running-log which makes the table available in the R session as the dataframe running_table. As you’ll see from the very first code chunk at the top of this post because this document is written in Quarto I instead use the orgutils::readOrg() package/function to read the table into R directly."
  },
  {
    "objectID": "posts/running-2022/index.html#quarto---things-ive-learnt",
    "href": "posts/running-2022/index.html#quarto---things-ive-learnt",
    "title": "Running in 2022",
    "section": "Quarto - Things I’ve learnt",
    "text": "Quarto - Things I’ve learnt\nSome things I’ve learnt about Quarto whilst preparing this document.\n\nCode Folding\nIt should be possible to set options at the global level by setting the following in the site index.qmd.\nexecute:\n  code-fold: true\n  code-tools: true\n  code-link: true\nI’m using the blogging feature of Quarto but adding this to the site index.qmd didn’t work when previewed locally. I tried adding it to the YAML header for the post itself (i.e. posts/running-2022/index.qmd) but no joy, the code chunks were still displayed. I could however set this on a per-chunk basis though so each code chunk carries the options.\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n\n\nTable and Figure caption locations are should be configurable\nCaptions were by default underneath each picture which is perhaps ok when reading as a PDF but this is rendered as HTML and I would prefer these to be at the top so that readers see the heading as they scroll down (I’m often torn about figure headings and labels and feel they should be included in the image itself so they are retained if/when they are used elsewhere).\nFortunately you can specify the location of table and figure captions. Unfortunately this doesn’t appear to render correctly when using the blogging feature and all captions are still at the bottom.\n##| fig-cap-location: [top|bottom|margin]\n##| tbl-cap-location: [top|bottom|margin]\n\n\nTwo for the price of one\nIts possible to include two or more sub-figures in a code chunk and have them both displayed.\n##| label: pace-per-month\n##| fig-cap: \"Pace per month.\"\n##| fig-subcap:\n##|    - \"Bar Chart\"\n##|    - \"Box Plot\"\n##| fig-alt: \"A plot of pace per month/week in 2022.\"\nIn the output format of the blog these do not appear side by side, but rather underneath each other.\n\n\nPlotly plays with Quarto\nUsing the plotly R package with Quarto “Just Works”, the plots render nicely in the page and are zoom-able with tool-tips appearing over key points.\n\n\nSome graphs appear where I don’t expect them to\nAstute readers will notice that some of the ridge plot graphs appear more than once. I couldn’t work out why this was, the code does not specify that they should be shown again. For example the Ridge Density plot for total Distance by Month also appeared under the total Distance by Week. To try working around this I attempted to explicitly use dev.off() after the initial generation of the first Ridge Density Plot, but this had the undesired effect of including output from the call and an additional code-chunk. Not one I’ve sorted yet. 🤔\n\n\nEmoji’s\nTo include emoji’s in the Markdown it’s necessary to add the following to the header of the specific page (i.e. posts/running_2022/index.qmd)\nfrom: markdown+emoji\nText based emoji names (e.g. :thinking: 🤔 ; :snake 🐍 ; :tada: 🎉) are then automatically included when rendering."
  },
  {
    "objectID": "posts/running-2022/index.html#r---things-ive-forgotten-remembered",
    "href": "posts/running-2022/index.html#r---things-ive-forgotten-remembered",
    "title": "Running in 2022",
    "section": "R - Things I’ve forgotten remembered",
    "text": "R - Things I’ve forgotten remembered\nIts been a few years since I used R on a daily basis. As a consequence I thought I’d forgotten a bunch of things I used to know how to do, but this exercise has reminded me that I perhaps have some vestigial remnants of my old knowledge lingering. Some things I had to look up and, unsurprisingly, the available tools, functions/methods have evolved in that time (viz. tidyr is now more generic and simpler than reshape2).\n\nFormatting Dates/Times in Axes\nI’ve a few more niggles to round out such as the formatting of the months/weeks which I should probably do up-front in the dataframe rather leaving them as POSIXct objects as then the and then the ggplot2 functions scale_x_datetime() can be used directly (in some places I convert to factors which doesn’t help).\n\n\nPreviewing Graphs\nOne very nice feature I discovered recently courtesy of a short video by Bruno Rodrigues (thanks Bruno 👍) is the ability to preview graphs in the browser. This is unlikely to be something you need if you use RStudio but as with most things I use Emacs and Emacs Speaks Statistics and so normally I get an individual X-window appearing showing the plot. Instead we can use the httpgd package to start web-server that renders the images. It keeps a history of lots that have been produced and you can scroll back and forth through them.\n&gt; httpgd::hgd()\nhttpgd server running at:\n  http://127.0.0.1:42729/live?token=beRmYcSn\nThen just create your plots and hey-presto the graphs appear at the URL. 🧙\n\n\nrenv\nIn order to have this page publish correctly I had to initialise a renv within the repository and include the renv.lockfile so that the GitHub action/workflow installed all of the packages I use and their dependencies in the runner that renders the blog.\nWhilst I’m familiar with virtual environments under Python this is something relatively new to me for R. I won’t write much other than the process involved initialising the environment, installing the dependencies then updating the lockfile.\n&gt; renv::init()\n&gt; install.packages(c(\"dplyr\", \"ggdark\", \"ggridges\", \"hms\", \"knitr\", \"lubridate\", \"orgutils\", \"plotly\", \"readr\",\n                     \"scales\", \"stringr\"))\n&gt; renv::snapshot()\n&gt; q()\ngit add renv.lockfile"
  },
  {
    "objectID": "posts/running-2022/index.html#conclusion",
    "href": "posts/running-2022/index.html#conclusion",
    "title": "Running in 2022",
    "section": "Conclusion",
    "text": "Conclusion\nIts been a fun exercise digging back into R and learning more about Quarto. If you’ve some data and an urge to summarise it I’d recommend having a play, although since Quarto supports more than just R you could alternatively use Python packages such as Pandas and Matplotlib to achieve the same. Here’s to more running 🏃 and maybe finding the time to write the Shiny dashboard to do a lot of this automatically and on the fly as I log runs in 2023 and beyond."
  },
  {
    "objectID": "posts/running-2022/index.html#links",
    "href": "posts/running-2022/index.html#links",
    "title": "Running in 2022",
    "section": "Links",
    "text": "Links\n\nTraining Summary\nEmacs\nOrgMode\nOrg-babel\nR\nEmacs Speaks Statistics\nQuarto\nquarto-emacs mode\n\nReturn to the top!"
  },
  {
    "objectID": "posts/running-2022/index.html#footnotes",
    "href": "posts/running-2022/index.html#footnotes",
    "title": "Running in 2022",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nConverting existing logs from Endomondo which were in TCX prompted me to write the Python package tcx2gpx to convert these to GPX which I could import to OpenTracks↩︎"
  },
  {
    "objectID": "posts/pre-commit/index.html",
    "href": "posts/pre-commit/index.html",
    "title": "Pre-Commit : Protecting your future self",
    "section": "",
    "text": "Pre-commit is a powerful tool for executing a range of hooks prior to making commits to your Git history. This is useful because it means you can automatically run a range of linting tools on your code across an array of languages to ensure your code is up-to-scratch before you make the commit."
  },
  {
    "objectID": "posts/pre-commit/index.html#background",
    "href": "posts/pre-commit/index.html#background",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Background",
    "text": "Background\nPre-commit is written in Python but that isn’t a limitation as it will lint YAML, JSON, C, JavaScript, Go, Rust, TOML, Terraform, Jupyter Notebooks, and so on. The list of supported hooks is vast.\nFor those unfamiliar with version control and Git in particular this will likely all sound alien. If you are new to the world of version control and Git I can highly recommend the Git & Github through GitKraken Client - From Zero to Hero! course offered by the Research Software Engineering at the University of Sheffield and developed by Alumni Anna Krystalli.\n\nWhat is a “hook”?\nIn computing a “hook” refers to something that is run prior to or in response to a requested action. In the context of the current discussion we are talking about hooks that relate to actions undertaken in Git version control and specifically actions that are run before a “commit” is made.\nWhen you have initialised a directory to be under Git version control the settings and configuration are stored in the .git/ sub-directory. There is the .git/config file for the repositories configuration but also the .git/hooks/ directory that is populated with a host of *.sample files with various different names that give you an in-road into what different hooks you might want to run. Its worth spending a little time reading through these if you haven’t done so yet as they provide useful examples of how various hooks work.\n\n\nWhy pre-commit hooks?\nTypically when writing code you should lint your code to ensure it conforms to agreed style guides and remove any “code smells” that may be lingering (code that violates design principles). It won’t guarantee that your code is perfect but its a good starting point to improving it. People who write a lot of code have good habits of doing these checks manually prior to making commits. Experienced coders will have configured their Integrated Development Environment (IDE) to apply many such “hooks” on saving a file they have been working on.\nAt regular points in your workflow you save your work and check it into Git by making a commit and that is where pre-commit comes in to play because it will run all the hooks it has been configured to run against the files you are including in your commit. If any of the hooks fail then your commit is not made. In some cases pre-commit will automatically correct the errors (e.g. removing trailing white-space; applying black formatting if configured) but in others you have to correct them yourself before a commit can be successfully made.\nInitially this can be jarring, but it saves you, and more importantly those who you are asking to review your code, time and effort. Your code meets the required style and is a little bit cleaner before being sent out for review. Long term linting your code is beneficial (see Linting - What is all the fluff about?)."
  },
  {
    "objectID": "posts/pre-commit/index.html#installation",
    "href": "posts/pre-commit/index.html#installation",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Installation",
    "text": "Installation\nPre-commit is written in Python and so you will need Python installed on your system in order to use it. Aside from that there is little else extra that is required to be manually installed as pre-commit installs virtual environments specific for each enabled hook.\nMost systems provide pre-commit in their package management system but typically you should install pre-commit within your virtual environment or under your user account.\npip install pre-commit\nconda install -c conda-forge pre-commit\nIf you are working on a Python project then you should include pre-commit as a requirement (either in requirements-dev.txt) or under the dev section of [options.extras_require] in your setup.cfg as shown below.\n[options.extras_require]\ndev =\n  pre-commit\n  pytest\n  pytest-cov"
  },
  {
    "objectID": "posts/pre-commit/index.html#configuration",
    "href": "posts/pre-commit/index.html#configuration",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Configuration",
    "text": "Configuration\nConfiguration of pre-commit is via a file in the root of your Git version controlled directory called .pre-commit-config.yaml. This file should be included in your Git repository, you can create a blank file or pre-commit can generate a sample configuration for you.\n# Empty configuration\ntouch .pre-commit-config.yaml\n# Auto-generate basic configuration\npre-commit sample-config &gt; .pre-commit-config.yaml\ngit add .pre-commit-config.yaml\n\nHooks\nEach hook is associated with a repository (repo) and a version (rev) within it. Many are available from the https://github.com/pre-commit/pre-commit-hooks. The default set of pre-commit hooks might look like the following.\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n      rev: v4.3.0 # Use the ref you want to point at\n      hooks:\n          - id: trailing-whitespace\n            types: [file, text]\n          - id: check-docstring-first\n          - id: check-case-conflict\n          - id: end-of-file-fixer\n            types: [python]\n          - id: requirements-txt-fixer\n          - id: check-yaml\n\n\nHooks from External Repositories\nSome hooks are available from dedicated repositories, for example the following runs Black, Flake8 and Pylint on your code and should follow under the above (with the same level of indenting to be valid YAML).\n  - repo: https://github.com/psf/black\n    rev: 22.6.0\n    hooks:\n        - id: black\n          types: [python]\n\n  - repo: https://gitlab.com/pycqa/flake8.git\n    rev: 3.9.2\n    hooks:\n        - id: flake8\n          additional_dependencies: [flake8-print]\n          types: [python]\n  - repo: https://github.com/pycqa/pylint\n    rev: v2.15.3\n    hooks:\n        - id: pylint\nAn extensive list of supported hooks is available. It lists the repository from which the hook is derived along with its name.\n\n\nLocal Hooks\nYou can also define new hook and configure them under the - repo: local.\n  - repo: local\n    hooks:\n      - id: &lt;id&gt;\n        name: &lt;descriptive name&gt;\n        language: python\n        entry:\n        types: [python]\nFor some examples of locally defined hooks see the Pandas .pre-commit-config.yaml."
  },
  {
    "objectID": "posts/pre-commit/index.html#usage",
    "href": "posts/pre-commit/index.html#usage",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Usage",
    "text": "Usage\nBefore pre-commit will run you need to install it within your repository. This puts the file .git/hooks/pre-commit in place that contains the hooks you have configured to run. To install this you should have your .pre-commit-config.yaml in place and then run the following.\npre-commit install\nOnce installed and configured there really isn’t much to be said for using pre-commit, just make commits and before you can make a successful commit pre-commit must run with all the hooks you have configured passing. By default pre-commit only runs on files that are staged and ready to be committed, if you have unstaged files these will be stashed prior to running the pre-commit hook and restored afterwards. Should you wish to run these manually without making a commit then, after activating a virtual environment if you are using one, simply make a git commit or you can run.\npre-commit run\nIf any of the configured hooks fail then the commit will not be made. Some hooks such as black may reformat files in place and you can then make another commit recording those changes and the hook should pass. Its important to pay close attention to the output.\nIf you want to run a specific hook you simply add the &lt;id&gt; after run.\npre-commit run &lt;id&gt;\nOr if you want to force running against all files (except unstaged ones) you can do so.\npre-commit run --all-files # Across all files/hooks\nAnd these two options can be combined to run a specific hook against all files.\npre-commit run &lt;id&gt; --all-files\nYou may find that you wish to switch branches to work on another feature or fix a bug but that your current work doesn’t pass the pre-commit and you don’t wish to sort that out immediately. The solution to this is to use git stash to temporarily save your current uncommitted work and restore the working directory and index to its previous state. You are then free to switch branches and work on another feature or fix a bug, commit and push those changes and then switch back.\nImagine you are working on branch a but are asked to fix a bug on branch b. You go to commit your work but find that a does not pass pre-commit but you wish to work on b anyway. Starting on branch a you stash your changes, switch branches, make and commit your changes to branch b then switch back to a and unstash your work there.\ngit stash\ngit checkout b\n... # Work on branch b\ngit add &lt;changed_files_on_branch_b&gt;\ngit commit -m \"Fixing bug on branch b\"\ngit push\ngit checkout a\ngit stash apply"
  },
  {
    "objectID": "posts/pre-commit/index.html#updating",
    "href": "posts/pre-commit/index.html#updating",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Updating",
    "text": "Updating\nYou can update hooks locally by running pre-commit autoupdate. This will update your .pre-commit-config.yaml with the latest version of repositories you have configured and these will run both locally and if you use CI/CD as described below. However this will not update any packages that are part of the - repo: local that you may have implemented and it is your responsibility to handle these."
  },
  {
    "objectID": "posts/pre-commit/index.html#pre-commit-cicd",
    "href": "posts/pre-commit/index.html#pre-commit-cicd",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Pre-commit CI/CD",
    "text": "Pre-commit CI/CD\nIdeally contributors will have setup their system to work with pre-commit and be running such checks prior to making pushes. It is however useful to enable running pre-commit as part of your Continuous Integration/Development pipeline (CI/CD). This can be done with both GitLab and GitHub although similar methods are available for many continuous integration systems.\n\nGitHub\nGitHub actions reside in the .github/workflows/ directory of your project. A simple pre-commit action is available on the Marketplace at pre-commit/action. Copy this template to .github/workflows/pre-commit.yml and include it in your Git repository.\ngit add .github/workflows/pre-commit.yml\ngit commit -m \"Adding pre-commit GitHub Action\" && git push\n\n\nGitLab\nIf you use GitLab the following article describes how to configure a CI job to run as part of your repository.\n\nHow to use pre-commit to automatically correct commits and merge requests with GitLab CI"
  },
  {
    "objectID": "posts/pre-commit/index.html#links",
    "href": "posts/pre-commit/index.html#links",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Links",
    "text": "Links\n\nPre-commit\nSupported hooks\nGitHub Action\nGitLab CI"
  },
  {
    "objectID": "posts/git-worktrees/index.html",
    "href": "posts/git-worktrees/index.html",
    "title": "Git - Worktrees",
    "section": "",
    "text": "A common task in Git workflow is the need to switch/checkout branches and check work that has been done on another branch, running tests, perhaps contributing to code that is out for Pull/Merge Review. If you’ve work in progress on your own branch this means either making a commit or stashing the work to come back to at a later date. Neither of these are particularly problematic as you can git pop stashed work to restore it or git commit --amend, or git commit --fixup and squash commits to maintain small atomic commits and avoid cluttering up the commit history with commits such as “Saving work to review another branch”. But, perhaps unsurprisingly, Git has another way of helping your workflow in this situation. Rather than having branches you can use “worktrees”.\nNormally when you’ve git clone’d a repository all configuration files for working with the repository are saved to the repository directory under .git and all files in their current state on the main branch are also copied to the repository directory. If we clone the pytest-examples directory we can look at its contents using tree -afHD -L 2 (this limits the depth as we don’t need to look deep inside the .git or mypy directories which contain lots of files).\nLets create the contributing branch\nIf we want to switch branches without making a commit but save our work in progress as we want to add more to the CONTRIBUTING.md file later we can stash the changes with a message. We then switch to main and create a new branch (citation) for and add a CITATION.cff file.\nWhen we are ready to return to our contributing branch we can switch and git pop the work we stashed. By default the last stash is popped, but its possible to view all the stashes and select which you wish to pop and restore to the current branch."
  },
  {
    "objectID": "posts/git-worktrees/index.html#worktrees-rather-than-branches",
    "href": "posts/git-worktrees/index.html#worktrees-rather-than-branches",
    "title": "Git - Worktrees",
    "section": "Worktrees rather than branches",
    "text": "Worktrees rather than branches\nWorktrees take a different approach to organising branches. They start with a --bare clone of the repository which implies the --no-checkout flag and means that the files that would normally be found under the &lt;repository&gt;/.git directory are copied but are instead placed in the top level of the directory rather than under .git/. No tracked files are copied as they may conflict with these files. You have all the information Git has about the history of the repository and the different commits and branches but none of the actual files.\nNB If you don’t explicitly state a target directory to clone to it will be the repository name suffixed with .git, i.e. in this example pytest-examples.git. I recommend sticking with the convention of using the same repository name so will explicitly state it.\ncd ..\nmv pytest-examples pytest-examples-orig-clone\ngit clone --bare git@github.com:ns-rse/pytest-examples.git pytest-examples\ncd pytest-examples\ntree -afhD -L 2\n[4.0K Mar 13 07:45]  .\n├── [ 129 Mar 13 07:45]  ./config\n├── [  73 Mar 13 07:45]  ./description\n├── [  21 Mar 13 07:45]  ./HEAD\n├── [4.0K Mar 13 07:45]  ./hooks\n│   ├── [ 478 Mar 13 07:45]  ./hooks/applypatch-msg.sample\n│   ├── [ 896 Mar 13 07:45]  ./hooks/commit-msg.sample\n│   ├── [4.6K Mar 13 07:45]  ./hooks/fsmonitor-watchman.sample\n│   ├── [ 189 Mar 13 07:45]  ./hooks/post-update.sample\n│   ├── [ 424 Mar 13 07:45]  ./hooks/pre-applypatch.sample\n│   ├── [1.6K Mar 13 07:45]  ./hooks/pre-commit.sample\n│   ├── [ 416 Mar 13 07:45]  ./hooks/pre-merge-commit.sample\n│   ├── [1.5K Mar 13 07:45]  ./hooks/prepare-commit-msg.sample\n│   ├── [1.3K Mar 13 07:45]  ./hooks/pre-push.sample\n│   ├── [4.8K Mar 13 07:45]  ./hooks/pre-rebase.sample\n│   ├── [ 544 Mar 13 07:45]  ./hooks/pre-receive.sample\n│   ├── [2.7K Mar 13 07:45]  ./hooks/push-to-checkout.sample\n│   ├── [2.3K Mar 13 07:45]  ./hooks/sendemail-validate.sample\n│   └── [3.6K Mar 13 07:45]  ./hooks/update.sample\n├── [4.0K Mar 13 07:45]  ./info\n│   └── [ 240 Mar 13 07:45]  ./info/exclude\n├── [4.0K Mar 13 07:45]  ./objects\n│   ├── [4.0K Mar 13 07:45]  ./objects/info\n│   └── [4.0K Mar 13 07:45]  ./objects/pack\n├── [ 249 Mar 13 07:45]  ./packed-refs\n└── [4.0K Mar 13 07:45]  ./refs\n    ├── [4.0K Mar 13 07:45]  ./refs/heads\n    └── [4.0K Mar 13 07:45]  ./refs/tags\n\n9 directories, 19 files\nWhat use is that? Well from this point you can instead of using git branch use git worktree add &lt;branch_name&gt; and it will create a directory with the name of the branch which holds all the files in their current state on that branch.\ngit worktree add main\nPreparing worktree (checking out 'main')\nHEAD is now at 2f7c382 Merge pull request #6 from ns-rse/ns-rse/tidy-print\ntree -afhD -L 2 main/\n[4.0K Mar 13 08:13]  main\n├── [  64 Mar 13 08:13]  main/.git\n├── [4.0K Mar 13 08:13]  main/.github\n│   └── [4.0K Mar 13 08:13]  main/.github/workflows\n├── [3.0K Mar 13 08:13]  main/.gitignore\n├── [1.0K Mar 13 08:13]  main/LICENSE\n├── [ 293 Mar 13 08:13]  main/.markdownlint-cli2.yaml\n├── [1.7K Mar 13 08:13]  main/.pre-commit-config.yaml\n├── [ 18K Mar 13 08:13]  main/.pylintrc\n├── [4.8K Mar 13 08:13]  main/pyproject.toml\n├── [4.0K Mar 13 08:13]  main/pytest_examples\n│   ├── [1.3K Mar 13 08:13]  main/pytest_examples/divide.py\n│   ├── [ 179 Mar 13 08:13]  main/pytest_examples/__init__.py\n│   └── [ 491 Mar 13 08:13]  main/pytest_examples/shapes.py\n├── [ 602 Mar 13 08:13]  main/README.md\n└── [4.0K Mar 13 08:13]  main/tests\n    ├── [ 681 Mar 13 08:13]  main/tests/conftest.py\n    ├── [1.7K Mar 13 08:13]  main/tests/test_divide.py\n    └── [1.6K Mar 13 08:13]  main/tests/test_shapes.py\n\n5 directories, 14 files\nEach branch can have a worktree added for it and then when you want to switch between them its is simply a case of cding into the worktree (/branch) you wish to work on. You use Git commands within the directory to apply them to that branch and Git keeps track of everything in the usual manner.\nLets create two worktree’s, the contributing and citation we created above when working with branches.\ngit worktree add contributing\ngit worktree add citation\nYou are now free to move between worktrees (/branches) and undertake work on each without having to git stash or git commit work in progress. We can add the CONTRIBUTING.md to the contributing worktree then jump to the citation worktree and add the CITATION.cff\ncd contributing\necho \"# Contributing\\n\\nContributions to this repository are welcome via Pull Requests.\" &gt; CONTRIBUTING.md\ncd ../citation\necho \"cff-version: 1.2.0\\ntitle: Pytest Examples\\ntype: software\" &gt; CITATION.cff\nNeither branches have had the changes committed so Git will not show any differences between them, but we can use diff -qr to compare the directories.\n diff -qr contributing citation\nOnly in citation: CITATION.cff\nOnly in contributing: CONTRIBUTING.md\nFiles contributing/.git and citation/.git differ\nIf we commit the changes to each we can git diff them.\ncd contributing\ngit add CONTRIBUTING.md\ngit commit -m \"Adding basic CONTRIBUTING.md\"\ncd ../citation\ngit add CITATION.cff\ngit commit -m \"Adding basic CITATION.cff\"\ngit diff citation contributing\nCITATION.cff --- Text\n1 cff-version: 1.2.0\n2 title: Pytest Examples\n3 type: software\n\nCONTRIBUTING.md --- Text\n1 # Contributing\n2\n3 Contributions to this repository are welcome via Pull Requests\nNB The output of git diff may depend on the difftool that you have configured, I use and recommend the brilliant difftastic which has easy integration with Git.\n\nListing Worktrees\nJust as you can git branch --list you can git worktree list\ngit worktree list\n/mnt/work/git/hub/ns-rse/pytest-examples               (bare)\n/mnt/work/git/hub/ns-rse/pytest-examples/citation      19ff076 [citation]\n/mnt/work/git/hub/ns-rse/pytest-examples/contributing  ad56b91 [contributing]\n/mnt/work/git/hub/ns-rse/pytest-examples/main          2f7c382 [main]\n\n\nMoving Worktrees\nYou can move worktrees to different directories, these do not even have to be within the bare repository that you cloned as Git keeps track of these in the worktrees/ directory which has a folder for each of the worktrees you create and the file gitdir points to the location of that particular worktree.\ncd pytest-examples   # Move to the bare repository\ntree -afhD -L 2 worktrees\n[4.0K Mar 13 09:27]  worktrees\n├── [4.0K Mar 13 09:31]  worktrees/citation\n│   ├── [  26 Mar 13 09:31]  worktrees/citation/COMMIT_EDITMSG\n│   ├── [   6 Mar 13 09:27]  worktrees/citation/commondir\n│   ├── [  55 Mar 13 09:27]  worktrees/citation/gitdir\n│   ├── [  25 Mar 13 09:27]  worktrees/citation/HEAD\n│   ├── [1.4K Mar 13 09:31]  worktrees/citation/index\n│   ├── [4.0K Mar 13 09:27]  worktrees/citation/logs\n│   ├── [   0 Mar 13 09:31]  worktrees/citation/MERGE_RR\n│   ├── [  41 Mar 13 09:27]  worktrees/citation/ORIG_HEAD\n│   └── [4.0K Mar 13 09:27]  worktrees/citation/refs\n├── [4.0K Mar 13 09:30]  worktrees/contributing\n│   ├── [  29 Mar 13 09:30]  worktrees/contributing/COMMIT_EDITMSG\n│   ├── [   6 Mar 13 09:27]  worktrees/contributing/commondir\n│   ├── [  59 Mar 13 09:27]  worktrees/contributing/gitdir\n│   ├── [  29 Mar 13 09:27]  worktrees/contributing/HEAD\n│   ├── [1.4K Mar 13 09:30]  worktrees/contributing/index\n│   ├── [4.0K Mar 13 09:27]  worktrees/contributing/logs\n│   ├── [   0 Mar 13 09:30]  worktrees/contributing/MERGE_RR\n│   ├── [  41 Mar 13 09:27]  worktrees/contributing/ORIG_HEAD\n│   └── [4.0K Mar 13 09:27]  worktrees/contributing/refs\n└── [4.0K Mar 13 08:13]  worktrees/main\n    ├── [   6 Mar 13 08:13]  worktrees/main/commondir\n    ├── [  51 Mar 13 08:13]  worktrees/main/gitdir\n    ├── [  21 Mar 13 08:13]  worktrees/main/HEAD\n    ├── [1.3K Mar 13 08:13]  worktrees/main/index\n    ├── [4.0K Mar 13 08:13]  worktrees/main/logs\n    ├── [  41 Mar 13 08:13]  worktrees/main/ORIG_HEAD\n    └── [4.0K Mar 13 08:13]  worktrees/main/refs\n\n10 directories, 19 files\nIf we look at the gitdir file in each worktree sub-directory we see where they point to.\ncat worktrees/*/gitdir\n/mnt/work/git/hub/ns-rse/pytest-examples/citation/.git\n/mnt/work/git/hub/ns-rse/pytest-examples/contributing/.git\n/mnt/work/git/hub/ns-rse/pytest-examples/main/.git\nThese mirror the locations reported by git worktree list, albeit with .git appended.\nIf you want to move a worktree you can do so, here we move citation to ~/tmp.\ngit worktree move citation ~/tmp\n\n\nRemoving worktrees\nIt’s simple to remove a worktree after the changes have been merged or it is no longer needed, make sure to “prune” the tree after having done so.\ngit worktree remove citation\ngit worktree prune\ngit worktree list\n/mnt/work/git/hub/ns-rse/pytest-examples               (bare)\n/mnt/work/git/hub/ns-rse/pytest-examples/contributing  ad56b91 [contributing]\n/mnt/work/git/hub/ns-rse/pytest-examples/main          2f7c382 [main]"
  },
  {
    "objectID": "posts/git-worktrees/index.html#conclusion",
    "href": "posts/git-worktrees/index.html#conclusion",
    "title": "Git - Worktrees",
    "section": "Conclusion",
    "text": "Conclusion\nGit Worktrees are a useful way of structuring your Git workflows if you have to switch branches regularly. They avoid the need to stash work in progress or make commits. If you do choose to use worktrees as an alternative to branches be mindful that you should remove and prune them after you have finished with them, particularly if you have a large codebase."
  },
  {
    "objectID": "posts/git-worktrees/index.html#links",
    "href": "posts/git-worktrees/index.html#links",
    "title": "Git - Worktrees",
    "section": "Links",
    "text": "Links\n\nGit - git-worktree Documentation\nExperiment on your code freely with Git worktree | Opensource.com"
  },
  {
    "objectID": "posts/linting/index.html",
    "href": "posts/linting/index.html",
    "title": "Linting - What is all the fluff about?",
    "section": "",
    "text": "NB This article originally appeared on RSE University of Sheffield but is updated here.\nIf you’ve been dabbling in programming for a while you may have heard of “linting your code” which is a process of static code analysis to remove the “fluff” from your code. Just as physically linting your clothes removes unwanted fluff, linting your code removes “fluff” and can help…\nThis helps reduce the technical debt which impacts the amount of time required for maintenance and further development of a code base. The main focus of this article is the use of linting to ensure consistent coding style, it focuses on Python under Linux but similar tools are available for other operating systems and languages."
  },
  {
    "objectID": "posts/linting/index.html#style-matters",
    "href": "posts/linting/index.html#style-matters",
    "title": "Linting - What is all the fluff about?",
    "section": "Style Matters",
    "text": "Style Matters\nWhat has style got to do with writing code? Trends come and go in fashion but coding styles are meant to be relatively static and not change with the season, although they can and do evolve over time. This is because using a consistent and widely used style when writing code makes it easier for other people, often your future self, to read and understand the code you have written. If code is easier to understand then its easier to modify, update, extend, improve and in general maintain.\nA useful insight from Gudio van Rossum, the creator of Python is that “code is read much more often than it is written” and so it should be easy to understand and not obfuscate its intent. Python is quite good for this as it is an expressive language which encourages coders to be explicit when naming variables, functions, classes and so forth so that their purpose and intention is clear, although the same is true of most modern languages. However, going a step further and using consistent styles to format and layout code helps enhance this."
  },
  {
    "objectID": "posts/linting/index.html#linting-in-python",
    "href": "posts/linting/index.html#linting-in-python",
    "title": "Linting - What is all the fluff about?",
    "section": "Linting in Python",
    "text": "Linting in Python\nThe most widely used Python style is defined in the long established PEP 8: The Style Guide for Python Code. There are a number of tools available that will lint your Python code for you and most integrate with your IDE, whether that is Visual Studio Code, PyCharm or Emacs. Some of the formatting and linting tools available for Python are…\n\nPylint - checks for errors in Python code, tries to enforce a coding standard and looks for code smells.\nYAPF - takes the code and reformats it to the best formatting that conforms to the style guide.\nBlack - The Uncompromising Code Formatter\nFlake8 - Your Tool For Style Guide Enforcement\nProspector - Python Static Analysis\nmypy - Optional Static Typing for Python\n\nHere we will work through linting and formatting the simple file below (available as a download here) using PyLint and Black.\nimport numpy as np\nfrom pathlib import Path\nfrom typing import Union\nimport csv\n\ndef save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -&gt; None:\n    \"\"\"Save a list of random numbers (floats) to the given file.\n\n    The stated number of random numbers will be saved to the given target file, if the directory structure\n    doesn't exist it will be created. Output will by default be over-written.\n    Parameters\n    ----------\n    size : int\n        Number of random numbers to generate\n    seed: int\n        Seed for random number generation\n    save_as : Union[str, Path]\n        Directory/file to save numbers to.\n    \"\"\"\n    rng = np.random.default_rng()\n    random_numbers = rng.random(size)\n\n    with Path(save_as).open('w') as out:\n        writer = csv.write(out)\n        writer.writerows(random_numbers)\n\nLinting with PyLint\nWe will lint this file using Pylint to find out what errors there are and how its style can be improved to conform with PEP8 guidelines.\nFirst you need to install pylint, typically in your virtual environment.\npip install pylint\nPylint can be configured using a ~/.pylintrc file in your home directory and over time this will grow as you customise your configuration but for now we will make one simple change from the default which is to increase the accepted line length. Create the file and save it with the following content.\n[FORMAT]\n## Maximum number of characters on a single line.\nmax-line-length=120\nOpen a terminal and navigate to the location you saved the example file save_random_numbers.py activate the virtual environment you installed pylint under if its not already being used and then type the following to run Pylint against your code…\npylint save_random_numbers.py\nYou should see output similar to the following…\n ❱ pylint save_random_numbers.py\n************* Module save_random_numbers\nsave_random_numbers.py:1:0: C0114: Missing module docstring (missing-module-docstring)\nsave_random_numbers.py:5:66: E0602: Undefined variable 'Union' (undefined-variable)\nsave_random_numbers.py:5:35: W0613: Unused argument 'seed' (unused-argument)\nsave_random_numbers.py:2:0: C0411: standard import \"from pathlib import Path\" should be placed before \"import numpy as np\" (wrong-import-order)\nsave_random_numbers.py:3:0: C0411: standard import \"import csv\" should be placed before \"import numpy as np\" (wrong-import-order)\n\n-------------------------------------------------------------------\nYour code has been rated at 0.00/10\nThe output tells us which module has been inspected on the first line. Each subsequent line indicates\n\nThe file.\nThe line the problem has been encountered on.\nThe column.\nA somewhat cryptic error code and then a message about the problem\nA more descriptive generic message associated with the error code.\n\nAt the moment we are only looking at one file, but when using PyLint against larger code bases this information is vital in helping direct you to the location of code that needs changing. At the end PyLint rates your code, ideally you should aim to get a score of 10.0/10.\nThe messages are quite informative, taking each in turn we can work through resolving them.\n\nMissing module docstring (missing-module-docstring)\nEach Python module should have a docstring as the very first line that describes what it does. In this example it might be considered superfluous but its good practice to get in the habit of writing these as it comes in useful when documentation is automatically generated from the docstrings in the code. To fix it we can add a short docstring at the top.\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport numpy as np\nfrom pathlib import Path\n\n\nUndefined variable 'Union' (undefined-variable)\nThis error arises because the type hint uses Union but it hasn’t been imported. It’s from the typing module so we can import it.\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport numpy as np\nfrom pathlib import Path\nfrom typing import Union\n\n\nUnused argument 'seed' (unused-argument)\nThis is very useful to be informed about because the seed argument, according to the docstring, is meant to be used in the call to the random number generator and ensures we will get the same set of random numbers generated each time we call the function with that seed, however, as Pylint has informed us we haven’t actually used it within the save_random_number() function. We can correct that by adding it when we instantiate the random number generator.\nrng = np.random.default_rng(seed=seed)\n\n\nstandard import \"from pathlib import Path\" should be placed before \"import numpy as np\" (wrong-import-order)\nThis message, like the one that follows it, is telling us that the order in which we have imported modules is incorrect, because the PEP8 guide recommends that core modules, which both csv and pathlib are, should be imported before other modules. We can correct this by changing the order (and because we have added an import from the typing module which is also a core module we move that too).\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport csv\nfrom pathlib import Path\nfrom typing import Union\n\nimport numpy as np\nOnce corrected your file should look like this…\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport csv\nfrom pathlib import Path\nfrom typing import Union\nimport numpy as np\n\ndef save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -&gt; None:\n    \"\"\"Save a list of random numbers (floats) to the given file.\n\n    The stated number of random numbers will be saved to the given target file, if the directory structure\n    doesn't exist it will be created. Output will by default be over-written.\n\n    Parameters\n    ----------\n    size : int\n        Number of random numbers to generate\n    seed: int\n        Seed for random number generation\n    save_as : Union[str, Path]\n        Directory/file to save numbers to.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    random_numbers = rng.random(size)\n\n    with Path(save_as).open('w') as out:\n        writer = csv.write(out)\n        writer.writerows(random_numbers)\n…and you can now run PyLint against it to see if you’ve improved your score.\n ❱ pylint save_random_numbers.py\n************* Module save_random_numbers\nsave_random_numbers.py:7:66: E1136: Value 'Union' is unsubscriptable (unsubscriptable-object)\n\n------------------------------------------------------------------\nYour code has been rated at 5.00/10 (previous run: 4.00/10, +1.00)\nThat is an improvement in score (of +1.00) but we now have another error telling us that E1136: Value 'Union' is unsubscriptable (unsubscriptable-object). You are unlikely to know what all the error codes mean, but there are a few handy on-line lists all PyLint codes or all PyLint messages and what they are telling you are worth consulting (The Little Book of Python Anti-Patterns is also useful). In this instance PyLint has returned a false-positive because Union can and should be subscripted here because it means the argument can be either a string (str) or a pathlib Path (Path). So how do we get around this complaint?\nYou can disable PyLint from complaining about specific error codes/messages on a per-file basis by adding a line that disables them. You can use either codes or messages (the bit in the brackets at the end of the line, in this case unsubscriptable-object) and it is advisable to use the message form as it is more informative to those who read your code subsequently.\nIf we add the following line it prevents PyLint from reporting the specific error…\nimport numpy as np\n\n# pylint: disable=unsubscriptable-object\n\ndef save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -&gt; None:\n…running PyLint against our code again we get a much better score.\n ❱ pylint save_random_numbers_tidy.py\n\n-------------------------------------------------------------------\nYour code has been rated at 10.00/10 (previous run: 5.00/10, +5.00)\n\n\n\nConfiguring PyLint\nThe last error we encountered is something that is likely to crop up again if you use Typehints liberally throughout your Python code (and I would encourage you to do so). Rather than having to remember to disable the error in each file/module we create we can configure PyLint via its configuration file ~/.pylintrc to always ignore this error. To do so add the following…\n[MESSAGES CONTROL]\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).\ndisable=unsubscriptable-object\nFor more on configuriong PyLint refer to the documentation and also details of how to integrate with your editor and IDE\n\n\nAutomated Formatting with Black\nBlack is The Uncompromising Code Formatter and is very strict about the way in which it formats code. This could be a good or bad thing depending on your point of view, but it does result in highly consistent code when applied to all files. It formats files in place, so be mindful of this if you run it against one of your files it will change it.\nInstall black in your virtual environment and make a backup of your save_random_number.py file that you have just tidied up with linting.\npip install black\ncp save_random_numbers.py tidy_save_random_numbers.py\nTo run black against your code pass it the input file, it will re-write it and you can then compare it against the backup you just made…\nblack save_random_numbers.py\n❱ diff save_random_numbers.py tidy_save_random_numbers.py\n5,8c5\n&lt;\n&lt; def save_random_numbers(\n  &lt;     size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\"\n  &lt; ) -&gt; None:\n---\n&gt; def save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -&gt; None:\n27c24\n&lt;     with Path(save_as).open(\"w\") as out:\n---\n&gt;     with Path(save_as).open('w') as out:\nIn this instance Black hasn’t changed much but it has reformatted the def save~randomnumbers~(...) line and moved the with Path() line as a consequence."
  },
  {
    "objectID": "posts/linting/index.html#when-to-lint",
    "href": "posts/linting/index.html#when-to-lint",
    "title": "Linting - What is all the fluff about?",
    "section": "When to Lint",
    "text": "When to Lint\nIt is worth linting your code from the outset of a project as not only does it result in a consistent style across your code base it also avoids the problem that can arise when applying linting retrospectively. If an existing code base has linting applied then the git blame, which indicates who the last person to edit a section was, then resides with the person who applied the linting, rather than the original author of the code. Its possible though that the person who applied the linting knows very little about the underlying functionality of the code but they may receive questions about it if they are indicated as the last person to have modified particular lines.\nFortunately there are a number of ways to automate and integrate linting into your workflow."
  },
  {
    "objectID": "posts/linting/index.html#automating-linting",
    "href": "posts/linting/index.html#automating-linting",
    "title": "Linting - What is all the fluff about?",
    "section": "Automating Linting",
    "text": "Automating Linting\n\nIDE Integration\nWhen programming it is really useful to use an Integrated Development Environment (IDE) as most allow the integration of linting tools and apply them to your code automatically, whether its using PyLint, YAPF, Black or otherwise. Setup and configuration is beyond the scope of this article but some links are provided to useful resources to get you started.\n\n\nVSCode\nVSCode supports linting in most languages, and both Python and R are supported along with other languages.\n\n\nPyCharm\nPyCharm supports automated formatting of code, for more information please refer to Reformat and rearrange code | PyCharm.\n\n\nEmacs\nThere are various options available for linting within Emacs, which you use depends on your preferences but LSP mode integrates with YAPF (via yapfify), Flake8 (via flycheck) and Black (via blacken)."
  },
  {
    "objectID": "posts/linting/index.html#git-integration",
    "href": "posts/linting/index.html#git-integration",
    "title": "Linting - What is all the fluff about?",
    "section": "Git Integration",
    "text": "Git Integration\nIf you are using an IDE then if configured correctly your code should be linted automatically for you, but an additional step that can capture anything that hasn’t been correctly formatted is to use a git hook to run linting on your code prior to making commits. There is git-pylint-commit-hook available on PyPi which runs automatically when you make commits to .py files."
  },
  {
    "objectID": "posts/linting/index.html#continuous-integration",
    "href": "posts/linting/index.html#continuous-integration",
    "title": "Linting - What is all the fluff about?",
    "section": "Continuous Integration",
    "text": "Continuous Integration\nIncluding a linting stage in your Continuous Integration (CI) pipeline pays dividends as we all make mistakes and sometimes forget to lint our code before making pushes."
  },
  {
    "objectID": "posts/linting/index.html#megalinter",
    "href": "posts/linting/index.html#megalinter",
    "title": "Linting - What is all the fluff about?",
    "section": "Megalinter",
    "text": "Megalinter\nPerhaps not necessary for everyone but worth mentioning the beast that is MegaLinter which will lint code across multiple languages and integrates easily into your pipeline (GitHub Action, CI on GitLab, Jenkins etc.). A useful article on doing so is Limit your technical debt and secure your code base using MegaLinter."
  },
  {
    "objectID": "posts/linting/index.html#pre-commit",
    "href": "posts/linting/index.html#pre-commit",
    "title": "Linting - What is all the fluff about?",
    "section": "Pre-commit",
    "text": "Pre-commit\nPre-commit is a Python package that adds a set of configurable hooks for linting your code, and not just Python, using a Git pre-commit hook. Hooks are run conditional on certain changes in states, in this case code that is run before commits are made. It creates a virtual Python Environment and installs the required packages there to lint your code. More will be written on this in a subsequent post."
  },
  {
    "objectID": "posts/linting/index.html#links",
    "href": "posts/linting/index.html#links",
    "title": "Linting - What is all the fluff about?",
    "section": "Links",
    "text": "Links\n\nPython\n\nFlake8 - Your Tool For Style Guide Enforcement\nBlack - The Uncompromising Code Formatter\nLinting Python in Visual Studio Code\nPylint - Overview of all Pylint messages\n\n\n\nR\n\nGitHub - r-lib/lintr: Static Code Analysis for R\nIntroduction to R: Linting R (and R Markdown)\n\n\n\nC++\n\ncpplint"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "I’ve a few other sites…\n\n\n\nSite\nDescription\n\n\n\n\nkimura\nA Dokuwiki site where I keep notes.\n\n\nFlickr\nPhotography (mostly landscape, climbing and cats).\n\n\nneil-snaps.co.uk\nWhere I fail to monetise my photography.\n\n\nSheffieldBoulder.uk\nAnother Dokuwiki site detailing artificial boulders around Sheffield.\n\n\nSoftware Design Patterns\nA “sub-blog” on Software Design Patterns.\n\n\nGitLab\nGit repos.\n\n\nGitHub (Personal)\nGit repos.\n\n\nGitHub (Work)\nGit repos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Custom domain on GitHub Pages\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Matplotlib\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPennine Way 2024\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nvirtualenvwrapper hooks\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Fail and Skip\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit - Worktrees\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit - Making Amends and Fixing things up\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit Remotes Revisited\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Parameterisation\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nvirtualenvwrapper\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nRepository Review with Scientific Python\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit Remotes\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nR Resources\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGitLab CI - Automatic Publishing to PyPI\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-commit and R Packaging\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nSphinx Documentation\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit : Useful Hooks\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPython Packaging\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nBrowser Extensions\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit.ci : Integrating Pre-Commit into CI/CD\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nRunning in 2022\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nWho’s to Blame\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit : Customising and Updating\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nLinux Command Line Alternatives\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit : Protecting your future self\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit : Custom SSH credentials for git repositories\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nLinting - What is all the fluff about?\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Neil and work as a Research Software Engineer at the University of Sheffield. My education started with BSc in Zoology and Genetics followed by a MSc in Genetic Epidemiology. My academic career started with just over a decade as a Genetics Statistician where I learnt GNU/Linux system administration and reproducible research practices before shifting to Medical Statistics and working on Clinical Trials for a similar amount of time and taught myself R before a stint as a Data Scientist in industry company where I not only learnt Python but about good software development practices.\nHere you’ll find posts about Research Software Engineering, Git/GitHub/GitLab, GNU/Linux (Gentoo, Arch, OpenWRT), Python, Bash, R, Emacs, Org-mode, Statistics, Genetics, Evolution and more.\nWhen not working I enjoy climbing, running, cycling, cooking, hiking, photography, gardening and spending time with my family.\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Custom domain on GitHub Pages\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Matplotlib\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPennine Way 2024\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nvirtualenvwrapper hooks\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Fail and Skip\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit - Worktrees\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit - Making Amends and Fixing things up\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit Remotes Revisited\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Parameterisation\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nvirtualenvwrapper\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nRepository Review with Scientific Python\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit Remotes\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nR Resources\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGitLab CI - Automatic Publishing to PyPI\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-commit and R Packaging\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nSphinx Documentation\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit : Useful Hooks\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPython Packaging\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nBrowser Extensions\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit.ci : Integrating Pre-Commit into CI/CD\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nRunning in 2022\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nWho’s to Blame\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit : Customising and Updating\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nLinux Command Line Alternatives\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit : Protecting your future self\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit : Custom SSH credentials for git repositories\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nLinting - What is all the fluff about?\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog.nshephard.dev",
    "section": "",
    "text": "Quarto Custom domain on GitHub Pages\n\n\n\n\n\n\nquarto\n\n\ngithub\n\n\nwebsite\n\n\ndomain\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Matplotlib\n\n\n\n\n\n\npython\n\n\ntesting\n\n\npytest\n\n\nmatplotlib\n\n\npytest-mpl\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPennine Way 2024\n\n\n\n\n\n\nholiday\n\n\nhiking\n\n\npennine way\n\n\ngpx\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nAug 30, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nvirtualenvwrapper hooks\n\n\n\n\n\n\npython\n\n\nvirtual environments\n\n\nbash\n\n\ndotfiles\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Fail and Skip\n\n\n\n\n\n\npython\n\n\ntesting\n\n\npytest\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit - Worktrees\n\n\n\n\n\n\ngit\n\n\nworktrees\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit - Making Amends and Fixing things up\n\n\n\n\n\n\ngit\n\n\namend\n\n\nfixup\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit Remotes Revisited\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\n\n\n\n\n\n\n\nFeb 17, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Parameterisation\n\n\n\n\n\n\npython\n\n\ntesting\n\n\npytest\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nvirtualenvwrapper\n\n\n\n\n\n\npython\n\n\nvirtual environments\n\n\n\n\n\n\n\n\n\nDec 23, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nRepository Review with Scientific Python\n\n\n\n\n\n\npre-commit\n\n\npackaging\n\n\nscientific python\n\n\npython\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit Remotes\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nR Resources\n\n\n\n\n\n\nR\n\n\ndocumentation\n\n\ndata science\n\n\nstatistics\n\n\ndata analysis\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGitLab CI - Automatic Publishing to PyPI\n\n\n\n\n\n\npython\n\n\ndocumentation\n\n\npackaging\n\n\ngitlab\n\n\nci\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-commit and R Packaging\n\n\n\n\n\n\nquarto\n\n\nR\n\n\ngit\n\n\npre-commit\n\n\ngithub actions\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nSphinx Documentation\n\n\n\n\n\n\nquarto\n\n\npython\n\n\ndocumentation\n\n\nsphinx\n\n\ngithub actions\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit : Useful Hooks\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\npre-commit\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPython Packaging\n\n\n\n\n\n\nquarto\n\n\npython\n\n\npackaging\n\n\nsetuptools\n\n\ngithub actions\n\n\npypi\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nBrowser Extensions\n\n\n\n\n\n\nbrowser\n\n\nfirefox\n\n\nopera\n\n\nvivaldi\n\n\nextensions\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit.ci : Integrating Pre-Commit into CI/CD\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\npre-commit\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nRunning in 2022\n\n\n\n\n\n\nquarto\n\n\nrunning\n\n\nemacs\n\n\nliterate programming\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nWho’s to Blame\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\nblame\n\n\n\n\n\n\n\n\n\nDec 17, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit : Customising and Updating\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\npre-commit\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nLinux Command Line Alternatives\n\n\n\n\n\n\ncode\n\n\nlinux\n\n\nbash\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Commit : Protecting your future self\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\npre-commit\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nGit : Custom SSH credentials for git repositories\n\n\n\n\n\n\nssh\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\nkeychain\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n\n\n\n\n\n\nLinting - What is all the fluff about?\n\n\n\n\n\n\ncode\n\n\nlinting\n\n\npython\n\n\nR\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/git-remotes/index.html",
    "href": "posts/git-remotes/index.html",
    "title": "Git Remotes",
    "section": "",
    "text": "Git and various forges such as GitHub GitLab are useful collaborative tools for version controlling, sharing and working collaboratively. Normally a repository resides on your local computer and it tracks a remote (often referred to as origin)"
  },
  {
    "objectID": "posts/git-remotes/index.html#gitconfig",
    "href": "posts/git-remotes/index.html#gitconfig",
    "title": "Git Remotes",
    "section": ".git/config",
    "text": ".git/config\nWhen you initialise or clone a repository a hidden directory .git is created. Within this resides the configuration file for the repository .git/config that defines how Git is to behave when performing actions on the repository.\n\ncore\n\n\nremote\nThe remote field defines the location of the repository to which code is synced via pushing and pulling. Typically and by default this is called the origin and you have a section that defines the url and the fetch for this remote.\nThe url field can take two forms, either one based on https in which case the prefix of the value will be https://github.com/ or one based on ssh in which case the prefix will be git@github.com:. What follows is then the user account or organisation (e.g. ns-rse for my personal repositories) followed by /&lt;repo-name&gt; (e.g. for this repository that is ns-rse.github.io).\nThe fetch field is what in Git parlance is called a Refspec (Reference Specification). This takes the form &lt;src&gt;:&lt;dst&gt; and is a method of mapping references on the remote side (&lt;src&gt;) to those locally (&lt;dst&gt;) and it is this that maps the local branches (refs/heads/*) to their counterparts on the remote (refs/remotes/origin/*). If there is a + prefix it tells git to up-date the reference even if there is no fast-forward.\n[remote \"origin\"]\n    url = git@github.com:ns-rse/ns-rse.github.io\n    fetch = +refs/heads/*:refs/remotes/origin/*\n\n\nbranch\nWhat follows is then a series of entries for branch which defines further how each local branch maps to a remote and where it should merge to. Each branch has a name define in the section header and within two parameters are set the remote which by default points to the origin (defined in the above remote) section and a merge field which defines the local reference to the branch under refs/head/&lt;branch-name&gt;."
  },
  {
    "objectID": "posts/git-remotes/index.html#having-a-private-branch",
    "href": "posts/git-remotes/index.html#having-a-private-branch",
    "title": "Git Remotes",
    "section": "Having a Private branch",
    "text": "Having a Private branch\nSometimes the need might arise to develop a feature in private. This is fine if you have no intention of sharing the work with others, you create a branch on your local computer and never push it to a forge. However, there will arise times where you do want to share you work with others whilst keeping things private. This too is possible and can be achieved by creating a private repository on your own account rather than using the original and configuring a branch to track that instead.\n\nCreating a second remote\nStart by creating a new repository on GitHub/GitLab and making it private from the outset. Do not include any additional information such as .gitignore, README.md or LICENSE.md it should be completely empty.\nYou can then add it as a new remote to your existing repository in one of two ways.\n\nGit at the Command Line\nYou should have some instructions shown on GitHub one of which is …or push an existing repository from the command line. The first line of this is telling you how to add a new remote to the repository. called origin and to point towards the repository you have just created. You will likely already have an origin remote defined so you need to choose a different name and point it to the correct location.\ngit remote add private-work git@github.com:ns-rse/private-work\n\n\nCheck your .git/config\nIf you now look at your .git/config file there should be two entries for [remote \"\"] one for origin (i.e. [remote \"origin\"]) and one for the new remote that has just been added [remote \"private-work\"]. Of course, because .git/config is just a text configuration file you can edit it and enter these values manually yourself if you want to.\nNB Either of the above methods will append these options to the bottom of the file so if you can’t see it immediately scroll down.\n\n\n\nTrack your new remote\nNow that you have a secondary remote defined you can create a branch and set it to track the private remote you have created and configured. Create a new branch by using git checkout -b\n\nCommand Line\ngit checkout -b private-feature\nYou then set the upstream using --set-upstream-to or its shorthand -u, at this stage you don’t have anything to include so make an empty commit\ngit push --set-upstream-to private-work private-work\n\n\nMagit\n\n\nCheck .git/config\nReturning to your .git/config file you should now have an additional entry for a branch underneath the new entry for the remote and the value for remote under branch is the name of the remote.\n[remote \"private-work\"]\n url = git@github.com:ns-rse/private-work.git\n fetch = +refs/heads/*:refs/remotes/private-work/*\n[branch \"private-feature\"]\n remote = private-work\n merge = refs/heads/private-feature"
  },
  {
    "objectID": "posts/git-remotes/index.html#keeping-up-to-date",
    "href": "posts/git-remotes/index.html#keeping-up-to-date",
    "title": "Git Remotes",
    "section": "Keeping up to Date",
    "text": "Keeping up to Date\nThe private repository you setup on GitHub/GitLab should now have a copy of the private-work branch you created locally.\nIf you are collaborating with others the main/master branch may move ahead of yours as others work is merged in. You therefore need to regularly update your private branch by merging or rebasing from the origin rather than the private-work remote your branch is configured to track. The difference between merging and rebasing is beyond the scope of this article, there are pros and cons to each\ngit checkout main\ngit pull\ngit checkout private-work\ngit merge main\nOr you can merge directly from origin with\ngit checkout private-feature\ngit merge origin/main"
  },
  {
    "objectID": "posts/git-remotes/index.html#checkout-someone-elses-private-branch",
    "href": "posts/git-remotes/index.html#checkout-someone-elses-private-branch",
    "title": "Git Remotes",
    "section": "Checkout someone else’s private branch",
    "text": "Checkout someone else’s private branch\nInevitably the need might arise for a collaborator to test your private branch. In which case they need to be granted permission to the private repository by the developer who created it. This is done via Settings &gt; Collaborators and inviting them to work on your repository.\nOnce they have accepted the invitation they will also have to add a secondary remote and the branch they are working on. They can follow the instructions above to set up a remote, however they should not follow the instructions to branch because that branch already exists on your private repository.\nbut because .git/config is just a text file you can copy the lines from your configuration and share it with your collaborator and they can add them to their .git/config file. Once these options have been added they"
  },
  {
    "objectID": "posts/git-remotes/index.html#bonus---using-magit",
    "href": "posts/git-remotes/index.html#bonus---using-magit",
    "title": "Git Remotes",
    "section": "Bonus - Using Magit",
    "text": "Bonus - Using Magit\nIf you use Emacs and the amazing Magit you can of course do this via the Magit interface. In the Magit buffer for the repository (e.g. magit: ns-rse.github.io) press M to bring up the Remote transient buffer. You then press a to add a remote and are prompted for a name for the remote (in this example we use the same as above private-work), this can be anything you want other than origin which is already defined, and then the URL this will be git@github.com:&lt;account-name&gt;/&lt;private-repo-name&gt;.git.\nTo create a branch in Magit press b then l to select a local branch and enter main (or master), then make sure its upto date with the origin by Pulling with P then u. Now create a new branch by pressing b and since it will be a new press c to create it. You will be prompted for the name of a branch from which to branch from and then for the name of your branch.\nIn the Magit buffer for the repository you are working on pushes are made with P. The Transient buffer then offers you a choice of places to push to, one of which is elsewhere which is accessed by the e key. You can then type in the name of the remote tracking branch as defined under the remote entry you configured above which in this case is private-work."
  },
  {
    "objectID": "posts/git-remotes-revisited/index.html",
    "href": "posts/git-remotes-revisited/index.html",
    "title": "Git Remotes Revisited",
    "section": "",
    "text": "I’ve written before about Git Remotes but in my on-going effort to improve my understanding of the tools I use daily I’ve discovered that it is possible to have a single remote push to two different URLs so am revisiting the topic and perhaps writing a little more clearly on it.\nRemotes are where other copies of your repository exist and typically where collaboration occurs (i.e. issue tracking, merge requests, bug reports etc.).\nThe main remote that a repository is configured to use by default is called origin but it is possible to have multiple remotes tracked by your local copy."
  },
  {
    "objectID": "posts/git-remotes-revisited/index.html#listing-remotes",
    "href": "posts/git-remotes-revisited/index.html#listing-remotes",
    "title": "Git Remotes Revisited",
    "section": "Listing Remotes",
    "text": "Listing Remotes\nList remotes with git remote [-v] the -v flag will show the URLs that are stored for the short-cut.\n❱ git remote -v\nforgejo forgejo@forgejo.hopto.org:nshephard/mvdate.git (fetch)\nforgejo forgejo@forgejo.hopto.org:nshephard/mvdate.git (push)\norigin git@gitlab.com:nshephard/mvdate.git (fetch)\norigin git@gitlab.com:nshephard/mvdate.git (push)\nYou can get more information about a remote using git remote show origin\n❱ git remote show origin\n * remote origin\n   Fetch URL: git@gitlab.com:nshephard/mvdate.git\n   Push  URL: git@gitlab.com:nshephard/mvdate.git\n   HEAD branch: main\n   Remote branches:\n     main                                                tracked\n     refs/merge-requests/18/head                         new (next fetch will store in remotes/origin\n     refs/pullreqs/15                                    stale (use 'git remote prune' to remove)\n     refs/remotes/origin/nshephard/update-pre-commit     stale (use 'git remote prune' to remove)\n     refs/remotes/origin/nshephard/update-readme         stale (use 'git remote prune' to remove)\n   Local branches configured for 'git pull':\n     main                            merges with remote main\n     nshephard/fix-mtime             merges with remote nshephard/fix-mtime\n     nshephard/update-pre-commit     merges with remote nshephard/update-pre-commit\n   Local ref configured for 'git push':\n     main pushes to main (local out of date)\nThis can be useful to show you what you need to tidy up if there are lots of stale branches around. In this example I can run git remote prune origin as advised to remove these.\n\nDefault Remote\nThe default remote to push to can be set with the following command, it will likely already be set to origin so this would not change anything.\ngit config --local remote.pushDefault origin\nThis adds the following to your .git/config if it wasn’t already there.\n[remote]\n    pushDefault = origin"
  },
  {
    "objectID": "posts/git-remotes-revisited/index.html#adding-remotes",
    "href": "posts/git-remotes-revisited/index.html#adding-remotes",
    "title": "Git Remotes Revisited",
    "section": "Adding Remotes",
    "text": "Adding Remotes\nIt is straight forward to add a remote with git remote add &lt;shortcut&gt; &lt;URL&gt; where the URL is either the https or the git URL.\n❱ git remote add forgejo forgejo@forgejo.hopto.org:nshephard/mvdate.git\nThis adds details to your .git/config so that it has the following\n[remote \"origin\"]\n    url = git@gitlab.com:nshephard/mvdate.git\n    fetch = +refs/heads/*:refs/remotes/origin/*\n    fetch = +refs/merge-requests/*/head:refs/pullreqs/*\n[remote \"forgejo\"]\n    url = ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git\n    fetch = +refs/heads/*:refs/remotes/forgejo/*\n[remote]\n    pushDefault = origin\n[branch \"main\"]\n    remote = origin\n    merge = refs/heads/main\nTwo remotes are defined, origin and forgejo, the default to push to is set to origin and the main branch is setup to track the remote origin.\n\nPushing to specific remote\nWith two remotes setup you can choose, at the branch level, where to push your changes by specifying the remote you wish to use. If you wanted to push a newly created branch, change-just-for-forgejo, to the newly added forgejo remote you would configure it with.\n❱ git switch -c change-just-for-forgejo\n❱ git commit --allow-empty -m \"Test push just to forgejo\"\n❱ git push --set-upstream forgejo change-just-for-forgejo\nEnumerating objects: 2, done.\nCounting objects: 100% (2/2), done.\nWriting objects: 100% (2/2), 376 bytes | 376.00 KiB/s, done.\nTotal 2 (delta 0), reused 0 (delta 0), pack-reused 0\nremote:\nremote: Create a new pull request for 'change-just-for-forgejo':\nremote:   &lt;https://forgejo.hopto.org/nshephard/mvdate/compare/main...change-just-for-forgejo&gt;\nremote:\nremote: . Processing 1 references\nremote: Processed 1 references in total\nTo ssh://forgejo.hopto.org:1234/nshephard/mvdate.git\n\n-   [new branch]      change-just-for-forgejo -&gt; change-just-for-forgejo\n\nbranch 'change-just-for-forgejo' set up to track 'forgejo/change-just-for-forgejo'.\nGit reports that the local change-just-for-forgejo has been setup to track forgejo/change-just-for-forgejo and the following entry has been added to .git/config\n[branch \"change-just-for-forgejo\"]\n    remote \"forgejo\"\n    merge = refs/heads/change-just-for-forgejo\n\n\nA Note on SSH Ports\nIf a remote is using a non-standard port for SSH connections (i.e. anything other than 22) then you have to use a different format for specifying the remote URL. Instead of forgejo@forgejo.hopto.org:nshephard/mvdate.git you must explicitly state the protocol (ssh://) and include the port so that it reads ssh://forgejo@forgejo.hopto.org:1234 and so to add it you would be added with the following\n❱ git remote add forgejo ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git\n❱ git remote -v\nforgejo ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (fetch)\nforgejo ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (push)\norigin  git@gitlab.com:nshephard/mvdate.git (fetch)\norigin  git@gitlab.com:nshephard/mvdate.git (push)\nI use a non-standard port and so use that convention for the remainder of this article. If you do not use a non-standard port you can either change the port (1234) to the default (22) or use the conventional syntax for referring to the remote."
  },
  {
    "objectID": "posts/git-remotes-revisited/index.html#mirroring-remotes",
    "href": "posts/git-remotes-revisited/index.html#mirroring-remotes",
    "title": "Git Remotes Revisited",
    "section": "Mirroring Remotes",
    "text": "Mirroring Remotes\nThe really neat thing is that it is possible to have a local repository track multiple remotes, which means when you push your changes it will go to both. You could configure an alias to push to both of the remotes we currently have defined, but there is an excellent post on StackOverflow that shows how to do this with Git itself because each remote can have multiple pushurls.\nAs we have added a second remote to our configuration our .git/config for mvdate our configuration currently looks like this.\n[remote \"origin\"]\n    url = git@gitlab.com:nshephard/mvdate.git\n    fetch = +refs/heads/*:refs/remotes/origin/*\n    fetch = +refs/merge-requests/*/head:refs/pullreqs/*\n[remote \"forgejo\"]\n    url = ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git\n    fetch = +refs/heads/*:refs/remotes/forgejo/*\n[remote]\n    pushDefault = origin\n[branch \"main\"]\n    remote = origin\n    merge = refs/heads/main\n[branch \"change-just-for-forgejo\"]\n    remote = forgejo\n    merge = refs/heads/change-just-for-forgejo\nAs above, there are two remotes are defined, origin and forgejo, the default to push to is set to origin and the main branch is setup to track origin whilst the change-just-for-forgejo branch is setup to track forgejo.\n❱ git remote -v\nforgejo ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (fetch)\nforgejo ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (push)\norigin  git@gitlab.com:nshephard/mvdate.git (fetch)\norigin  git@gitlab.com:nshephard/mvdate.git (push)\nHow do we get the origin remote setup and configured to push to both gitlab and forgejo? This can be done using the set-url --add --push options to git remote, below we add forgejo as a push target to origin.\nNB Note I use a non-standard SSH port in the following, see above note.\n❱ git remote set-url --add --push origin ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git\n❱ git remote -v\nforgejo ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (fetch)\nforgejo ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (push)\norigin  git@gitlab.com:nshephard/mvdate.git (fetch)\norigin  ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (push)\nBut this has removed the original push target under origin which pointed to gitlab so we need to add that back in.\n❱ git remote set-url --add --push origin git@gitlab.com:nshephard/mvdate.git\n❱ git remote -v\nforgejo ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (fetch)\nforgejo ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (push)\norigin  git@gitlab.com:nshephard/mvdate.git (fetch)\norigin  ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (push)\norigin  git@gitlab.com:nshephard/mvdate.git (push)\nWe now have two push targets on origin, one pointing to gitlab.com (using the default port 22) and one pointing to forgejo.hopto.org (on port 1234) and as the default target is origin when we git push it will send the changes to both. We still have the forgejo remote defined and it only tracks the forgejo URL.\nWe can test this with an empty commit on a new branch, test-both, which we first create.\n❱ git switch -c test-both\n❱ git commit --allow-empty -m \"Testing pushing to GitLab and Forgejo\"\n[test-both c07caf6] Testing pushing to GitLab and Forgejo\n❱ git push\nEnumerating objects: 1, done.\nCounting objects: 100% (1/1), done.\nWriting objects: 100% (1/1), 210 bytes | 210.00 KiB/s, done.\nTotal 1 (delta 0), reused 0 (delta 0), pack-reused 0\nremote:\nremote: Create a new pull request for 'test-both':\nremote:   &lt;https://forgejo.hopto.org/nshephard/mvdate/compare/main...test-both&gt;\nremote:\nremote: . Processing 1 references\nremote: Processed 1 references in total\nTo ssh://forgejo.hopto.org:1234/nshephard/mvdate.git\n\n-   [new branch]      test-both -&gt; test-both\n\nbranch 'test-both' set up to track 'origin/test-both'.\nEnumerating objects: 26, done.\nCounting objects: 100% (26/26), done.\nWriting objects: 100% (26/26), 16.75 KiB | 8.37 MiB/s, done.\nTotal 26 (delta 0), reused 0 (delta 0), pack-reused 0\nremote:\nremote: To create a merge request for test-both, visit:\nremote:   &lt;https://gitlab.com/nshephard/mvdate/-/merge_requests/new?merge_request%5Bsource_branch%5D=test-both&gt;\nremote:\nTo gitlab.com:nshephard/mvdate.git\n\n-   [new branch]      test-both -&gt; test-both\n\nbranch 'test-both' set up to track 'origin/test-both'.\nThe output above shows that the branch test-both was pushed to both the URLs we have configured as push targets to origin and if you visit the repositories you will find the branches now exist there."
  },
  {
    "objectID": "posts/git-remotes-revisited/index.html#deleting-remotes",
    "href": "posts/git-remotes-revisited/index.html#deleting-remotes",
    "title": "Git Remotes Revisited",
    "section": "Deleting Remotes",
    "text": "Deleting Remotes\nIn my use case I simply want to push both remotes so that they mirror each other so I can delete the forgejo remote. This will leave the push URL for that remote under the configuration for origin and allows us to set any branch to use the origin as a the remote and any changes will be pushed to both.\nThere may be instances where you want to leave the additional remote in place if you wanted to push some changes just to that remote so its not essential that you remove it, but if you want to you can delete reference to a remote from your local configuration.\n❱ git remote remove forgejo\n❱ git remote -v\norigin  git@gitlab.com:nshephard/mvdate.git (fetch)\norigin  ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git (push)\norigin  git@gitlab.com:nshephard/mvdate.git (push)\nBecause the change-just-for-forgejo was setup to track the forejo remote we would need to change that target, we can do so with the following\n❱ git switch change-just-for-forgejo\n❱ git branch --set-upstream-to=origin/change-just-for-forgejo change-just-for-forgejo\nThis changes the remote target for the branches definition and our configuration now looks like the following.\n[remote \"origin\"]\n    url = git@gitlab.com:nshephard/mvdate.git\n    fetch = +refs/heads/*:refs/remotes/origin/*\n    fetch = +refs/merge-requests/*/head:refs/pullreqs/*\n    pushurl = ssh://forgejo@forgejo.hopto.org:1234/nshephard/mvdate.git\n    pushurl = git@gitlab.com:nshephard/mvdate.git\n[remote]\n    pushDefault = origin\n[branch \"main\"]\n    remote = origin\n    merge = refs/heads/main\n[branch \"change-just-for-forgejo\"]\n    remote = origin\n    merge = refs/heads/change-just-for-forgejo"
  },
  {
    "objectID": "posts/git-remotes-revisited/index.html#conclusion",
    "href": "posts/git-remotes-revisited/index.html#conclusion",
    "title": "Git Remotes Revisited",
    "section": "Conclusion",
    "text": "Conclusion\nHaving a local repository push to two remotes is a simple way of a mirroring. Whether you have a use case for it depends on what you are doing. I could easily imagine this could get very complicated if changes were pushed by others to each remote, but I wouldn’t be surprised if Git is regularly used in this way by others.\nOf course if you want to push a branch that you wish to keep private to one remote only then you would have to be very careful in how you use this setup. The original StackOverflow solution inspired that this post suggests creating an independent remote (e.g. all) so that you can push changes to origin or the second remote (in this example forgejo) and use all only when you wish to push changes to both.\nIts been good for me to return to a topic I’ve delved into in the past, this second time round I feel I’ve got a slightly better grasp of what I’m doing and have a neater solution to achieve what is required."
  },
  {
    "objectID": "posts/git-remotes-revisited/index.html#links",
    "href": "posts/git-remotes-revisited/index.html#links",
    "title": "Git Remotes Revisited",
    "section": "Links",
    "text": "Links\n\nGit - Working with Remotes\nGit - git-remote Documentation\ngithub - Git - Pushing code to two remotes - Stack Overflow"
  },
  {
    "objectID": "posts/whos_to_blame/index.html",
    "href": "posts/whos_to_blame/index.html",
    "title": "Who’s to Blame",
    "section": "",
    "text": "Git blame shows who made changes to which line of code for a given point in its history."
  },
  {
    "objectID": "posts/whos_to_blame/index.html#usage",
    "href": "posts/whos_to_blame/index.html#usage",
    "title": "Who’s to Blame",
    "section": "Usage",
    "text": "Usage\nGit blame works on individual files and so requires a filename, there are a host of options, for example -e prints the authors email address -w ignores changes to white space and -L 10,20 restricts output to the specified line range. If you want a the blame for a specific revision then you must include the hash.\ngit blame -e -w -L 10,20 f923la git_blame.org"
  },
  {
    "objectID": "posts/whos_to_blame/index.html#alias",
    "href": "posts/whos_to_blame/index.html#alias",
    "title": "Who’s to Blame",
    "section": "Alias",
    "text": "Alias\nSome people don’t like the pejorative nature of the word blame. That’s ok though, with a tweak to our configuration its possible to use the alias praise or simply who.\n# blame alias\ngit config --global alias.praise blame\ngit praise -L1,30 git_blame.org\n# who alias\ngit config --global alias.who blame\ngit who -L1,30 git_blame.org\nFor more detailed information on the array of options refer to the official documentation or see git blame --help."
  },
  {
    "objectID": "posts/whos_to_blame/index.html#ignoring-blame",
    "href": "posts/whos_to_blame/index.html#ignoring-blame",
    "title": "Who’s to Blame",
    "section": "Ignoring blame",
    "text": "Ignoring blame\nSometimes the case arises where you want to ignore blame. Perhaps the most common example is when an existing code base has been linted to conform to a particular style guide. Looking at who performed these changes is not informative and masks who made the changes and why. Its possible to ignore specific commits on the command line with --ignore-revs &lt;hash&gt; &lt;file&gt;, but it will quickly become tedious to remember to ignore all blame across multiple commits. Fortunately you can save the commits to ignore to the file .git-blame-ignore-revs (along with comments) so that they are stored. The full commit (40 characters) of hashes should be used.\n# PEP8 compliance for module X\nc00177a6121f86c001f338feff3280fd576fdbf3\n\n# PEP8 compliance for module Y\ndb27fa5f18299ca631efc430512a3f358c2b154f\nNow that you have the revisions in place to be ignored when reporting blame you can choose not to use it.\ngit blame --ignore-revs-file .git-blame-ignore-revs git_blame.org\n…but this is tedious to remember to have to do each time and ideally others on your team should use this file too. You can configure Git to use this file by modifying the local configuration. Make sure to add it to your repository so that others can use it.\ngit config blame.ignoreRevsFile .git-blame-ignore-revs\ngit add .git-blame-ignore-revs\nAs of 2022-03-08 GitHub will also ignore commits in the blame view that are listed in .git-blame-ignore-revs providing this file is in the root of your project folder."
  },
  {
    "objectID": "posts/whos_to_blame/index.html#links",
    "href": "posts/whos_to_blame/index.html#links",
    "title": "Who’s to Blame",
    "section": "Links",
    "text": "Links\n\nGeneral\n\nAtlassian | git blame\n\n\n\nResources\n\nIgnoring bulk change commits with git blame\nLittle things I like to do with Git\nIs there a way to customize the output of git blame"
  },
  {
    "objectID": "posts/python-packaging/index.html",
    "href": "posts/python-packaging/index.html",
    "title": "Python Packaging",
    "section": "",
    "text": "This post describes steps in creating a Python package. If you are looking for information on installing packages this is done using Python PIP.\nPython packaging is in a constant state of flux. There is the official Python Packaging User Guide and the Python Packaging Authority (PyPA) which is probably the best resource to read but things change, and often quickly. The focus here is on the PyPA Setuptools using pyproject.toml which works with Python &gt;= 3.7, but you may wish to consider other packages such as Poetry or PDM which offer some advantages but with additional frameworks to learn.\nA few examples of Python packages that I have packaged are listed below, most have also been released to PyPI."
  },
  {
    "objectID": "posts/python-packaging/index.html#package-structure",
    "href": "posts/python-packaging/index.html#package-structure",
    "title": "Python Packaging",
    "section": "Package Structure",
    "text": "Package Structure\nYou should place your code within a Git version controlled directory for your project. It is then normal to place all files in an organised hierarchy with a sub-directory of the same name for Python code, known as a \"flat\" structure and tests under tests directory. It is possible to have more than one directory containing code but for now I'm sticking to the flat structure.\n.\n    ├── ./build\n    ├── ./dist\n    ├── ./\n    ├── ./my_package\n    ├── ./my_package/__init__.py\n    ├── ./my_package/module_a.py\n    ├── ./my_package/module_b.py\n    ├── ./my_package/something/module_c.py\n    └── ./tests\n        ├── ./tests/conftest.py\n        ├── ./tests/resources\n        ├── ./tests/test_module_a.py\n        ├── ./tests/test_module_b.py\n        └── ./tests/something/test_module_c.py\n\n__init__.py\nIn older versions of Python (&lt;3.3) a __init__.py was required in every directory and sub-directory that was to be a module/sub-module. In more recent versions of Python (&gt;\\=3.3) they are not essential though as Python uses namespace packages. But in most cases its simpler to include such a file in the top level of your directory. __init__.py files can be completely empty or they can contain code that is used throughout your package, such as setting up a logger."
  },
  {
    "objectID": "posts/python-packaging/index.html#configuration-pyproject.toml",
    "href": "posts/python-packaging/index.html#configuration-pyproject.toml",
    "title": "Python Packaging",
    "section": "Configuration pyproject.toml",
    "text": "Configuration pyproject.toml\nPackage configuration has been and is in a state of flux, there was originally setup.py which was then complemented and gradually replaced by setup.cfg. The new method on the block though is pyproject.toml which, with a little tweaking and judicious choice of packages can handle everything.\nSetuptools is shifting towards using pyproject.toml and whilst it is still under development its already highly functional. It’s written in Tom's Obvious Minimal Language and isn't too dissimilar in structure to setup.cfg.\nA useful reference for writing your configuration in pyproject.toml is Configuring setuptools using pyproject.toml files. It is based around PEP 621 – Storing project metadata in pyproject.toml | peps.python.org.\nA bare-bones pyproject.toml file should reside in the top level of your directory with the following (NB This includes the minimum versions and setuptools_scm extension for dynamically setting package version)…\n\nbuild-system\n[build-system]\nrequires = [\"setuptools&gt;=65.6.3\", \"setuptools_scm[tools]&gt;=6.2\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\nTraditionally configuration of meta-data such as author, code repository and license was made via setup.py but you can either specify some (or most) of this in pyproject.toml or a concurrent setup.cfg.\n\n\nproject\nThis is the main body of the project description detailing name, authors, description, readme, license, keywords, classifiers, dependencies and version amongst other things.\nThe type of license you have chosen to apply to your package. For guidance see Choose an Open Source License.\nThe README of your package which may be in Markdown or Restructured Text.\nSets the components of your package which are set dynamically. In this example we only set the version dynamically using setuptools_scm.\nThe dependencies are those that are required for running the code. They should not include packages that are required for development (e.g. black. flake8, ruff, pre-comit, pylint etc.), nor those required for testing (e.g. pytest, pytest-regtest, pytest-cov etc.), documentation (e.g. Sphinx, numpydoc, sphinx_markdown_table, sphinx-autodoc-typehints, sphinxcontrib-mermaid etc.) as these are defined in a separate section.\n[project]\nname = \"my_package\"\nauthors = [\n  {name = \"Author 1\", email=\"author1@somewhere.com\"},\n  {name = \"Author 2\", email=\"author2@somewhere.com\"},\n  {name = \"Author 3\", email=\"author3@somewhere.com\"},\n]\ndescription = \"A package that does some magic!\"\nlicense = \"GNU GPLv3 only\"\nreadme = \"README.md\"\ndynamic = [\"version\"]\ndependencies = [\n  \"numpy\",\n  \"pandas\",\n  \"tqdm\",\n]\nAll other sections are considered subsections, either of project or tool and are defined under their own heading with [project|tool].&lt;package&gt;[.&lt;options&gt;].\n\nproject.urls\nThese are important as they define where people can find the Source, Documentation and Bug_Tracker amongst other things. There may be more fields that can be configured here but I've not used the yet. Substitute these to reflect where your package is hosted, your username and the package name.\n[project.urls]\nSource = \"https://gitlab.com/username/my_package\"\nBug_Tracker = \"https://gitlab.com/username/my_package/issues\"\nDocumentation = \"https://username.gitlab.com/my_package\"\n\n\nproject.optional-dependencies\nThis is where you list dependencies that are not required for running a package but are required for different aspects such as development, documentation, publishing to PyPI, additional Notebooks and so forth, the options are limitless.\n[project.optional-dependencies]\ndev = [\n  \"black\",\n  \"flake8\",\n  \"Flake8-pyproject\",\n  \"pre-commit\",\n  \"pylint\",\n  \"ruff\",\n]\ndocs = [\n  \"Sphinx\",\n  \"myst-parser\",\n  \"numpydoc\",\n  \"pydata_sphinx_theme\",\n  \"sphinx-autodoc-typehints\",\n  \"sphinx_markdown_tables\",\n  \"sphinxcontrib-mermaid\",\n]\npypi = [\n  \"build\",\n  \"pytest-runner\",\n  \"setuptools-lint\",\n  \"setuptools_scm\",\n  \"twine\",\n  \"wheel\"\n]\ntest = [\n \"pytest\",\n \"pytest-cov\",\n]\nnotebooks = [\n  \"ipython\",\n  \"ipywidgets\",\n  \"jupyter_contrib_nbextensions\",\n  \"jupyterthemes\",\n]\n\n\nproject.scripts (Entry Points)\nEntry points or scripts are a neat method of providing a simple command line interface to your package that links directly into a specific module to provide a command line interface to your programme.\nThese are defined under project.scripts section.\n[project.scripts]\ntcx2gpx = \"tcx2gpx:process\"\n\n\n\ntool\n\ntool.setuptools\nsetuptools is perhaps the most common package for configuring Python packages and is the one that is being exposed here. Its configuration is multi-level depending on which component you are configuring.\n\ntool.setuptools.packages.find\nUses the find utility to search for packages to include, based on my understanding it looks for __init__.py in a directory and includes it (see above note about these no longer being required in every directory). Typically you would want to exclude tests/ from a package you are making as most users won’t need to run the test suite (if they do they would clone from the source repository).\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"tcx2gpx\"]\nexclude = [\"tests\"]\n\n\n\ntool.setuptools.package-data\nThis allows additional, non .py files to be included, they are listed on a per package basis and are a table (in toml parlance, list in Python terms).\n  [tool.setuptools.packages-data]\n  tcx2gpx = [\"*.yaml\", \"*.json\"]\n\n\ntool.pytest\n[tool.pytest.ini_options]\nminversion = \"7.0\"\naddopts = \"--cov --mpl\"\ntestpaths = [\n    \"tests\",\n]\nfilterwarnings = [\n    \"ignore::DeprecationWarning\",\n    \"ignore::UserWarning\"\n]\n\n\ntool.black\n[tool.black]\nline-length = 120\ntarget-version = [\"py38\", \"py39\", \"py310\", \"py311\"]\nexclude = '''\n\n(\n  /(\n      \\.eggs         # exclude a few common directories in the\n    | \\.git          # root of the project\n    | \\.venv\n  )/\n)\n'''\n\n\ntool.flake8\nThe developers of Flake8 will not be supporting pyproject.toml for configuration. This is a shame but a work around is available in the form of Flake8-pyproject. Make sure to add this to your requirements section to ensure it is installed when people use pre-commit.\n[tool.flake8]\nignore = ['E231', 'E241']\nper-file-ignores = [\n    '__init__.py:F401',\n]\nmax-line-length = 120\ncount = true\n\n\ntool.setuptools_scm\nsetuptools_scm is a simple to use extension to setuptools that dynamically sets the package version based on the version control data. It is important to note that by default setuptools_scm will attempt to bump the version of the release. The following configuration forces the use of the current git tag.\n[tool.setuptools_scm]\nwrite_to = \"tcx2gpx/_version.py\"\nversion_scheme = \"post-release\"\nlocal_scheme = \"no-local-version\"\ngit_describe_command = \"git describe --tags\"\n\n\ntool.ruff\nruff is a Python linter written in Rust which is therefore very fast. It provides the same functionality as black, flake8 and pylint and can auto-correct many issues if configured to do so. A GitHub Actions is also available. I'd recommend checking it out.\n[tool.ruff]\nfixable = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"R\", \"S\", \"W\", \"U\"]\nunfixable = []"
  },
  {
    "objectID": "posts/python-packaging/index.html#versioning",
    "href": "posts/python-packaging/index.html#versioning",
    "title": "Python Packaging",
    "section": "Versioning",
    "text": "Versioning\nTypically the version is defined in the __version__ variable/object in the top-level __init__.py or as a value in [metadata] of either setup.cfg or pyproject.toml but this has some downsides in that you have to remember to update the string manually when you are ready for a release and it doesn't tie in with using tags in Git to tag versions of your commits.\nIt is worth taking a moment to read and understand about Semantic Versioning which you are likely to use when tagging versions of your software to work with setuptools_scm.\n\nSetuptools-scm\nsetuptools_scm is simpler to setup and use than versioneer as it relies solely on configuration via pyproject.toml rather than being dependent on now deprecated setup.py.\nAs shown above you should have set the minimum versions of \"setuptools&gt;=45\" and \"setuptools_scm[toml]&gt;=6.2\", dynamic = [\"version\"] under project and set the write_to = \"pkg/_version.py\" (NB substitute pkg for your package directory, whether its src or the package name).\n[build-system]\nrequires = [\"setuptools&gt;=65.6.3\", \"setuptools_scm[toml]&gt;=6.2\"]\n\n[project]\ndynamic = [\"version\"]\n\n[tool.setuptools_scm]\nwrite_to \"pkg/_version.py\"\nversion_scheme = \"post-release\"\nlocal_scheme = \"no-local-version\"\ngit_describe_command = \"git describe --tags\"\n\nIncluding Version in Sphinx Documentation\nIf you have Sphinx documentation you can add the following to docs/conf.py\nfrom importlib.metadata import version\nrelease = version(\"myproject\")\nversion = \".\".join(release.split(\".\")[:2])"
  },
  {
    "objectID": "posts/python-packaging/index.html#building-your-package",
    "href": "posts/python-packaging/index.html#building-your-package",
    "title": "Python Packaging",
    "section": "Building your Package",
    "text": "Building your Package\n\nGenerate Distribution Archive\nIn your package directory you can create a distribution of your package with the latest versions of setuptools and wheel. To do this in your virtual environment run the following. The documentation for how to do this is at Building and Distributing Packages with Setuptools.\n[build-system]\nrequires = [\n  \"setuptools &gt;= 65.6.3\",\n  \"wheel\",\n]\nbuild-backend = \"setuptools.build_meta\"\nThe package can now be built locally with…\npython -m pip install --upgrade setuptools wheel\npython -m build --no-isolation\n…and the resulting package will be generated in the dist/ directory."
  },
  {
    "objectID": "posts/python-packaging/index.html#publishing-to-pypi",
    "href": "posts/python-packaging/index.html#publishing-to-pypi",
    "title": "Python Packaging",
    "section": "Publishing to PyPI",
    "text": "Publishing to PyPI\nBefore pushing the package to the main PyPi server it is prudent to test things out on TestPyPI first. You must first generate an API Token from your account settings page. It needs a name and the scope should be `Entire account (all projects)`. This token will be shown once so do not navigate away from the page until you have copied it.\nYou use twine to upload the package and should create a .pypirc file in the root of the package directory that contains your API key and the username __token__. For the TestPyPI server it follows the following format.\n[testpypi]\n  username = __token__\n  password = pypi-dfkjh9384hdszfkjnkjahkjfhd3YAJKSHE0089asdf0lkjsjJLLS_-0942358JKHDKjhkljna39o854yurlaoisdvnzli8yw459872jkhlkjsdfkjhasdfadsfasdf\nOnce this is in place you are ready to use twine to upload the package using the configuration file you have just created.\ntwine upload --config-file ./.pypirc --repository testpypi dist/*\n\nTesting Download\nAfter having uploaded your package to the TestPyPI server you should create a clean virtual environment and try installing the package from where you have just uploaded it. You can do this using pip and the --index-url and --extra-index-url, the former installs your package from TestPyPI, the later installs dependencies from PyPI.\npip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ your-package\nOnce installed you can try running the code, scripts or notebooks associated with the package as you would normally.\n\n\nRepeat for PyPI\nOnce you are happy this is working you can repeat the process on the main PyPI server. You can add the token that you generate to /.pypirc under a separate heading.\n[testpypi]\n  username = __token__\n  password = pypi-dfkjh9384hdszfkjnkjahkjfhd3YAJKSHE0089asdf0lkjsjJLLS_-0942358JKHDKjhkljna39o854yurlaoisdvnzli8yw459872jkhlkjsdfkjhdfJZZZZZF\n[pypi]\n  username = __token__\n  password = pypi-dfkjh9384hdszfkjnkjahkjfhd3YAJKSHE0089asdf0lkjsjJLLS_-0942358JKHDKjhkljna39o854yurlaoisdvnzli8yw459872jkhlkjsdfkjhdfJZZZZZF\n\n\nGitHub Action\nManually uploading is somewhat time consuming and tedious. Fortunately though with setuptools_scm in place and tokens generated we can automate the process of building and uploading packages to PyPI using the GitHub Action gh-action-pypi-publish (read more about GitHub Actions). You will have already generated a PYPI token (and similarly one for test PyPI) and these can stored on the projects GitHub account under Settings &gt; Secrets &gt; Actions with the names PYPI_API_TOKEN and TEST_PYPI_API_TOKEN respectively. You can then add the following GitHub Action under .github/workflow/pypi.yaml.\nname: Publish package to PyPi\n\non:\n  push:\ntags:\n  - v*\njobs:\n  build-release:\nruns-on: ubuntu-latest\nname: Publish package to PyPi\nsteps:\n  - uses: actions/checkout@v3\n    with:\n      fetch-depth: 0\n  - name: Setup Python\n    uses: actions/setup-python@v4.3.0\n    with:\n      python-version: 3.9\n      cache: 'pip'\n  - name: Installing the package\n    run: |\n      pip3 install .\n      pip3 install .[pypi]\n  - name: Build package\n    run: |\n      python -m build --no-isolation\n  - name: Publish package to PyPI\n    uses: pypa/gh-action-pypi-publish@release/v1\n    with:\n      user: __token__\n      password: ${{ secrets.PYPI_API_TOKEN }}\n\n\nReleasing via GitHub\nWith setuptools_scm in place and a GitHub Action setup and configured it is now possible to make a release to PyPI via GitHub Releases.\n\nGo to the Releases page (its linked from the right-hand side of the front-page).\nDraft a New release.\nCreate a new tag using semantic versioning and select “Create new tag v#.#.# on publish”.\nClick the \"Generate Release Notes\" button, this adds all the titles for all Pull Requests, I'll often remove all these but leave the link to the ChangeLog that is generated for the release.\nWrite your release notes.\nSelect \"Set as latest release\".\nSelect \"Create a discussion for this releases\" and select \"Announcements\".\nClick on \"Publish Release\"."
  },
  {
    "objectID": "posts/python-packaging/index.html#packaging-frameworks",
    "href": "posts/python-packaging/index.html#packaging-frameworks",
    "title": "Python Packaging",
    "section": "Packaging Frameworks",
    "text": "Packaging Frameworks\nThere are some frameworks that are meant to ease the pain of this process and make it easier. I'm yet to test these for two reasons. Firstly I wanted to understand what is going on rather than learn another framework. Secondly it was an additional framework to learn.\n\nPDM\nPDM (Python package and Dependency Manager) handles all stages of setting up and creating a package and managing its dependencies. In essence its a tool for interactively generating the configuration files described above. I've not yet.\n\n\nPoetry\nPoetry is another package for managing packaging and dependencies. Again, I've not yet used it."
  },
  {
    "objectID": "posts/python-packaging/index.html#links",
    "href": "posts/python-packaging/index.html#links",
    "title": "Python Packaging",
    "section": "Links",
    "text": "Links\n\nPyPA : Building and Distributing Packages with Setuptools\nPyPA : Specifications\nPackaging Python Projects\nPython package structure information — pyOpenSci Python Packaging Guide\nPackaging Data files in a Python Distribution\nPDM - Python package and Dependency Manager\nWhy you shouldn't invoke setup.py directly\npython-versioneer/python-versioneer: version-string management for VCS-controlled trees\npypa/setuptoolsscm: the blessed package to manage your versions by scm tags\nrye one-shop-stop for Python"
  },
  {
    "objectID": "posts/browser-extensions/index.html",
    "href": "posts/browser-extensions/index.html",
    "title": "Browser Extensions",
    "section": "",
    "text": "Most people use web-browsers a fair bit. There are a number of extensions available which make their use more stream-lined and efficient. This post covers those that I use. If there is something that you use regularly and find useful I’d love to hear about it (see links at top of page)."
  },
  {
    "objectID": "posts/browser-extensions/index.html#browser-choice",
    "href": "posts/browser-extensions/index.html#browser-choice",
    "title": "Browser Extensions",
    "section": "Browser Choice",
    "text": "Browser Choice\nI deliberately eschew Chrome and even its open-source relative Chromium because of the tracking built into the system. Instead I use Firefox as my main browser and for work I use Opera. I’ve also dabbled with Vivaldi which I quite like and should use more.\nMost of the plugins discussed here work across browsers, although for Opera and Vivaldi it is often the case of installing the Chrome extensions which work because of the development/toolkit on which the browser is based."
  },
  {
    "objectID": "posts/browser-extensions/index.html#pluginsextensions",
    "href": "posts/browser-extensions/index.html#pluginsextensions",
    "title": "Browser Extensions",
    "section": "Plugins/Extensions",
    "text": "Plugins/Extensions\nPlugins/extensions add additional functionality to your browser. I use many for blocking adverts and trackers and I list those below and find they make browsing cleaner and faster (I also block many such sites via my router but that is a separate post). But protecting your privacy is not the only purpose of extensions, many can streamline your browser usage and workflow and that is the main focus of this article.\nFirefox Extensions are available at Firefox Extensions, as mentioned those for Opera and Vivaldi are typically installed via the Chrome Web Store.\n\nBibItNow\nThis is invaluable if you undertake any academic work and use citations. Once installed it adds a short-cut for generating BibTex and other citation formats from the page being visited. This can then be copy and pasted into your database with a few keystrokes.\nIt is possible to customise the fields that are included e.g. by default Abtract is not included, but its something I like to include in my citation database for a quick overview of what a paper is about.\n\n\nCopy URL to Clipboard\nLinks are the blood of the internet and when reading and taking notes I like to link to the source I am using. This plugin makes it a doddle and will create a link to the page that is being viewed (or highlighted text) to a range of formats including Markdown, Org-mode, LaTeX, reStructuredText.\nBinding each link type to specific keys means its incredibly easy to copy and paste links from browser to Emacs (where I do most of my writing).\n\n\nUnpaywall\nMany papers are, unfairly given the research that paid for them is often from the public purse, behind PayWalls. Thankfully with the rise of pre-print servers such as arXiv, biorXiv, F1000 and the forthcoming Octopus pre-prints and alternatives to pay-walled articles are available and Unpaywall is a plugin that automatically finds them for you. Install it and when you visit an articles page if its behind a paywall but available freely elsewhere a green-symbol with an unlocked padlock appears on the right-hand side of your browser. If its not available this is a locked padlock on a grey background.\n\n\nBrowserPass\nEveryone should use a Password Manager of some description, I use Pass: The Standard Unix Password Manager and to get it to work seamlessly with my browsers I use BrowserPass. It requires a little configuration so read the GitHub page carefully but once working it is seamless. I visit a web-site and because I organise my passwords to include the URL all I need to do is use Ctrl-Shift-f and if my GPG key is unlocked the password is entered for me. My GPG key is unlocked using my Yubikey so if this isn’t plugged in and unlocked I’m prompted to do so. Makes logging in to web-sites so much faster.\n\n\norg-capture\nProbably only useful if you use the amazing “Capture” web-site, title and selected text to Emacs in Org-mode via org-protocol.\n\n\nGitLab Notify\nGet notifications from GitLab in your browser.\n\n\nOctotree\nNot used this much as I only discovered it recently whilst working on this article but it improves navigation of GitHub repositories. The main, free, feature that is of most use is a sidebar to aid navigation of a repositories code. This can be pinned if required."
  },
  {
    "objectID": "posts/browser-extensions/index.html#keyboard-shortcuts",
    "href": "posts/browser-extensions/index.html#keyboard-shortcuts",
    "title": "Browser Extensions",
    "section": "Keyboard Shortcuts",
    "text": "Keyboard Shortcuts\nBinding actions you take with extensions to keyboard short-cuts can save a considerable amount of time, particularly if you are a heavy typist, as it saves the small amount of time taken to move the hand to the mouse, locate the pointer and move it to where it needs to be.\nNot every extension has shortcuts associated with it, but for those that do in Firefox you can configure this by going to Add-ons and themes and at the top-right of the page listing the installed extensions, adjacent to Manage Your Extensions is a cog. Left-click on this once and a menu appears and at the bottom you can select Manage Extension Shortcuts. This allows you to bind “key-chords” (combinations of keys) to each plugins action. If there are conflicts (i.e. the same key-binding is bound to two actions) then these are highlighted and can be corrected."
  },
  {
    "objectID": "posts/browser-extensions/index.html#summary",
    "href": "posts/browser-extensions/index.html#summary",
    "title": "Browser Extensions",
    "section": "Summary",
    "text": "Summary\nThese are but a few of the vast array of productivity extensions you can make use of. Which you find useful and would use will be dependent on your browser usage and work\n\nProductivity Extensions\n\n\n\nExtension\nDescription\nShortcut\n\n\n\n\nBibItNow!\nCreates BibTex and other citation formats from web-pages. Really useful when browsing for journal articles, books and other sites that you want to add to your citation database.\nAlt-C\n\n\nCopy URL to Clipboard\nCopy the URL of a page along with its title (or selected text) to any number of different link formats.\nAlt-m (Markdown); Alt-o (Org-mode).\n\n\nUnpaywall\nAutomatically provides links to free versions of pay-walled journal articles and books.\nNot Required\n\n\nBrowserPass\nAuto-fill website login details stored in your Pass: The Standard Unix Password Manager.\nCtrl+Shift-F\n\n\norg-capture\n“Capture” web-site, title and selected text to Emacs via org-protocol.\nCtrl-Shift-L\n\n\nGitLab Notify\nGet notifications from GitLab in your browser.\nNot Available\n\n\nSci-Hub Now!\nAccess papers on Sci-Hub.\nNot Available\n\n\n\n\n\nPrivacy Protecting Extensions\nI’ve not gone into detail about the privacy protecting extensions I use but have listed them below. There is overlap/redundancy in what I’m using but that’s not necessarily a bad thing. I do find it breaks some sites (e.g. Amazon) but that isn’t necessarily a bad thing as it encourages me to shop more ethically and I can always switch browsers if needs be.\n\n\n\nExtension\nDescription\n\n\n\n\nClearURLs\nAutomatically remove tracking elements from URLs to help protect your privacy.\n\n\nCookieBlock\nAutomating Cookie Consent and GDPR Violation Detection.\n\n\nDecentralEyes\nProtects you against tracking through “free”, centralized, content delivery. It prevents a lot of requests from reaching networks like Google Hosted Libraries, and serves local files to keep sites from breaking. Complements regular content blockers.\n\n\nDuckDuckGo Privacy Essentials\nTracker blocking, cookie protection, DuckDuckGo private search, email protection, HTTPS upgrading and more.\n\n\nHTTPS Everywhere\nFrom the Electronic Frontier Foundation, retired 2023 as most browser can be set up to use HTTPS by Default\n\n\nI don’t care about cookies 3.4.6\nGet rid of cookie warnings from almost all websites.\n\n\nTemporary Containers\nEnhance your privacy in Firefox with Temporary COntainers\n\n\nUTM Remover\nRemove Google Analytics UTM tracking parameters fromURLS for privacy.\n\n\nUntrackMe\nRemoves parts of URLs that track you (also worth enabling Do Not Track).\n\n\nuBlock Origin\nBlock adverts"
  },
  {
    "objectID": "posts/pre-commit-updates/index.html",
    "href": "posts/pre-commit-updates/index.html",
    "title": "Pre-Commit : Customising and Updating",
    "section": "",
    "text": "Pre-commit is a tool for running hooks prior to making commits to your Git history. If you’re not familiar with it then you may want to read the earlier post Pre-Commit : Protecting your future self. This article discusses updating pre-commit and is prompted by a change in the flake8 repository."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#pre-commit-hooks",
    "href": "posts/pre-commit-updates/index.html#pre-commit-hooks",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Pre-commit hooks",
    "text": "Pre-commit hooks\nA lot of the power of pre-commit comes from the vast array of hooks that are available that users make available. These are included under repos: section of the .pre-commit-config.yaml and typically require a minimum of the repo: and the rev: to use and then optionally a hooks: section. The sample-config that pre-commit will auto-generate looks like…\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\nAfter finding a repository and hook that you wish to use hooks repository you need to add it to your .pre-commit-config.yaml. Here we add the pylint repository and whilst it only has one hook we explicitly add it.\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n-   repo: https://github.com/PyCQA/pylint\n    rev: v2.15.5\n    hooks:\n    -   id: pylint\nIf a repository has more than one hook available then it can be enabled by listing its id: as is the case in the hooks above for the pre-commit-hooks repository."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#local-hooks",
    "href": "posts/pre-commit-updates/index.html#local-hooks",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Local Hooks",
    "text": "Local Hooks\nIn some instances the provisioned repositories do not always meet the requirements. One example of this is the pylint action which parses the code-base to detect errors using pylint. Typically most Python packages have their own dependencies but because the Pylint action pulls down and uses its own virtual environment these packages are not installed. As a consequence pylint reports a lot of import-error as its unable to import the required dependencies.\nThe solution to this is to write a local hook, which instead of defining a GitHub repository as the repo: uses the value local. Thus to run pylint in a local environment from pre-commit you would add the following to your .pre-commit-config.yaml\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n# -   repo: https://github.com/PyCQA/pylint\n#     rev: v2.15.5\n#     hooks:\n#     -   id: pylint\n-   repo: local\n    hooks:\n    -   id: pylint\n        name: PyLint\n        entry: python -m pylint.__main__\n        language: system\n        files: \\.py$\nFor this to work you would have to ensure that you have a virtual environment activated that includes the package dependencies, including pylint, when you make you git commit so that pre-commit can find and import all the required packages."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#updating-pre-commit",
    "href": "posts/pre-commit-updates/index.html#updating-pre-commit",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Updating pre-commit",
    "text": "Updating pre-commit\nAfter adding a new repo and hook it will not be immediately ready to use as the environment has not been initialised. You can wait until your next commit or force this with the autoupdate option. This will update all repositories that are defined in your configuration.\n$ pre-commit autoupdate\nUpdating https://github.com/pre-commit/pre-commit-hooks ... updating v3.2.0 -&gt; v4.3.0.\nUpdating https://github.com/PyCQA/pylint ... [INFO] Initializing environment for https://github.com/PyCQA/pylint.\nalready up to date."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#repository-changes",
    "href": "posts/pre-commit-updates/index.html#repository-changes",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Repository Changes",
    "text": "Repository Changes\nSometimes, albeit rarely, repositories change their location as was the case recently when flake8 moved from GitLab to GitHub. As a consequence any pre-commit that uses flake8 repo/hook and configured to run in Continuous Integration pipelines failed as it was unable to download and run the flake8 environment. The solution is simply to update the repo:.\nBefore this change the entry for flake8 looked like…\n-   repo: https://gitlab.com/pycqa/flake8.git\n    rev: 3.9.2\n    hooks:\n    -   id: flake8\n        additional_dependencies: [flake8-print]\n        args: [\"topostats\", \"tests\"]\n        types: [python]\nTo update to use the new repository it should point to github.com as shown below.\n-   repo: https://github.com/pycqa/flake8.git\n    rev: 3.9.2\n    hooks:\n    -   id: flake8\n        additional_dependencies: [flake8-print]\n        args: [\"topostats\", \"tests\"]\n        types: [python]\nAfter making this change you have to pre-commit autoupdate to force downloading and updating from the new source, otherwise your existing older revision will be used locally."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#links",
    "href": "posts/pre-commit-updates/index.html#links",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Links",
    "text": "Links\n\nPre-Commit : Protecting your future self\nPre-commit\nPre-commit hooks\npylint\nflake8"
  },
  {
    "objectID": "posts/pytest-param/index.html",
    "href": "posts/pytest-param/index.html",
    "title": "Pytest Parameterisation",
    "section": "",
    "text": "Pytest is an excellent framework for writing tests in Python. One of the neat features it includes is the ability to parameterise your tests which means you can write one test and pass different sets of parameters into it to test the range of actions that the function/method are meant to handle."
  },
  {
    "objectID": "posts/pytest-param/index.html#example",
    "href": "posts/pytest-param/index.html#example",
    "title": "Pytest Parameterisation",
    "section": "Example",
    "text": "Example\nA simple example to work through is provided in my ns-res/pytest_examples repository. We want to have a state where the function can fail so we’ll use a very simple function that carries out division.\ndef divide(a: float | int, b: float | int) -&gt; float:\n    \"\"\"Divide a by b.\n\n    Parameters\n    ----------\n    a: float | int\n        Number to be divided.\n    b: float | int\n        Number to divide by.\n\n    Returns\n    -------\n    float\n        a divided by b.\n    \"\"\"\n    try:\n        return a / b\n    except TypeError as e:\n        if not isinstance(a, (int | float)):\n            raise TypeError(f\"Error 'a' should be int or float, not {type(a)}\") from e\n        raise TypeError(f\"Error 'b' should be int or float, not {type(b)}\") from e\n    except ZeroDivisionError as e:\n        raise ZeroDivisionError(f\"Can not divide by {b}, choose another number.\") from e"
  },
  {
    "objectID": "posts/pytest-param/index.html#structuring-tests",
    "href": "posts/pytest-param/index.html#structuring-tests",
    "title": "Pytest Parameterisation",
    "section": "Structuring Tests",
    "text": "Structuring Tests\nPytest is well written and will automatically find your tests in a few places. Personally I use a flat rather than src/ based package layout and keep my tests in the tests/ directory of the package root. Pytest looks in this directory automatically for files that begin with test_ and within each file for functions/methods that begin with test_.\nWith the above function we could write the following basic test to make sure it works because we know that if we divide 10 by 5 we should get 2 as the answer.\nfrom pytest_examples.divide import divide\n\n\ndef test_divide_unparameterised() -&gt; None:\n    \"\"\"Test the divide function.\"\"\"\n    assert divide(10, 5) == 2\nYou can find this test along with others in the tests/test_divide.py file of the accompanying repository."
  },
  {
    "objectID": "posts/pytest-param/index.html#parameterising-tests",
    "href": "posts/pytest-param/index.html#parameterising-tests",
    "title": "Pytest Parameterisation",
    "section": "Parameterising Tests",
    "text": "Parameterising Tests\nIn order to make our test suite robust we should test more scenarios and edge cases, in particular making sure we capture the exceptions that can be raised. This is where the pytest.mark.parameterize() fixture comes into play. It takes as a first argument a tuple of variables that you are going to define values for and pass into your test. Following it is a list of tuples with the values that you want to include, one for each of the variables you have first defined. Here we define a, b and the expected value of dividing a by b which is the value the divide() function should return.\nIf we expand the number of scenarios we wish to test using @pytest.mark.parametrize() we can write our test as follows.\nimport pytest\n\nfrom divide import divide\n@pytest.mark.parametrize(\n    (\"a\", \"b\", \"expected\"),\n    [\n        (10, 5, 2),\n        (9, 3, 3),\n        (5, 2, 2.5),\n\n    ]\n)\ndef test_divide(a: float | int, b: float | int, expected: float) -&gt; None:\n    \"\"\"Test the divide function.\"\"\"\n    assert divide(a, b) == expected"
  },
  {
    "objectID": "posts/pytest-param/index.html#parameter-set-ids",
    "href": "posts/pytest-param/index.html#parameter-set-ids",
    "title": "Pytest Parameterisation",
    "section": "Parameter set IDs",
    "text": "Parameter set IDs\nFor some time I simply wrote my tests and if the structure was complicated I used comments to mark the code to indicate what the test was doing. When they (inevitably!) failed there was a cryptically long indication of what had failed based on the filename, test name and the values of the various parameters that were in use at the point of failure. These helped narrow down which test failed but took a bit of mental over-head to decipher.\nFor the above test without ID’s we can force them to fail by adding 1 to the expected value (i.e. == expected + 1) and the resulting output shows how the parameters are concatenated to indicate which test failed.\n======================= short test summary info ====================================\nFAILED tests/test_divide.py::test_divide_fail[10-5-2] - assert 2.0 == (2 + 1)\nFAILED tests/test_divide.py::test_divide_fail[9-3-3] - assert 3.0 == (3 + 1)\nFAILED tests/test_divide.py::test_divide_fail[5-2-2.5] - assert 2.5 == (2.5 + 1)\n======================= 3 failed in 0.79s ==========================================\nWhilst it is possible to work out which failed test is which if you have many sets of parameters with multiple values and only one or two are failing it can take a while to work out which set has failed.\nRecently though I was put onto the pytest.param() function by a toot from @danjac@masto.ai and instantly saw the benefit of using this as it allows us to give each set of parameters a unique id which is then used by Pytest when reporting failures.\n@pytest.mark.parameterize(\n    (\"a\", \"b\", \"expected\"),\n    [\n        pytest.param(10, 5, 2, id=\"ten divided by five\"),\n        pytest.param(9, 3, 3, id=\"nine divided by three\"),\n        pytest.param(5, 2, 2.5, id=\"five divided by two\"),\n\n    ]\n)\ndef test_divide(a: float | int, b: float | int, expected: float) -&gt; None:\n    \"\"\"Test the divide function.\"\"\"\n    assert divide(a, b) == expected\nThen if/when a test fails the id parameter is reported for the failed test, making it much easier to narrow down where the failure occurred.\nNot only does it allow each set of parameters to be given a unique id = \"\" to aid with identifying tests that fail it also allows each set of parameters to be marked with marks = &lt;&gt; to indicate the expected behaviour for example pytest.mark.xfail or pytest.mark.skipif.\nWe could therefore add another set of parameters that should fail because one of the exceptions is raised.\nimport pytest\n\nfrom pytest_examples.divide import divide\n\n\n@pytest.mark.parameterize(\n    (\"a\", \"b\", \"expected\"),\n    [\n        pytest.param(10, 5, 2, id=\"ten divided by five\"),\n        pytest.param(9, 3, 3, id=\"nine divided by three\"),\n        pytest.param(5, 2, 2.5, id=\"five divided by two\"),\n        pytest.param(\n            10, 0, ZeroDivisionError, id=\"zero division error\", marks=pytest.mark.xfail\n        ),\n    ],\n)\ndef test_divide(a: float | int, b: float | int, expected: float) -&gt; None:\n    \"\"\"Test the divide function.\"\"\"\n    assert divide(a, b) == expected"
  },
  {
    "objectID": "posts/pytest-param/index.html#testing-exceptions",
    "href": "posts/pytest-param/index.html#testing-exceptions",
    "title": "Pytest Parameterisation",
    "section": "Testing Exceptions",
    "text": "Testing Exceptions\nThe above example shows that Pytest allows us to combine tests that pass and fail (in the above example a ZeroDivisionError) via parmeterisation. However, whilst tests can and should be parameterised, some consider that it is better to keep tests focused and on-topic and write a separate test for different outcomes such as raising exceptions.\nThis is slightly different from the way the Pytest documentation suggests to undertake Parameterising conditional raising but there is a school of thought, which I like, which states that testing different states/behaviours should be separate (see the following thread for some discussion Why should unit tests test only one thing?).\nWith this in mind we can separate out the tests that raise exceptions under different scenarios to their own tests (NB obviously its excessive to parameterise test-divide_zero_division_error()).\n@pytest.mark.parametrize(\n    (\"a\", \"b\", \"exception\"),\n    [\n        pytest.param(\"a\", 5, TypeError, id=\"a is string\"),\n        pytest.param(9, \"b\", TypeError, id=\"b is string\"),\n        pytest.param([1], 2, TypeError, id=\"a is list\"),\n        pytest.param(10, [2], TypeError, id=\"b is list\"),\n    ],\n)\ndef test_divide_type_errors(a: float | int, b: float | int, exception: float) -&gt; None:\n    \"\"\"Test that TypeError is raised when objects other than int or float are passed as a and b.\"\"\"\n    with pytest.raises(exception):\n        divide(a, b)\n@pytest.mark.parametrize(\n    (\"a\", \"b\", \"exception\"),\n    [\n        pytest.param(10, 0, ZeroDivisionError, id=\"b is zero\"),\n    ],\n)\ndef test_divide_zero_division_error(a: float | int, b: float | int, exception: float) -&gt; None:\n    \"\"\"Test that ZeroDivsionError is raised when attempting to divide by zero.\"\"\"\n    with pytest.raises(exception):\n        divide(a, b)"
  },
  {
    "objectID": "posts/pytest-param/index.html#parameterising-with-fixtures",
    "href": "posts/pytest-param/index.html#parameterising-with-fixtures",
    "title": "Pytest Parameterisation",
    "section": "Parameterising with Fixtures",
    "text": "Parameterising with Fixtures\nFixtures are a common and useful feature of the Pytest framework that allow you to define “defined, reliable and consistent context for the tests”. What this means is that if you always need a particular object, whether that is an instantiated class (a new instance of a class) or something else, you can mark a function with @pytest.fixture() and use it in subsequent tests (often fixtures are defined in tests/conftest.py to keep things tidy, at least that is what I do!)1.\nIt can be useful to parameterise fixtures themselves so that they too test a number of different states and this saves writing more sets of parameters under the @pytest.mark.parameterize() decorator of each test.\nFor this example we use a simple function summarise_shapes() which returns the results of summarising a 2-D Numpy array using scikit-image and its skimage.measure.regionprops() function (see pytest_examples/shapes.py).\n\"\"\"Summarise Shapes.\"\"\"\nimport numpy.typing as npt\nfrom skimage import measure\n\n\ndef summarise_shape(shape: npt.NDArray) -&gt; list:\n    \"\"\"\n    Summarise the region properties of a 2D numpy array using Scikit-Image.\n\n    Parameters\n    ----------\n    shape : npt.NDArray\n        2D binary array of a shape.\n\n    Returns\n    -------\n    list\n        List of Region Properties each item describing one labelled region.\n    \"\"\"\n    return measure.regionprops(shape)\nWe want to write some tests for these using fixtures which we define in tests/conftest.py. These define two Numpy 2-D binary arrays of 0’s and 1’s in particular shapes (the names should give an indication of the shapes!)\nimport numpy as np\nimport numpy.typing as npt\nimport pytest\n\nfrom skimage import draw\n\n\n@pytest.fixture\ndef square() -&gt; npt.NDArray:\n    \"\"\"Return a 2D numpy array of a square.\"\"\"\n    square = np.zeros((6, 6), dtype=np.uint8)\n    start = (1, 1)\n    end = (5, 5)\n    rr, cc = draw.rectangle_perimeter(start, end, shape=square.shape)\n    square[rr, cc] = 1\n    return square\n\n\n@pytest.fixture\ndef circle() -&gt; npt.NDArray:\n    \"\"\"Return a 2D numpy array of a circle.\"\"\"\n    circle = np.zeros((7, 7), dtype=np.uint8)\n    rr, cc = draw.circle_perimeter(r=4, c=4, radius=2, shape=circle.shape)\n    circle[rr, cc] = 1\n    return circle\nThere are two different methods to using these fixtures in parameterised tests.\n\nrequest.getfixturevalue()\nThe first uses request.getfixturevalue() which “is a special fixture providing information of the requesting test function.”, in this case the “named fixture function”.\nYou define the fixture name (in quotes) in the @pytest.mark.parametrize() and then when the parameter, in this case shape, is referred to in the test itself, you wrap it in request.getfixturevalue() and the named fixture is then returned and used.\n\"\"\"Test the shapes module.\"\"\"\nimport pytest\n\nfrom pytest_examples.shapes import summarise_shape\n\n\n@pytest.mark.parametrize(\n    (\"shape\", \"area\", \"feret_diameter_max\", \"centroid\"),\n    [\n        pytest.param(\"square\", 11, 7.810249675906654, (1.3636363636363635, 1.3636363636363635), id=\"summary of square\"),\n        pytest.param(\"circle\", 12, 5.385164807134504, (4, 4), id=\"summary of circle\"),\n    ],\n)\ndef test_summarise_shape_get_fixture_value(\n    shape: str, area: float, feret_diameter_max: float, centroid: tuple, request\n) -&gt; None:\n    \"\"\"Test the summarisation of shapes.\"\"\"\n    shape_summary = summarise_shape(request.getfixturevalue(shape))\n    assert shape_summary[0][\"area\"] == area\n    assert shape_summary[0][\"feret_diameter_max\"] == feret_diameter_max\n    assert shape_summary[0][\"centroid\"] == centroid\n\n\npytest-lazy-fixture\nAn alternative is to use the Pytest plugin pytest-lazy-fixture and instead of marking the value to be obtained in the test itself you do so when setting up the parameters by referring to the fixture name as an argument to pytest.lazy_fixture() within @pytest.mark.parametrize().\n\"\"\"Test the shapes module.\"\"\"\nimport pytest\n\nfrom pytest_examples.shapes import summarise_shape\n\n\n@pytest.mark.parametrize(\n    (\"shape\", \"area\", \"feret_diameter_max\", \"centroid\"),\n    [\n        pytest.param(\n            pytest.lazy_fixture(\"square\"),\n            11,\n            7.810249675906654,\n            (1.3636363636363635, 1.3636363636363635),\n            id=\"summary of square\",\n        ),\n        pytest.param(pytest.lazy_fixture(\"circle\"), 12, 5.385164807134504, (4, 4), id=\"summary of circle\"),\n    ],\n)\ndef test_summarise_shape_lazy_fixture(\n    shape: str, area: float, feret_diameter_max: float, centroid: tuple, request\n) -&gt; None:\n    \"\"\"Test the summarisation of shapes.\"\"\"\n    shape_summary = summarise_shape(shape)\n    print(f\"{shape_summary[0]['centroid']=}\")\n    assert shape_summary[0][\"area\"] == area\n    assert shape_summary[0][\"feret_diameter_max\"] == feret_diameter_max\n    assert shape_summary[0][\"centroid\"] == centroid"
  },
  {
    "objectID": "posts/pytest-param/index.html#parameterise-fixtures",
    "href": "posts/pytest-param/index.html#parameterise-fixtures",
    "title": "Pytest Parameterisation",
    "section": "Parameterise Fixtures",
    "text": "Parameterise Fixtures\nThe pytest-lazy-fixture also allows fixtures themselves to be parameterised using the pytest_lazyfixture.lazy_fixture() function and demonstrated in the packages README which I’ve reproduced below.\nThe fixture called some() uses lazy_fixture() to include both the one() and the two() fixtures which return their respective integers. test_func() then checks that the value returned by the some() fixture is in the list [1, 2]. Obviously this example is contrived but it serves to demonstrate how fixtures themselves can be parameterised.\nimport pytest\nfrom pytest_lazyfixture import lazy_fixture\n\n@pytest.fixture(params=[\n    lazy_fixture('one'),\n    lazy_fixture('two')\n])\ndef some(request):\n    return request.param\n\n@pytest.fixture\ndef one():\n    return 1\n\n@pytest.fixture\ndef two():\n    return 2\n\ndef test_func(some):\n    assert some in [1, 2]"
  },
  {
    "objectID": "posts/pytest-param/index.html#conclusion",
    "href": "posts/pytest-param/index.html#conclusion",
    "title": "Pytest Parameterisation",
    "section": "Conclusion",
    "text": "Conclusion\nPytest is a powerful and flexible suite for writing tests in Python. One of the strengths is the ability to parameterise the tests to test multiple scenarios. This can include both successes and failures, however a common approach is to separate tests based on the expected behaviour, although Pytest allows you the flexibility to choose.\nUltimately though parameterising tests is a simple and effective way of reducing the amount of code you have to write to unit-tests for different aspects of your code."
  },
  {
    "objectID": "posts/pytest-param/index.html#links",
    "href": "posts/pytest-param/index.html#links",
    "title": "Pytest Parameterisation",
    "section": "Links",
    "text": "Links\n\nPytest\nParametrizing tests — pytest documentation\nSrc Layout vs Flat Layout\npytest using fixtures as arguments in parametrize - Stack Overflow\nHow do you solve multiple asserts?\nWhy should unit tests test only one thing? - Stack Overflow"
  },
  {
    "objectID": "posts/pytest-param/index.html#footnotes",
    "href": "posts/pytest-param/index.html#footnotes",
    "title": "Pytest Parameterisation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA caveat to this is the use of Random Number Generators as once seeded these can produce different numbers depending on the order in which the fixture is used but that is beyond the scope of this post.↩︎"
  },
  {
    "objectID": "posts/git-amend-fixup/index.html",
    "href": "posts/git-amend-fixup/index.html",
    "title": "Git - Making Amends and Fixing things up",
    "section": "",
    "text": "Git is the world’s most popular version control software out there at the moment and if you write code and version control it the chances are you are using it. It’s a complex tool though and there are a bewildering array of options. In this short post we will look at some options for changing commits that have been made."
  },
  {
    "objectID": "posts/git-amend-fixup/index.html#python-examples",
    "href": "posts/git-amend-fixup/index.html#python-examples",
    "title": "Git - Making Amends and Fixing things up",
    "section": "Python Examples",
    "text": "Python Examples\nWe’ll use my pytest-examples as an example for this work but you can do this in any repository you have going. We’ll clone the repository and make a new branch called amend-fixup-tutorial.\ngit clone git@github.com:ns-rse/pytest-examples.git\ncd pytest-examples\ngit switch -c amend-fixup-tutorial\n  Switched to a new branch 'amend-fixup-tutorial'"
  },
  {
    "objectID": "posts/git-amend-fixup/index.html#the-git-dance",
    "href": "posts/git-amend-fixup/index.html#the-git-dance",
    "title": "Git - Making Amends and Fixing things up",
    "section": "The Git Dance",
    "text": "The Git Dance\nA typical Git work flow involves making some changes to one or more files (or adding a new one into the repository). These are staged with git add &lt;files&gt;, or you can use a shortcut git add -u/--update to add all currently tracked files that have been modified if you are not adding anything new, before committing them with a message using git commit -m \"&lt;meaningful message about content being added&gt;\".\nLets add a simple CONTRIBUTING.md file to the repository.\necho \"# Contributing\\n\\nContributions via pull requests are welcome.\" &gt; CONTRIBUTING.md\ngit add CONTRIBUTING.md\ngit commit -m \"Adding CONTRIBUTING.md\"\ngit log --oneline\n  01191a2 (HEAD -&gt; amend-fixup-tutorial) Adding CONTRIBUTING.md\nThis should be familiar to users of Git whether you use the command line interface (CLI), Emacs’ amazing Git porcelain magit or any other tool such as GitKraken or the support in you IDE such as RStudio or VSCode."
  },
  {
    "objectID": "posts/git-amend-fixup/index.html#making-amends",
    "href": "posts/git-amend-fixup/index.html#making-amends",
    "title": "Git - Making Amends and Fixing things up",
    "section": "Making Amends",
    "text": "Making Amends\nSometimes you will have made a commit and you realise that you want to add more to it or perhaps you forgot to run your test suite and find that on running it your tests fail so you need to make a correction. In this example we want to be more explicit about how to make contributions and let people know they should fork the branch.\necho \"\\n Please make a fork of this repository, make your changes and open a Pull Request.\" &gt;&gt; CONTRIBUTING.md\nNow you could make a second commit…\ngit add -u\ngit commit -m \"Ask for PRs via fork in CONTRIBUTING.md\"\ngit log --oneline\n9f0655b (HEAD -&gt; amend-fixup-tutorial) Ask for PRs via fork in CONTRIBUTING.md\n01191a2 Adding CONTRIBUTING.md\n…and there is nothing wrong with that. However, Git history can get long and complicated when there are lots of small commits, because these two changes to CONTRIBUTING.md are essentially the same piece of work and if we’d been thinking clearly we would have written about making forks in the first place and made a single commit.\nFortunately Git can help here as there is the git commit --amend option which adds the staged changes to the last commit and allows you to edit the last commit message (if nothing is currently staged then you will be prompted to edit the last commit message). We can undo the last commit using git reset HEAD~1 and instead amend the first commit that added the CONTRIBUTING.md\ngit add -u\ngit commit --amend\ngit log --oneline\n  4fda15f (HEAD -&gt; amend-fixup-tutorial) Adding CONTRIBUTING.md\ncat CONTRIBUTING.md\n# Contributing\n\nContributions via pull requests are welcome.\n\nPlease make a fork of this repository, make your changes and open a Pull Request.\nWe now have one commit which contains the new CONTRIBUTING.md file that contains all the changes we wished to have in the file in the first place and our Git history is slightly more compact."
  },
  {
    "objectID": "posts/git-amend-fixup/index.html#fixing-things-up",
    "href": "posts/git-amend-fixup/index.html#fixing-things-up",
    "title": "Git - Making Amends and Fixing things up",
    "section": "Fixing things up",
    "text": "Fixing things up\nAmending commits is great providing the commit you want to change is the last commit you made (i.e. HEAD). But sometimes you might wish to correct a commit further back in your history and git commit --amend is of no use here. Git can however help here with the git commit --fixup command which allows you to mark a commit as being a “fix up” of an older commit. These can then be autosquashed via an interactive Git rebase.\nLet’s add a few empty commits to our amend-fixup-tutorial branch to so we can do this.\ngit commit --allow-empty -m \"Empty commit for demonstration purposes\"\ngit commit --allow-empty -m \"Another empty commit for demonstration purposes\"\ngit log --oneline\n  8061221 (HEAD -&gt; amend-fixup-tutorial) Another empty commit for demonstration purposes\n  65587ce Empty commit for demonstration purposes\n  4fda15f Adding CONTRIBUTING.md\nAnd let’s expand our CONTRIBUTING.md file further.\necho \"\\nPlease note this repository uses [pre-commit](https://pre-commit.com) to lint the Python code and Markdown files.\" &gt;&gt; CONTRIBUTING.md\nWe want to merge this commit with the first one we made in this tutorial using git commit --fixup. To do this we need to know the hash (4fda15f see output from above git log) or the relative reference of the commit we want which in this case is HEAD~2 as it is three commits back from the current HEAD (which is commit 0, most indexing in computing starts at 0 rather than 1). Use one for the following git commit --fixup commands (adjusting the hash to yours if you are using that option, you can find this using git log --oneline).\ngit add -u\ngit commit --fixup 4fda15f\ngit commit --fixup HEAD~2\nWe see the commit we have just made starts with fixup! and is then followed by the commit message that it is fixing.\ngit log --oneline\n  97711a4 (HEAD -&gt; amend-fixup-tutorial) fixup! Adding CONTRIBUTING.md\n  8061221 Another empty commit for demonstration purposes\n  65587ce Empty commit for demonstration purposes\n  4fda15f Adding CONTRIBUTING.md\nThe final step is to perform the automatic squashing via an interactive rebase, again you can either use the hash or the relative reference.\ngit rebase -i --autosquash 4fda15f\ngit rebase -i --autosquash HEAD~2\nThis will open the default editor and because the --autosquash option has been used it will already have marked the commits that need combining with fixup. All you have to do is save the file and exit and we can check the history and look at the contents of the file.\nNB If you find that the necessary commit isn’t already marked navigate to that line and delete pick. The lines below the file you have open give instructions on how you can mark commits for different actions, in this case you can replace pick with either f or fixup. Save and exit and the commits are squashed.\ngit log --oneline\n  0fda21e (HEAD -&gt; amend-fixup-tutorial) Another empty commit for demonstration purposes\n  65587ce Empty commit for demonstration purposes\n  4fda15f Adding CONTRIBUTING.md\ncat CONTRIBUTING.md\n  # Contributing\n\n  Contributions via pull requests are welcome.\n\n  Please make a fork of this repository, make your changes and open a Pull Request.\n\n  Please note this repository uses [pre-commit](https://pre-commit.com) to lint the Python code and Markdown files.\nAnd you’re all done! If you were doing this for real on a repository you could now git push or continue your work. As this was just an example we can switch branches back to main and force deletion of the branch we created.\ngit switch main\ngit branch -D amend-fixup-tutorial"
  },
  {
    "objectID": "posts/git-amend-fixup/index.html#conclusion",
    "href": "posts/git-amend-fixup/index.html#conclusion",
    "title": "Git - Making Amends and Fixing things up",
    "section": "Conclusion",
    "text": "Conclusion\nGit has lots of commands to help you maintain a clean history by using --amend and --fixup flags to git commit and in the later case then performing an interactive git rebase -i. This takes a little discipline to get into the practice of but once in the habit of doing so it greatly improves the readability of the Git history and avoids including commit messages such as Fixing typo / Linting code / Fixing tests / I've gone mad!.\nIf all of this sounds completely unfamiliar to you but you would like to learn more about Git I can highly recommend the introductory course developed by Dr Anna Krystalli Git and GitHub through GitKraken : From Zero to Hero. This course is run regularly by myself and colleagues in Research Software Engineering for post-graduate researchers and staff at the University of Sheffield."
  },
  {
    "objectID": "posts/pytest-mpl/index.html",
    "href": "posts/pytest-mpl/index.html",
    "title": "Pytest Matplotlib",
    "section": "",
    "text": "Pytest is an excellent framework for writing tests in Python. Sometimes the code you want to test will generate images using Matplotlib or related libraries based around it such as Seaborn or plotnine and ideally your pytest suite should check that your functions generate the images you expect. There is an excellent pytest plugin to help with this pytest-mpl and this post covers how to use it and a very useful feature to help you investigate differences in images when your tests of images fail."
  },
  {
    "objectID": "posts/pytest-mpl/index.html#installation",
    "href": "posts/pytest-mpl/index.html#installation",
    "title": "Pytest Matplotlib",
    "section": "Installation",
    "text": "Installation\nSetup a new virtual environment, here I use virtualenvwrapper to create a temporary virtual environment (which will be deleted on deactivation) and install the pytest-mpl package (it pulls in matplotlib, pytest and a bunch of other dependencies).\nmktmpenv\npip install pytest-mpl"
  },
  {
    "objectID": "posts/pytest-mpl/index.html#function-and-test",
    "href": "posts/pytest-mpl/index.html#function-and-test",
    "title": "Pytest Matplotlib",
    "section": "Function and Test",
    "text": "Function and Test\nWe need a simple example to write tests for. As with previous posts I’ve made the code available in the pytest-examples repository. You can clone this repository to follow along and play around with this worked example.\ngit clone git@github.com:ns-rse/pytest-examples.git\ncd pytest-examples\n\nscatter()\nThis repository contains the module pytestexample/mpl_example.py and the function scatter() to plot a scatter-plot of two random variables along with a few common options.\n#| label: python-test\n#| code-fold: true\n#| code-link: true\n#| code-tools: true\n#| eval: false\n#| echo: true\n\"\"\"Example code for pytest-mpl exposition.\"\"\"\n\nimport numpy as np\nimport numpy.typing as npt\nimport matplotlib.pyplot as plt\n\n\ndef scatter(\n    n_obs: int,\n    figsize: tuple[int, int],\n    title: str | None = None,\n    seed: int = 3513387,\n) -&gt; tuple:\n    \"\"\"\n    Generate a scatter plot of two random variables.\n\n\n    Parameters\n    ----------\n    n_obs : int\n        Number of random observations to generate.\n    figsize : tuple[int, int]\n        Shape to plot.\n    seed : int\n        Seed for pseudo-random number generation.\n\n    Returns\n    -------\n    tuple(fig, ax)\n\n    \"\"\"\n    # Generate two random sets of numbers\n    rng = np.random.default_rng(seed)\n    x = np.random.randn(n_obs)\n    y = np.random.randn(n_obs)\n    # Create the figure\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot()\n    ax.scatter(x, y)\n    plt.title(title)\n\n    return (fig, ax)\n\n\nDefining a test\nWe now need a test to check that, for a given set of inputs, the same output is always returned. Following the naming conventions used by pytest we create this at tests/test_mpl_example.py. Matplotlib provides its own image_comparison decorator from Matplotlib itself which requires at a bare minimum the baseline_image. I’m going to skip over its usage and instead introduce the pytest-mpl extension as it offers a few extra features which are really neat. Instead of using the @image_comparsion() decorator we can add the pytest-mpl package to the optional test dependencies in pyproject.toml.\n[project.optional-dependencies]\ntests = [\n  \"py\",\n  \"pytest\",\n  \"pytest-cov\",\n  \"pytest-mpl\",\n  \"pytest-tmp-files\",\n]\nWe use the @pytest.mark.mpl_image_compare() decorator to mark a test as having Matplotlib output to indicate that we want to compare images. We need to set the baseline_dir which is where the images against which tests will be compared are stored here it is set to baseline which is relative to the position of the file. We then call our function, in this case mpl_example.scatter() with different sets of parameters (courtesy of pytest parameterisation).\nThe test itself must return fig of the desired plot so that it can be compared to the reference image each time the test is run.\n\n\n\n\n\n\nNote\n\n\n\nWe also use the pytest.mark.parametrize() to setup two test scenarios.\n\n\n\"\"\"Tests of the mpl_example module.\"\"\"\n\nimport pytest\n\nfrom pytest_examples import mpl_example\n\n\n@pytest.mark.mpl_image_compare(baseline_dir=\"baseline\")\n@pytest.mark.parametrize(\n    (\"n_obs\", \"figsize\", \"title\", \"seed\"),\n    [\n        pytest.param(300, (6, 6), \"\", 3513387, id=\"300 points; 6x6; no title\"),\n        pytest.param(3000, (6, 6), \"Lots of points!\", 3513387, id=\"3000 points; 6x6; Lots of points\"),\n    ],\n)\ndef test_scatter(n_obs: int, figsize: tuple[int, int], title: str, seed: int) -&gt; None:\n    \"\"\"Test of the scatter() function.\"\"\"\n    fig, _ = mpl_example.scatter(n_obs, figsize, title, seed)\n    return fig\nOnce you have decorated your test you need to generate the images against which subsequent tests are to be compared to. This is done using the --mpl-generate-path= flag and providing an appropriate argument. The path is relative to where pytest is running from, in this example we use tests/baseline. We can restrict the test to the specific one we are working by specifying the path to the file and optionally the test within the file as in the below example where the path to the test file (tests/test_mpl_example.py) and the test name (test_scatter_hist) are separated by double colons (::).\nmkdir -p tests/resources/img\npytest --mpl-generate-path=tests/baseline `tests/test_mpl_example.py::test_scatter`\n\n\n\n\n\n\nNote\n\n\n\nThere is subtle difference between the baseline_dir parameter specified in the fixture itself (baseline) and the argument given to --mpl-generate-path= (tests/baseline).\nThis is because pytest searches for all files beginning with test_ in the directory tests and when running pytest you do so from the directory level above where tests resides which is typically the root of your package. Because the test files reside within the tests/ directory the relative path to the directory the parameter baseline_dir argument must omit this leading directory.\n├── ./tests\n    ├── ./tests/baseline\n    └── ./tests/test_mpl_example.py\n\n\nThe tests are skipped “since generating image.”\n❱ pytest --mpl --mpl-generate-path=baseline tests/test_mpl_example.py\n======================== test session starts ==========================\nplatform linux -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0\nMatplotlib: 3.9.2\nFreetype: 2.6.1\nrootdir: /mnt/work/git/hub/ns-rse/pytest-examples/main\nconfigfile: pyproject.toml\nplugins: regtest-2.1.1, anyio-4.6.0, icdiff-0.9, pylint-0.21.0, pytest_tmp_files-0.0.2, syrupy-4.7.1, mpl-0.17.0, cov-5.0.0, mock-3.14.0, xdist-3.6.1, durations-1.3.1\ncollected 2 items\n\ntests/test_mpl_example.py ss                                                                                                                            [100%]\n\n---------- coverage: platform linux, python 3.12.7-final-0 -----------\nName                            Stmts   Miss  Cover\n---------------------------------------------------\npytestexamples/divide.py           16     16     0%\npytestexamples/mpl_example.py      13      0   100%\npytestexamples/shapes.py            5      5     0%\n---------------------------------------------------\nTOTAL                              34     21    38%\n\n======================== short test summary info =====================\nSKIPPED [2] ../../../../../../../home/neil/.virtualenvs/pytest-examples/lib/python3.12/site-packages/pytest_mpl/plugin.py:925: Skipped test, since generating image.\n======================== 2 skipped in 2.46s ===========================\nWe can look at the generated images, nothing fancy just some random dots.\n \nThe generated images reside within the tests/baseline/ directory and as the baseline_dir=baseline argument to the @pytest.mark.mpl_image_compare() is relative to the location of the test file itself which is in test we are good to go. We can re-run the tests with the --mpl flag to check they pass.\npytest --mpl tests/test_mpl_example::test_scatter\n======================== test session starts ==========================\nplatform linux -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0\nMatplotlib: 3.9.2\nFreetype: 2.6.1\nrootdir: /mnt/work/git/hub/ns-rse/pytest-examples/main\nconfigfile: pyproject.toml\nplugins: regtest-2.1.1, anyio-4.6.0, icdiff-0.9, pylint-0.21.0, pytest_tmp_files-0.0.2, syrupy-4.7.1, mpl-0.17.0, cov-5.0.0, mock-3.14.0, xdist-3.6.1, durations-1.3.1\ncollected 2 items\n\ntests/test_mpl_example.py ..                                                                                                                            [100%]\n\n---------- coverage: platform linux, python 3.12.7-final-0 -----------\nName                            Stmts   Miss  Cover\n---------------------------------------------------\npytestexamples/divide.py           16     16     0%\npytestexamples/mpl_example.py      13      0   100%\npytestexamples/shapes.py            5      5     0%\n---------------------------------------------------\nTOTAL                              34     21    38%\n\n=========================== 2 passed in 2.56s ===========================\n\n\nUpdate pytest options\nWe don’t want to have to remember to use the --mpl flag each time we run the test and we also want to make sure its used in Continuous Integration. This can be achieved by adding the --mpl flag to the pytest options that are defined in pyproject.toml.\n[tool.pytest.ini_options]\n  ...\n  addopts = [\"--cov\", \"--mpl\", \"-ra\", \"--showlocals\", \"--strict-config\", \"--strict-markers\"]\n  ...\n\n\n\n\n\n\nNote\n\n\n\nDon’t forget to stage and commit these changes to your repository.\n\n\nNow each time you run your test suite the output of calling the test is compared to the reference images that reside under tests/baseline/ (or whatever directory you specified)."
  },
  {
    "objectID": "posts/pytest-mpl/index.html#failing-tests",
    "href": "posts/pytest-mpl/index.html#failing-tests",
    "title": "Pytest Matplotlib",
    "section": "Failing tests",
    "text": "Failing tests\nSometimes your tests might fail. To demonstrate this we change the title of one of for one of the parameters to Another title in tests/test_mpl_example.py and re-run the tests, sure enough the test fails. We are told the RMS Value (the Root Mean Square difference value) along with the location of the test files of which there are three images, a copy of the baseline, the result and a difference between the two.\n❱ pytest --mpl  tests/test_mpl_example.py\n========================== test session starts =======================\nplatform linux -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0\nMatplotlib: 3.9.2\nFreetype: 2.6.1\nrootdir: /mnt/work/git/hub/ns-rse/pytest-examples/main\nconfigfile: pyproject.toml\nplugins: regtest-2.1.1, anyio-4.6.0, icdiff-0.9, pylint-0.21.0, pytest_tmp_files-0.0.2, syrupy-4.7.1, mpl-0.17.0, cov-5.0.0, mock-3.14.0, xdist-3.6.1, durations-1.3.1\ncollected 2 items\n\ntests/test_mpl_example.py F.                                                                                                                            [100%]\n\n========================== FAILURES ==================================\n________________ test_scatter[300 points; 6x6; no title] ______\nError: Image files did not match.\n  RMS Value: 8.551853600243634\n  Expected:\n    /tmp/tmpn9ucgc47/tests.test_mpl_example.test_scatter_300 points; 6x6; no title/baseline.png\n  Actual:\n    /tmp/tmpn9ucgc47/tests.test_mpl_example.test_scatter_300 points; 6x6; no title/result.png\n  Difference:\n    /tmp/tmpn9ucgc47/tests.test_mpl_example.test_scatter_300 points; 6x6; no title/result-failed-diff.png\n  Tolerance:\n    2\n\n---------- coverage: platform linux, python 3.12.7-final-0 -----------\nName                            Stmts   Miss  Cover\n---------------------------------------------------\npytestexamples/divide.py           16     16     0%\npytestexamples/mpl_example.py      13      0   100%\npytestexamples/shapes.py            5      5     0%\n---------------------------------------------------\nTOTAL                              34     21    38%\n\n========================== short test summary info ======================\nFAILED tests/test_mpl_example.py::test_scatter[300 points; 6x6; no title] - Failed: Error: Image files did not match.\n========================== 1 failed, 1 passed in 2.45s ==================\nNavigating to and viewing these files is pretty easy when there is only one test that has failed but when more than one test fails they are all in their own directory and navigating and viewing them takes a bit longer.\n\n--mpl-generate-summary\nThis is where the --mpl-generate-summary option comes in really handy as it will generate a report of the differences in either html, json or basic-html. Here I’ll show the use of the html option and what it produces.\nBy default the report is created under /tmp/ but with the --mpl-results-path you can specify a location relative to where the tests are being run from. You do not need to create the directory/path it will be created for you. That said I’m not that bothered about keeping the test image comparisons though so I typically omit the --mpl-generate-path option and instead the output and use the /tmp/ directory, that way my project directory doesn’t get cluttered with files as this is wiped when the computer is rebooted.\n\n\n\n\n\n\nNote\n\n\n\nIf you do use --mpl-results-path to specify a nested directory within your repository you would probably want to exclude it from being included in version control though so add the path to .gitignore.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember we have added the --mpl flag to the general tool.pytest.ini_options in pyproject.toml to ensure pytest-mpl extension is enabled and run.\n\n\nLets run the tests, which we know will fail, with the --mpl-generate-summary=html option enabled (and not using --mpl-results-path) .\npytest --mpl-generate-summary=html tests/test_mpl_example.py\nThe output is virtually identical but there is an additional line at the end…\n❱ pytest --mpl  --mpl-generate-summary=html tests/test_mpl_example.py\n========================== test session starts =======================\nplatform linux -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0\nMatplotlib: 3.9.2\nFreetype: 2.6.1\nrootdir: /mnt/work/git/hub/ns-rse/pytest-examples/main\nconfigfile: pyproject.toml\nplugins: regtest-2.1.1, anyio-4.6.0, icdiff-0.9, pylint-0.21.0, pytest_tmp_files-0.0.2, syrupy-4.7.1, mpl-0.17.0, cov-5.0.0, mock-3.14.0, xdist-3.6.1, durations-1.3.1\ncollected 2 items\n\ntests/test_mpl_example.py F.                                                                                                                            [100%]\n\n========================== FAILURES ==================================\n_________________ test_scatter[300 points; 6x6; no title; green] _____\nError: Image files did not match.\n  RMS Value: 8.551853600243634\n  Expected:\n    /tmp/tmp1bzvguuq/tests.test_mpl_example.test_scatter_300 points; 6x6; no title; green/baseline.png\n  Actual:\n    /tmp/tmp1bzvguuq/tests.test_mpl_example.test_scatter_300 points; 6x6; no title; green/result.png\n  Difference:\n    /tmp/tmp1bzvguuq/tests.test_mpl_example.test_scatter_300 points; 6x6; no title; green/result-failed-diff.png\n  Tolerance:\n    2\n\n---------- coverage: platform linux, python 3.12.7-final-0 -----------\nName                            Stmts   Miss  Cover\n---------------------------------------------------\npytestexamples/divide.py           16     16     0%\npytestexamples/mpl_example.py      13      0   100%\npytestexamples/shapes.py            5      5     0%\n---------------------------------------------------\nTOTAL                              34     21    38%\n\n========================== short test summary info ======================\nFAILED tests/test_mpl_example.py::test_scatter[300 points; 6x6; no title; green] - Failed: Error: Image files did not match.\n========================== 1 failed, 1 passed in 2.50s ==================\nA summary of test results can be found at: /tmp/tmp1bzvguuq/fig_comparison.html\nIf we open the test results in our browser we have a nice summary of the tests that have passed and failed.\n\n\n\nOverview generated by pytest --mpl-generate-summary=html\n\n\nIf we want to look at a specific failed test we can, on the left we see the baseline image, on the right the test image and in-between a plot showing the differences between the two. We also have the “Root Mean Square Error” reported for the test.\n\n\n\nDetailed view of a failed test generated by pytest --mpl-generate-summary=html\n\n\n\n\n\nDetailed view of a pass test generated by pytest --mpl-generate-summary=html"
  },
  {
    "objectID": "posts/pytest-mpl/index.html#conclusion",
    "href": "posts/pytest-mpl/index.html#conclusion",
    "title": "Pytest Matplotlib",
    "section": "Conclusion",
    "text": "Conclusion\nIf your package produces plots its easy to write tests that check they are correctly generated using the pytest-mpl extension and there is a neat convenience option to generate summaries of tests to make viewing the failures and differences in your browser convenient.\n\nKey Points\n\nYou have to generate reference images against which the tests are run.\nYou should add the --mpl option to the tools.pytest.ini_options section of your pyproject.toml under addopts to ensure the extension is used whenever tests are run, be that locally or in Continuous Integration.\nIf you find tests are failing you can easily generate a web-page summarising the passes and failures using the --mpl-generate-summary=html flag when invoking pytest to get an HTML summary that is easy to navigate."
  },
  {
    "objectID": "posts/sphinx-docs/index.html",
    "href": "posts/sphinx-docs/index.html",
    "title": "Sphinx Documentation",
    "section": "",
    "text": "How to generate documentation websites for your Python package using Sphinx, including generating API documentation automatically, build multiple versions across releases and automatically build and host them on GitHub Pages."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#pre-requisites",
    "href": "posts/sphinx-docs/index.html#pre-requisites",
    "title": "Sphinx Documentation",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nThe instructions here assume that you have your Python Packaging well structured, under version control and backed up on GitHub."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#initial-setup",
    "href": "posts/sphinx-docs/index.html#initial-setup",
    "title": "Sphinx Documentation",
    "section": "Initial Setup",
    "text": "Initial Setup\nSphinx comes with the sphinx-quickstart interactive tool which will help setup your repository with a basic conf.py and Makefile. There are a number of command line options but it is also interactive so you can answer questions to configure your setup. I like to keep the source and build directories separate and so use the --sep flag as well as the --makefile flag to generate a Makefile for building documentation on GNU/Linux or OSX (if you use M$-Win the use the --batchfile flag instead).\nI keep documentation under docs/ directory within the root of the package directory.\ncd ~/path/to/package\nmkdir docs\ncd docs\nsphinx-quickstart --makefile"
  },
  {
    "objectID": "posts/sphinx-docs/index.html#conf.py",
    "href": "posts/sphinx-docs/index.html#conf.py",
    "title": "Sphinx Documentation",
    "section": "conf.py",
    "text": "conf.py\nConfiguration is via a conf.py the automatically generated conf.py produced by sphinx-quickstart. It is well commented and instructive on how to use it to configure Sphinx and contains details on adding/modifying various sections of the this file.\nKey sections are the list of extensions that your documentation uses."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#index.rst",
    "href": "posts/sphinx-docs/index.html#index.rst",
    "title": "Sphinx Documentation",
    "section": "index.rst",
    "text": "index.rst\nThe front-page of your website, typically index.html for static sites, is derived from index.rst. You can write welcome details about your project and link to other pages you have written. Typically I write all but the front matter in Markdown.\nWelcome to my packages documentation\n====================================\n\nThis is my package, there are many packages like it but this one is mine.\n\nIncluding Markdown\nI already know Markdown fairly well and would rather use that to write documents (as I do with thisblog). Fortunately Sphinx can incorporate documentation written in Markdown using the myst_parser package. Simply include it in the extensions.\n\nextensions = [\n    \"myst_parser\",\n]\n\nBy default it works with extensions of .md but if there are other flavours you wish to include (e.g. .Rmd for RMarkdown or .qmd for Quarto) you add them to the source_suffix in docs/conf.py\n\nsource_suffix = {\".rst\": \"restructuredtext\", \".md\": \"markdown\"}\n\nIn your index.rst you can then list the Markdown filenames, without extensions. For example if you have an installation.md and configuration.md place them in the same directory as index.rst (the root docs/) directory and have in your index.rst have…\nWelcome to my packages documentation\n====================================\n\nThis is my package, there are many packages like it but this one is mine.\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Getting Started\n   introduction\n   configuration\n\nMarkdown Tables\nIf you have tables in Markdown (and its likely that you will) then you will need the sphinx-markdown-tables package which ensures they are rendered correctly.\n\n\nMermaid Diagrams\nFurther Sphinx has support for Mermaid diagrams that have been written in Markdown documents via the sphinxcontrib-mermaid package. This means that you can include all sorts of neat diagrams such as Git Graph.\n\n\n\n\n\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true,'showCommitLabel': true, 'rotateCommitLabel': true}} }%%\ngitGraph\n    commit\n    commit\n    branch bug1\n    checkout main\n    commit\n    checkout bug1\n    commit\n    commit\n    checkout main\n    branch bug2\n    checkout bug2\n    commit\n    commit\n    checkout bug1\n    commit\n    checkout main\n    merge bug1 tag: \"v0.1.1\"\n    checkout bug2\n    commit\n    commit\n    checkout main\n    merge bug2 tag: \"v0.1.2\"\n    commit"
  },
  {
    "objectID": "posts/sphinx-docs/index.html#including-api-documentation",
    "href": "posts/sphinx-docs/index.html#including-api-documentation",
    "title": "Sphinx Documentation",
    "section": "Including API Documentation",
    "text": "Including API Documentation\nAs you write your package it is good practice include docstrings for each module/class/method/function that you write. For Python there are several different styles for writing these, my personal preference is for numpydoc style but regardless of your preference you should write them. They are invaluable to users (including your future self) to understand how the code works and as many modern Integrated Development Environments (IDEs) supporting functionality to show the documentation for functions as you type they are an invaluable reference. If you’re an Emacs user then you can leverage the numpydoc package to automatically insert NumPy docstrings in Python function definitions based on the function definition, it automatically detects names, type hints, exceptions and return types to generate the docstring (yet another reason to use Emacs!).\nWhilst it is useful to have this API available in an IDE as you work it is also useful to include the reference on a packages website and this is relatively straight-forward with Sphinx which provides several tools and extensions.\n\nsphinx-apidoc\nThe first is the the sphinx-apidoc command to generate documentation from the embedded docstrings. This is a command line tool that could be added to the Makefile.\n\n\nsphinx-autoapi\nHowever, rather than learning the intricacies of using this command the package Sphinx extensions sphinx-autoapi can be leveraged to automatically build the API documentation for you. This is particularly useful when you come to build multiple versions of your documentation as it means you do not have to include the .rst files that sphinx-apidoc generates in your repository they are generated on the fly when Sphinx builds each version of the documentation.\nConfiguration is via docs/conf.py and the package needs referencing in the extensions section then configuring at a bare minimum which directories to generate documentation from. i\n\nextensions = [\n    \"autoapi.extension\",\n]\n\n# -- autoapi configuration ---------------------------------------------------\nautotype_api = \"python\"\nautoapi_dirs = [\"../mypackage\"]\n\nThere are a lot of subtle configuration options and I would recommend reading the documentation and working through the Tutorials and How To Guides.\nThis has the added advantage that it works with ReadTheDocs.\n\n\nSphinx Autosummary\nIn addition the sphinx_ext_autosummary automates summarising the API docstrings.\nAdd the package as a dependency to the extensions…\n\nextensions = [\n    \"sphinx.ext.autosummary\"\n]\n\nUnder the index.rst you should include a section header for the api that references an api.rst page for inclusion.\n.. toctree::\n   :maxdepth: 2\n   :caption: API\n\n   api\nAnd then create the api.rst page which need only have the following. By including :recursive: the sub-modules will be included automatically.\nAPI\n===\n\n.. autosummary::\n   :recursive:\n   :toctree: generated\n\n   mypackage"
  },
  {
    "objectID": "posts/sphinx-docs/index.html#multiple-versions",
    "href": "posts/sphinx-docs/index.html#multiple-versions",
    "title": "Sphinx Documentation",
    "section": "Multiple Versions",
    "text": "Multiple Versions\nOver time code and in turn documentation changes, not just the API but the documents written to demonstrate installation and usage of software. Not everyone always uses the latest version of your software and so it can be useful to provision documentation for each version that is available. Fortunately the Sphinx extension sphinx-multiversion makes this relatively painless.\nYou need to include it in the list of extensions of docs/conf.py\n\nextensions = [\n    \"sphinx_multiversion\",\n]\n\n\nConfiguring Versions\n\nSidebar\nFor versions to not just be built but available you need to include a section on your site that allows selecting which version of the documentation to view. Sidebars are included via HTML templates and you need to configure the path to this directory and the name of the HTML file within it. The following options in the conf.py configure the _templates directory and within it the versioning.html file.\n\ntemplates_path = [\n    \"_templates\",\n]\n\nhtml_sidebars =  {\"**\":   \"versioning.html\",}\n\nThe versioning.html file can take a number of formats, refer to the documentation for all options, but the following is an example.\n{% if versions %}\n&lt;h3&gt;{{ _('Versions') }}&lt;/h3&gt;\n&lt;ul&gt;\n  {%- for item in versions %}\n  &lt;li&gt;&lt;a href=\"{{ item.url }}\"&gt;{{ item.name }}&lt;/a&gt;&lt;/li&gt;\n  {%- endfor %}\n&lt;/ul&gt;\n{% endif %}\nEnsure this file is under Git version control, it is needed to build your pages on GitHub.\n\n\nTags/Branches\nIf no options are set then sphinx-multiversion will build documentation for all branhces, which is probably undesirable. Typically you want to restrict this to the released versions which are identified by git tags and perhaps your main/master branch. If you prefix your tags with v and you want to build the documentation for the HEAD of your main (or master) branch then you should set options as shown below for sphinx-multiversion. I like to be able to test documentation builds and so I have a section that allows me to include a given branch.\n\nsmv_tag_whitelist = r\"^v\\d+.*$\"  # Tags beginning with v#\nsmv_branch_whitelist = r\"^main$\"  # main branch\n# If testing changes locally comment out the above and the smv_branch_whitelist below instead. Replace the branch name\n# you are working on (\"ns-rse/testing-branch\" in the example below) with the branch you are working on and run...\n#\n# cd docs\n# sphinx-multiversion . _build/html\n#\n# smv_branch_whitelist = r\"^(main|ns-rse/testing-branch)$\"  # main branch\nsmv_released_pattern = r\"^tags/.*$\"  # Tags only\n# smv_released_pattern = r\"^(/.*)|(main).*$\"  # Tags and HEAD of main\nsmv_outputdir_format = \"{ref.name}\"\n\nIf you are testing locally be sure to revert the commented sections so that the branch is not built on GitHub Pages."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#themes",
    "href": "posts/sphinx-docs/index.html#themes",
    "title": "Sphinx Documentation",
    "section": "Themes",
    "text": "Themes\nThere are a number of different themes available for including in your package. Which is used is defined by the html_theme variable in docs/conf.py. I like the pydata-sphinx-theme that is used by Pandas/Matplotlib.\n\nhtml_theme = \"pydata_sphinx_theme\""
  },
  {
    "objectID": "posts/sphinx-docs/index.html#package-dependencies",
    "href": "posts/sphinx-docs/index.html#package-dependencies",
    "title": "Sphinx Documentation",
    "section": "Package Dependencies",
    "text": "Package Dependencies\nSince the documentation is part of your package it is important to include all of the dependencies that are required for building the documentation dependencies of your package so they can be easily installed and are available to Sphinx (since Sphinx will try loading anything listed in your docs/conf.py). These days you should really be using pyproject.toml to configure and manage your package, if you are unfamiliar with the packaging process see my post on Python Packaging.\n\n[project.optional-dependencies]\n\ndocs = [\n  \"Sphinx\",\n  \"myst_parser\",\n  \"numpydoc\",\n  \"pydata_sphinx_theme\",\n  \"sphinx-autoapi\",\n  \"sphinx-autodoc-typehints\",\n  \"sphinx-multiversion\",\n  \"sphinx_markdown_tables\",\n  \"sphinx_rtd_theme\",\n  \"sphinxcontrib-mermaid\",\n]\n\nEnsure all of these dependencies are installed in your Virtual Environment.\ncd ~/path/to/package\npip install .[docs]"
  },
  {
    "objectID": "posts/sphinx-docs/index.html#building-documentation",
    "href": "posts/sphinx-docs/index.html#building-documentation",
    "title": "Sphinx Documentation",
    "section": "Building Documentation",
    "text": "Building Documentation\nYou are now ready to build your documentation locally.\ncd ~/path/to/package/docs\nmkdir -p _build/html\nsphinx-multiversion . _build/html\nOutput should reside under the ~/path/to/package/docs/_build/html/ directory and there should be a directory for every tag as well as main (or master).\n\nDeploying on GitHub Pages\nThe final stage is to leverage GitHub Pages to host your documentation. This can be achieved using a GitHub Action. These are a way of running certain tasks automatically on GitHub in response to certain actions. You can configure your actions to use those defined by others. I found the action-sphinx-docs-to-gh-pages action for generating Sphinx documentation but it didn’t support generating API documentation nor multiple versions of documentation so I have forked it and added this functionality (I intend to work with the authors and push the changes upstream).\nTo use this action you need to create a file in ~/path/to/package/.github/workflows/ called sphinx_docs_to_gh_pages.yaml and copy and paste the following YAML.\nname: Sphinx docs to gh-pages\n\non:\n  push:\n  workflow_dispatch:\n\njobs:\n  sphinx_docs_to_gh-pages:\n    runs-on: ubuntu-latest\n    name: Sphinx docs to gh-pages\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n      - name: Setup Python\n        uses: actions/setup-python@v4.3.0\n        with:\n          python-version: 3.9\n      - name: Installing the Documentation requirements\n        run: |\n          pip3 install .[docs]\n      - name: Running Sphinx to gh-pages Action\n        uses: ns-rse/action-sphinx-docs-to-gh-pages@main\n        with:\n          # When testing set this branch to your branch, when working switch to main. It WILL fail if not\n          # defined as it defaults to 'main'.\n          branch: main\n          dir_docs: docs\n          sphinxapiexclude: '../*setup* ../*tests* ../*.ipynb ../demo.py ../make_baseline.py ../jupyter_notebook_config.py ../demo_ftrs.py'\n          sphinxapiopts: '--separate -o . ../'\n          sphinxopts: ''\n          multiversion: true\n          multiversionopts: ''\n\nSave, add and commit to your Git repository and push the changes to GitHub."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#links",
    "href": "posts/sphinx-docs/index.html#links",
    "title": "Sphinx Documentation",
    "section": "Links",
    "text": "Links\n\nSphinx\n\n\nSphinx Extensions\n\nmyst_parser\nsphinx-autoapi\nsphinx-markdown-tables\nsphinx-multiversion\nsphinxcontrib-mermaid\nsphinx-ext-autosummary\n\n\n\nGitHub\n\nGitHub Pages\nGitHub Action\nSphinx docs to GitHub Pages · Actions · GitHub Marketplace ( my fork with added sphinx-multiversion support)."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html",
    "href": "posts/gitlab-ci-pypi/index.html",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "",
    "text": "I’ve written previously on Python Packaging and in that article included details of how to automate publishing to PyPI from GitHub. This article details how to automatically publish your package to PyPI from GitLab."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#repository-configuration",
    "href": "posts/gitlab-ci-pypi/index.html#repository-configuration",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "Repository Configuration",
    "text": "Repository Configuration\n\nCI Variables\nThe environment variables $TWINE_USERNAME (__token__) and $TWINE_PASSWORD which will be the token you generate for publishing on PyPI or Test PyPI. These are saved under the repository _Settings &gt; CI/CD &gt; Varialbes_ section and how to create and save these is described below.\n\n\nProtecting Tags\nThis really stumped me I could build and push automatically from the master branch but could not use the - if  $CI_COMMIT_TAG condition to publish commits that were tagged. I wrote a post on the GitLab Forums asking how to do this and posted it to Mastodon asking if anyone had any ideas. I got two replies (one from @manu_faktur@mastodon.social and one from @diazona@techhub.social) both asking if I’d protected the tags on my repository.\nI had no idea that you could protect tags on GitLab (or GitHub for that matter) so looked up the documentation on Protected tags and sure enough this was possible. Go to settings &gt; Repository &gt; Protected tags and set a wildcard to protect my tags, e.g. v* and the pypi CI job defined below will work as expected, building and uploading to PyPI on tagged commits."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#ci-configuration",
    "href": "posts/gitlab-ci-pypi/index.html#ci-configuration",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "CI Configuration",
    "text": "CI Configuration\n\nCI\nGitLabs CI/CD is configured via a YAML file .gitlab-ci.yaml in the root of your project folder, a useful reference for writing these files is the .gitlab-ci.yml reference.\nAn example file from the tcx2gpx package is shown below (see here).\nThis defines the following…\n\nimage - the use of a Docker Python 3.11 image for running the pipeline.\nvariables - Configures pre-commit to run and automatically fix issues found on pull requests.\nstages - the subsequent stages to run (NB the debug stage which prints the environment variables is commented out).\npylint - runs linting on Python 3.10 and 3.11.\npytest - Runs tests on Python 3.10 and 3.11.\npages - Builds the documentation pages.\npypi - Builds and uploads the package to PyPI if the commit has a tag associated.\n\nimage: python:3.11\n\nvariables: # since we're not using merge request pipelines in this example, # we will configure the pre-commit job to\n  run on branch pipelines only.  # If you ARE using merge request pipelines, you can omit this section\n  PRE_COMMIT_AUTO_FIX: '1' PRE_COMMIT_DEDUPLICATE_MR_AND_BRANCH: 'false' PRE_COMMIT_AUTO_FIX_BRANCH_ONLY: 'false'\n\n\nbefore_script:\n    - python --version\n    - pip install .\n\n# pre-commit autofix (https://gitlab.com/yesolutions/gitlab-ci-templates /\n#                     https://stackoverflow.com/collectives/gitlab/articles/71270196/)\ninclude: remote: https://gitlab.com/yesolutions/gitlab-ci-templates/raw/main/templates/pre-commit-autofix.yaml\n\nstages: # - debug - pylint - pytest - pages - pypi\n\n# print-all-env-vars-job:\n#     stage: debug\n#     script:\n#         - echo \"GitLab CI/CD | Print all environment variables\"\n#         - env\n\n.pylint: script:\n        - pip install pylint pytest\n        - pylint --rcfile .pylintrc tcx2gpx/\n        - pylint --rcfile .pylintrc tests/\n\npylint-3-10: extends: .pylint stage: pylint image: python:3.10 allow_failure: true\n\npylint-3-11: extends: .pylint stage: pylint image: python:3.11 allow_failure: true\n\n.pytest: script:\n        - pip install pytest pytest-cov\n        - python -m \"pytest\"\n\npytest-3-10: extends: .pytest stage: pytest image: python:3.10 allow_failure: true\n\npytest-3-11: extends: .pytest stage: pytest image: python:3.11 coverage: /(?i)total.*?\n    (100(?:\\.0+)?\\%|[1-9]?\\d(?:\\.\\d+)?\\%)$/\n\npages: stage: pages rules:\n        - if: $CI_COMMIT_BRANCH == \"master\" script:\n        - pip install .[docs]\n        - cd docs\n        - git fetch --tags\n        - git tag -l\n        - make html\n        - mkdir ../public\n        - mv _build/html/* ../public/ artifacts: paths:\n            - public\n\n\npypi: stage: pypi rules:\n        - if: $CI_COMMIT_TAG script:\n        - pip install .[pypi]\n        - pip install build\n        - python -m build\n        - twine upload --non-interactive --repository pypi dist/*\nThe pypi stage is named and a rule is defined that says to only run this stage if the value of the environment variable $CI_COMMIT_TAG is True. This only happens when a commit has a (protected 😉) tag.\nThe script section then installs the package along with the project.optional-dependencies defined in the pypi section of the pyproject.toml.\nThe package is then built using build and twine is used to push the to push the built package to PyPI.\n\n\nPyPI Tokens\nYou should first test building and deploying to the Test PyPI and when this is working simply switch to using the main PyPI. To do so you will need to create an account on both1. Once you have set yourself up with an account you can generate an API token to authenticate with PyPI. After verifying your email got to Account Settings and select Add API token. These are generated once so copy and paste it into the .pypirc of your project (add this file to your .gitignore so it doesn’t accidentally get added). Remember to do this twice, once for PyPI and once for Test PyPI and once for PyPI for reference.\n[testpypi]\nusername = __token__\npassword = pypi-&lt;token_value&gt;\n\n[pypi]\nusername = __token__\npassword = pypi-&lt;token_value&gt;\nIn GitLab go to your repositories Settings &gt; CI/CD &gt; Variables and add two new variables TWINE_USERNAME with the value __token__ and TWINE_PASSWORD with the token for your account on Test PyPI (remember it should include the prefix pypi- as shown in the above example .pypirc). You have options on how these variables are used and should ensure that all three check boxes are selected, this enables…\n\nProtect variable Export variable to pipelines running on protected branches and tags only.\nMask variable Mask this variable in job logs if it meets regular expression requirements.\nExpand variable reference $ will be treated as the start of a reference to another variable."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#testing",
    "href": "posts/gitlab-ci-pypi/index.html#testing",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "Testing",
    "text": "Testing\nNow that you are setup you can test your configuration. To do so you need to first use the API key from the Test PyPI server that you created as the value for $TWINE_PASSWORD (see above) and set the repository twine --repository option to testpypi. Your pypi stage should look like the following…\npypi:\n    stage: pypi\n    rules:\n        - if: $CI_COMMIT_TAG\n    script:\n        - pip install .[pypi]\n        - pip install build\n        - python -m build\n        - twine upload --non-interactive --repository testpypi dist/*\nOnce this is set create a tag for the current commit using the Code &gt; Tags settings from the left menu of your repository and then the New tag button on the top right. The tag you create should match the wild card pattern you have set for protecting tags and it should comply to the Public version identifiers specified in PEP440. On creation it triggers the Pipeline, you can check progress and status by navigating to CI/CD &gt; Pipelines and then viewing it. The pypi job should complete and you should be able to navigate to your package on Test PyPI. You can find it under your account settings.\nIf you find there is a problem you will have to correct it and either delete the tag you created and try again or increment the version. PyPI, and in turn Test PyPI which is a mirror with the same functionality, does not permit uploading packages with a version number that already exists."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#publishing-to-pypi",
    "href": "posts/gitlab-ci-pypi/index.html#publishing-to-pypi",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "Publishing to PyPI",
    "text": "Publishing to PyPI\nOnce you have successfully published to the Test PyPI you are ready to publish to PyPI. There three things you need to do.\n\nDelete the existing tag, if you want to apply the same tag to publish to PyPI you can do so.\nModify the repository option to point to PyPI --repository pypi (or remove it, the default is PyPI).\nChange the key stored in the $TWINE_PASSWORD to that which you generated for PyPI instead of the one used for testing with Test PyPI.\n\nOnce you have done so you can create a new tag and the upload will be made to PyPI.\n\nReleases\nAn alternative way to apply tags to commits is to make a Releases. In creating a release you apply a tag to the current commit. In addition GitLab will build and compress snapshot of the files and you can add Release Notes detailing what has changed. GitLab will automatically build release artifacts of your repository and make them available for download directly from GitLab."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#links",
    "href": "posts/gitlab-ci-pypi/index.html#links",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "Links",
    "text": "Links\n\nPython Packaging\n\nPyPA : Building and Distributing Packages with Setuptools\nPyPA : Specifications\nPackaging Python Projects\nPython package structure information — pyOpenSci Python Packaging Guide\n\n\n\nGitLab Documentation\n\nUse CI/CD to build your application | GitLab\nThe .gitlab-ci.yml file | GitLab +\nProtected tags"
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#footnotes",
    "href": "posts/gitlab-ci-pypi/index.html#footnotes",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPyPI now enforces Two Factor Authentication (2FA) for new accounts, see 2FA Enforcement for New User Registrations↩︎"
  },
  {
    "objectID": "posts/quarto-cname/index.html",
    "href": "posts/quarto-cname/index.html",
    "title": "Quarto Custom domain on GitHub Pages",
    "section": "",
    "text": "This blog is published using the excellent Quarto framework to GitHub Pages which offers free website hosting. However, astute readers familiar with this sort of setup will have noticed that the domain is no the usual [github-user].github.io and instead it resolves to blog.nshephard.dev. This post explains how to achieve that as it took me a little bit of work to sort out properly."
  },
  {
    "objectID": "posts/quarto-cname/index.html#why",
    "href": "posts/quarto-cname/index.html#why",
    "title": "Quarto Custom domain on GitHub Pages",
    "section": "Why?",
    "text": "Why?\nI found that despite setting a custom domain in the GitHub pages Settings (Settings &gt; Pages &gt; Custom domain) each time I published the site (e.g. on a new blog post) the CNAME file disappeared from the gh-pages branch and the domain didn’t resolve. I searched but couldn’t find a solution to this so raised an issue to include it in the quarto-actions.\nIn the meantime I added a custom step to the publish.yaml to add the CNAME file to the gh-pages branch after Render and Publish which worked, but I was subsequently pointed to a cleaner existing way of achieving the same result."
  },
  {
    "objectID": "posts/quarto-cname/index.html#what-isnt-covered",
    "href": "posts/quarto-cname/index.html#what-isnt-covered",
    "title": "Quarto Custom domain on GitHub Pages",
    "section": "What isn’t covered",
    "text": "What isn’t covered\nThis post doesn’t cover…\n\nHow to setup a blog/website using Quarto.\nHow to write Quarto Markdown.\nHow to publish a blog or website using Quarto, they have excellent documentation\nSetting a CNAME with your DNS registrar.\n\nOn this last point, how you set your CNAME to redirect your custom domain to that of the GitHub Pages is dependent on the domain registrar service you use. I use OVH."
  },
  {
    "objectID": "posts/quarto-cname/index.html#what-is-covered",
    "href": "posts/quarto-cname/index.html#what-is-covered",
    "title": "Quarto Custom domain on GitHub Pages",
    "section": "What is covered",
    "text": "What is covered\n\nHow to add a CNAME file to the gh-pages branch that is produced by the quarto-dev/quarto-actions/publish GitHub Action so that the pages resolve to a custom domain."
  },
  {
    "objectID": "posts/quarto-cname/index.html#the-problem",
    "href": "posts/quarto-cname/index.html#the-problem",
    "title": "Quarto Custom domain on GitHub Pages",
    "section": "The Problem",
    "text": "The Problem\nIf you use plain GitHub Pages then you can set a custom CNAME by going to Settings &gt; Pages &gt; Custom Domain and entering the address there. However, as documented (see item 4) this doesn’t work if you use a custom GitHub Action to publish your website, which is the case when you use the quarto-dev/quarto-actions/publish GitHub Action to publish using Quarto.\nWhy? Because each time the action runs it regenerates the content of the gh-pages branch on the runner and pushes it to your repository and doesn’t include a custom CNAME file."
  },
  {
    "objectID": "posts/quarto-cname/index.html#solution-1",
    "href": "posts/quarto-cname/index.html#solution-1",
    "title": "Quarto Custom domain on GitHub Pages",
    "section": "Solution 1",
    "text": "Solution 1\nThe solution I initially hit upon was to add an extra step to my .github/workflows/publish.yaml that…\n\nChecks out the gh-pages branch.\nCreates the CNAME file with the domain I use.\nAdds this to the gh-pages branch.\nPushes back up-stream.\n\nYou should already have added a publish action to your repository which runs the quarto-dev/quarto-actions/publish GitHub Action. This runs quarto publish $TARGET where $TARGET is set by the chosen output configured in the target argument. In this example gh-pages\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\nAfter this you should add the following, substituting blog.nshephard.dev for your own domain.\n      - name: Add CNAME file to gh-pages branch\n        run: |\n          rm renv/activate.R\n          git switch gh-pages\n          git pull\n          echo 'blog.nshephard.dev &gt; CNAME\n          git add CNAME\n          git commit -m \"Adding CNAME\"\n          git push\nThis…\n\nRemoves the lingering renv/activate.R which prevents switching branches.\nSwitches branches to gh-pages.\ngit pull to get the just published version of the branch.\nCreates the CNAME file with the domain you specify.\nAdds (stages) the CNAME file to the gh-pages branch.\nCommits the change.\nPushes the commit to origin (i.e. the gh-pages branch on GitHub)\n\nThis worked as the CNAME file is added each time the workflow runs but, unsurprisingly, there is a simpler solution."
  },
  {
    "objectID": "posts/quarto-cname/index.html#solution-2---the-correct-way-of-doing-this",
    "href": "posts/quarto-cname/index.html#solution-2---the-correct-way-of-doing-this",
    "title": "Quarto Custom domain on GitHub Pages",
    "section": "Solution 2 - The correct way of doing this",
    "text": "Solution 2 - The correct way of doing this\nTurns out the answer provided by @cscheid is, unsurprisingly, much simpler.\nYou need to add CNAME in the _quarto.yaml so that it is included in the project.\nproject:\n  type: website\n  resources:\n    ...\n    - \"CNAME\"\nYou also have to create and add the CNAME file with your domain to the repository.\necho 'blog.nshephard.dev` &gt; CNAME\ngit add CNAME _quarto.yaml\ngit commit -m \"Adding CNAME to repository\"\ngit push\nThen, because it’s included in the resources, the file will carry through/persist when the publishing action runs to create the gh-pages branch with each new update and run on GitHub Actions :magic:."
  },
  {
    "objectID": "posts/quarto-cname/index.html#related",
    "href": "posts/quarto-cname/index.html#related",
    "title": "Quarto Custom domain on GitHub Pages",
    "section": "Related",
    "text": "Related\nThis feature was introduced in Quarto 1.2 and others have encountered problems with the file being deleted on first running quarto publish gh-pages (if you’re having problems perhaps worth keeping an eye on that thread).\nThere is a draft Pull Request to add it to the documentation its not been merged which is perhaps why I couldn’t find how to do this in the documentation (see also discussion in this issue)."
  },
  {
    "objectID": "posts/repository-review/index.html",
    "href": "posts/repository-review/index.html",
    "title": "Repository Review with Scientific Python",
    "section": "",
    "text": "I’ve written before about Python Packaging and pre-commit which I’m a big fan of. Today I discovered a really useful tool for checking your packaging configuration and pre-commit configuration from the Scientific Python Development Guide.\nThis development guide is really well written and has Tutorials and Topical Guides that cover package development, style guides, type checking and tests amongst other things."
  },
  {
    "objectID": "posts/repository-review/index.html#repo-review",
    "href": "posts/repository-review/index.html#repo-review",
    "title": "Repository Review with Scientific Python",
    "section": "Repo Review",
    "text": "Repo Review\nOf particular use is the Repo-Review - Scientific Python Development Guide which will automatically check a GitHub hosted repository against the Scientific Python development guidelines and make recommendations where improvements can be made to the package configuration and pre-commit configuration.\nHaving tested it against AFM-SPM/TopoStats main branch there are a lot of useful and simple recommendations made."
  },
  {
    "objectID": "posts/repository-review/index.html#more-pre-commit-hooks",
    "href": "posts/repository-review/index.html#more-pre-commit-hooks",
    "title": "Repository Review with Scientific Python",
    "section": "More Pre-Commit hooks",
    "text": "More Pre-Commit hooks\nMore pre-commit hooks is only a good thing in my view, they don’t take that long to run against most reasonably sized repositories, particularly if ruff is used in favour of more traditional tools such as flake8 and isort.\nThere are a few pre-commit hooks that I wasn’t aware of and will be adding to my projects such as..,\n\nCheck-manifest that verifies you have working SDist (source) packages.\nCodespell for checking the spelling used in code.\nvalidate-pyproject for validating pyproject.toml."
  },
  {
    "objectID": "posts/repository-review/index.html#conclusion",
    "href": "posts/repository-review/index.html#conclusion",
    "title": "Repository Review with Scientific Python",
    "section": "Conclusion",
    "text": "Conclusion\nIf you are involved in Python package development of any sort give your repository a review and see what recommendations are suggested. You may not agree with all of the recommendations but the vast majority of tools are highly configurable so you can disable the checks you don’t want applied. It won’t do any harm.\nNow I just need to find time to apply MyPy."
  }
]