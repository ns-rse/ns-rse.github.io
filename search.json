[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ns-rse",
    "section": "",
    "text": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nnshephard\n\n\n\n\n\n\n  \n\n\n\n\nRunning in 2022\n\n\n\n\n\n\n\nquarto\n\n\nrunning\n\n\nemacs\n\n\nliterate programming\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nnshephard\n\n\n\n\n\n\n  \n\n\n\n\nWho‚Äôs to Blame\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\nblame\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2022\n\n\nnshephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Customising and Updating\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\npre-commit\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nnshephard\n\n\n\n\n\n\n  \n\n\n\n\nLinux Command Line Alternatives\n\n\n\n\n\n\n\ncode\n\n\nlinux\n\n\nbash\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nGit : Custom SSH credentials for git repositories\n\n\n\n\n\n\n\nssh\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\nkeychain\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nnshephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Protecting your future self\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nnshephard\n\n\n\n\n\n\n  \n\n\n\n\nLinting - What is all the fluff about?\n\n\n\n\n\n\n\ncode\n\n\nlinting\n\n\npython\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html",
    "href": "posts/pre-commit-ci/index.html",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "",
    "text": "NB If you‚Äôve not read it already I would recommend reading my previous post on using pre-commit as the contents described herein assume that you are already using pre-commit in your development.\nHaving pre-commit setup locally to run before making commits is great. Typically code lives in a ‚Äúforge‚Äù such as GitHub or GitLab and as pre-commit is run on each commit you shouldn‚Äôt have any problems when you come to git push your code to the remote origin repository (i.e.¬†the repository hosted on GitHub/GitLab) as all pre-commit checks will have to have passed before this will take place.\nBut what if for some reason you disabled pre-commit just to make some changes rather than addressing the failed linting or test? Or if you work on an open-source project and someone else contributes how can you ensure that their contributed code meets the code-style chosen by the project and that all tests pass in light of the changes that are being introduced?"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#continuous-integration-continuous-delivery-cicd",
    "href": "posts/pre-commit-ci/index.html#continuous-integration-continuous-delivery-cicd",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "Continuous Integration / Continuous Delivery (CI/CD)",
    "text": "Continuous Integration / Continuous Delivery (CI/CD)\nThe solution to this is Continuous Integration/Continuous Delivery (CI/CD) which runs various hooks on GitHub/GitLab etc. in response to specific tasks/actions that occur on the repository. The exact name or system used depends on the forge, on GitHub these are GitHub Actions (see also Actions Marketplace) whilst on GitLab uses Pipelines. There are even standalone systems which integrate with both such as the popular Jenkins.\nBy employing pre-commit as part of your CI/CD pipeline you ensure code meets the standards (linting, tests etc.) you wish contributions to meet before it is merged into your main/master branch`\nThese work by running processes under certain conditions, for example on a push to the main branch or a tag that begins with v, and they might run processes such as running the test suite for your project to ensure all tests pass, build web-pages or build the package for deployment to a repository (e.g.¬†PyPI). They are really useful and flexible systems and can be leveraged to run pre-commit on your code when Pull Requests (PR) are made to ensure the PR passes the various hooks. Ultimately a PR results in a commit to master/main and so its logically consistent that Pull Requests should pass pre-commit prior to being merged.\nUnder any system you could write your own hook to run pre-commit but there is an even easier and more efficient solution if you use GitHub in the form of pre-commit.ci."
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#github-and-pre-commit.ci",
    "href": "posts/pre-commit-ci/index.html#github-and-pre-commit.ci",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "GitHub and pre-commit.ci",
    "text": "GitHub and pre-commit.ci\nCurrently pre-commit.ci only supports GitHub although support of other systems is in the pipeline. pre-commit.ci doesn‚Äôt need any configuration beyond your already existing .pre-commit-config.yaml (see Pre-commit : Protecting Your Future Self). Where a pre-commit hook corrects formatting issues as is the case with some of the defaults such as trailing-whitespace or check-yaml, or if you are using Python linters such as black or ruff which fix errors, pre-commit.ci can commit these changes and push them back to the Pull Request automatically. In a similar vein it will also routinely update the rev used in your .pre-commit-config.yaml, commit the change and push it back to your repository.\nIt is also really fast because pre-commit.ci keeps the virtual environments that are used in tests cached whereas if you wrote your own action to run this the GitHub runner that is spun up to run GitHub Actions would have to download all of these each time the action is run is they are not persistent.\nUse of pre-commit.ci is free for open-source repositories and there are paid options for private or organisation repositories.\n\nBenefits of pre-commit.ci\n\nSupports GitHub but more to come in the future.\nZero configuration, just need .pre-commit-config.yaml.\nCorrects & commits formatting issues automatically without need for developer to reformat.\nAutomatically updates .pre-commit-config.yaml for you (e.g.¬†new rev).\nFaster than your own GitHub Action.\nFree for open source repositories (paid for version for private/organisation repositories).\n\n\n\nConfiguration (.pre-commit-config.yaml)\nWhilst not required it is possible to configure the behaviour of pre-commit.ci by adding a ci: section to your .pre-commit-config.yaml. The fields are fairly self-explanatory as the example below shows. Its possible to toggle whether to autofix_prs and to set the autofix_commit_msg. The autoupdate_schedule can be set to weekly, monthly or quarterly along with a custom autoupdate_commit_msg. Finally you can optionally disable some hooks from being run only in pre-commit.ci.\nci:\n  autofix_prs: true\n  autofix_commit_msg: '[pre-commit.ci] Fixing issues with pre-commit'\n  autoupdate_schedule: weekly\n  autoupdate_commit_msg: '[pre-commit.ci] pre-commit automatically updated revs.'\n  skip: [pylint] # Optionally list ids of hooks to skip on CI\n\n\nSetup\nSetup is relatively straight-forward, head to https://pre-commit.ci and sign-in with your GitHub account and grant pre-commit.ci access to your account.\n\nOnce you have granted access you can choose which repositories pre-commit.ci has access to. It is possible to grant access to all repositories but I would recommend doing so on a per-repository basis so you know and are in control of what is happening across your repositories. If you have administration rights to organisation repositories these should be listed in the ‚ÄúSelect repositories‚Äù pull-down menu.\n\n\n\npre-commit.ci jobs\nWhen logged into pre-commit.ci using your GitHub account you are presented with a page similar to the following which lists the accounts and any organisations that you have authorised pre-commit.ci to access.\n\nYou can follow the links through to view the history of jobs run be pre-commit.ci and whether they pass or fail. The page shows the current status and provides both Markdown and reStructured Text code for adding badges to your source documents (e.g.¬†the Markdown badge can be added to your repositories top-level README.md and the badge will be displayed on GitHub)\n\nYou can click through and see the results of a given run and when they pass they look similar to the output you would have seen when making commits locally.\n\nBut sometimes things will fail as shown below where the trailing-whitespace hook failed and the file was modified. But since pre-commit.ci corrects and pushes such changes automatically you can see at the bottom that these changes were pushed to the Pull Request from which the originated."
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#gitlab",
    "href": "posts/pre-commit-ci/index.html#gitlab",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "GitLab",
    "text": "GitLab\nAs pre-commit.ci doesn‚Äôt (yet) support GitLab integrating pre-commit into your GitLab Pipeline is a little more involved. What follows is based on the excellent post on StackOverflow describing how to achieve this integration.\nYou should already have a valid .pre-commit-config.yaml in place (if not work through Pre-commit : Protecting your future self (blog-post)). To enable pre-commit on your GitLab Pipeline you need to to have a pipeline in place. This is a file in the root of your repository called .gitlab-ci.yml. You need to add the following to this file‚Ä¶\nvariables:\n  # since we're not using merge request pipelines in this example,\n  # we will configure the pre-commit job to run on branch pipelines only.\n  # If you ARE using merge request pipelines, you can omit this section\n  PRE_COMMIT_DEDUPLICATE_MR_AND_BRANCH: false\n  PRE_COMMIT_AUTO_FIX_BRANCH_ONLY: true\n\ninclude:\n  - remote: 'https://gitlab.com/yesolutions/gitlab-ci-templates/raw/main/templates/pre-commit-autofix.yaml'\nThis uses the pre-commit-autofix.yaml from yesolutions to run pre-commit and as the configuration shows automatically apply fixes pre-commit makes to your code. There are more options available for configuring this pipeline and they are documented here.\nBecause you are allowing a third-party pipeline to access your repository when pushing the changes pre-commit makes back to your repository for this to work you must create a project access token. Under the repositories Settings > Access Tokens you can create a new token with an expiry date. You must then create a CI/CD variable called PRE_COMMIT_ACCESS_TOKEN with this token as a value.\nOnce you have done this your CI/CD pipeline should show at the very start the .pre stage‚Ä¶\n\n‚Ä¶and you can click through on this to see the details of the pipeline. Note that it takes a while to run as it has to download and intialise all of the environments for each configured hook unlike pre-commit.ci (this is akin to writing your own GitHub Action to run pre-commit which would also have to download and initialise the environments)."
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#summary",
    "href": "posts/pre-commit-ci/index.html#summary",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "Summary",
    "text": "Summary\nThis article has covered\n\nWhy to integrate pre-commit into your Continuous Integration/Delivery pipeline.\nWhat the pre-commit.ci service is and the benefits it provides.\nHow to integrate pre-commit.ci with GitHub repositories.\nHow to integrate pre-commit with GitLab repositories.\n\nBy automating linting and testing in this manner you improve and shorten the feedback loop for developers and contributors which frees up more time and focus on the code itself."
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#links",
    "href": "posts/pre-commit-ci/index.html#links",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "Links",
    "text": "Links\n\nPre-commit : Protecting your future self (blog-post)- pre-requisite reading if you are not already using pre-commit\nPre-commit : Protecting your future self (slides) - slides from a talk given at Research Computing at the University of Leeds that extended the above blog post to cover the material in this post (hit s to see the ‚Äúspeaker notes‚Äù üòâ).\npre-commit\npre-commit.ci\nHow to use pre-commit to automatically correct commits and merge requests with GitLab CI - Stack Overflow"
  },
  {
    "objectID": "posts/whos_to_blame/index.html",
    "href": "posts/whos_to_blame/index.html",
    "title": "Who‚Äôs to Blame",
    "section": "",
    "text": "Git blame shows who made changes to which line of code for a given point in its history."
  },
  {
    "objectID": "posts/whos_to_blame/index.html#general",
    "href": "posts/whos_to_blame/index.html#general",
    "title": "Who‚Äôs to Blame",
    "section": "General",
    "text": "General\n\nAtlassian | git blame"
  },
  {
    "objectID": "posts/whos_to_blame/index.html#resources",
    "href": "posts/whos_to_blame/index.html#resources",
    "title": "Who‚Äôs to Blame",
    "section": "Resources",
    "text": "Resources\n\nIgnoring bulk change commits with git blame\nLittle things I like to do with Git\nIs there a way to customize the output of git blame"
  },
  {
    "objectID": "posts/git-ssh/index.html",
    "href": "posts/git-ssh/index.html",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "",
    "text": "How to configure individual Git repositories to use specific SSH keys. This is useful if you have more than one account on a forge, for example a personal and work account."
  },
  {
    "objectID": "posts/git-ssh/index.html#background",
    "href": "posts/git-ssh/index.html#background",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Background",
    "text": "Background\nTypically when pushing and pulling changes to a forge such as GitHub, GitLab or Codeberg you use an SSH (Secure SHell) key to authenticate that you have permission to access the repository."
  },
  {
    "objectID": "posts/git-ssh/index.html#ssh-keys",
    "href": "posts/git-ssh/index.html#ssh-keys",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "SSH Keys",
    "text": "SSH Keys\n\nConcept\nSSH keys are, in conjunction with ‚Äúkeychains‚Äù, used to save you having to enter a password each time you connect from one computer to another. They are generated on your computer and consist of two parts, a private key which remains on your computer and a public key which you place on remote computers you wish to connect to. There is a password associated with your key which is required to ‚Äúunlock‚Äù your private key on your computer. Only an unlocked private key will match with a public key. Think of the public key as the lock on your front door, and the private key the key you carry on your traditional, physical, keychain/keyring. Only when the two match will things be unlocked, although you have to unlock your private key when you want to use it just as you have to get your keys out of your pocket (although ‚Äúkeychains‚Äù help with this).\n\n\nGeneration\nThere are different algorithms for generating SSH key pairs. DSA is no longer considered secure and RSA keys should have at least 2048-bits if not 4096-bits. A good choice these days is to use an elliptic curve based key such as ed25519 as they are shorter and faster. For more on why you should use this key see the article Upgrade your SSH keys!.\nTo generate a key use the following command entering a secure (i.e.¬†long) password.\nssh-keygen -o -a 100 -t ed25519\nYou will be prompted for a filename to save your keys to, so you should know where to find them (the default is ~/.ssh/id_ed25519[.pub]). You have a private key ~/.ssh/id_ed25519 and a public ~/.ssh/id_ed25519.pub and we will use this to set up authentication on your Git Forge.\n\n\nForge Configuration\nUnder your account settings on your chosen Git Forge navigate to Settings > SSH and GPG Keys and select Add New Key on (GitHub). On GitLab navigate to Preferences > SSH Keys GitLab), this page allows you to add a new key.\nYou need to copy and paste your public key into the Key box on these pages and give it a name (typically the hostname of your computer is a good choice). To view your public key simply use cat and copy and paste it. You can optionally choose to set an expiration date for your key which is good practice but means you have to generate new keys in the future.\ncat ~/.ssh/id_ed25519.pub"
  },
  {
    "objectID": "posts/git-ssh/index.html#git-global-ssh-configuration",
    "href": "posts/git-ssh/index.html#git-global-ssh-configuration",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Git Global SSH Configuration",
    "text": "Git Global SSH Configuration\nTypically your global configuration for which key to use is set in ~/.ssh/config with an entry similar to the below.\nHost github.com\n     User git\n     Port 22\n     PreferredAuthentications publickey\n     IdentityFile ~/.ssh/id_ed25519\nHere it uses the User name git on port 22. The preferred authentication method is using a publickey and the private key used is stored locally at ~/.ssh/id_ed25519.\nWhen asked to connect to a forge using SSH (e.g.¬†git pull or git push) will look through the ~/.ssh/config file to see if there is a configuration section that matches the target and if so use the configuration defined there-in. You will then be prompted for your SSH private key password.\n\nWhat are Keychains?\nYou may be wondering how an SSH key makes your life easier, you are still prompted to enter a password when trying to interact with a Git Forge, or use it in a more traditional manner to connect over SSH to another server. This is where the magic of a ‚Äúkeychain‚Äù steps in to make your life easier, you still have to enter a password but only once to add your SSH key to the ‚Äúkeychain‚Äù. Typically keychains are front-ends for interacting with and managing SSH agent. The name is apt since you add your SSH key to the keychain once, typically on log-in, and are asked for your password to unlock it and then stores it in the SSH agent. Then each time SSH requires an SSH key it retrieves it from the keychain rather than prompting you for a password.\nThere are many different implementations of keychain such as the Funtoo Keychain Project, Seahorse the GNOME GUI management tool,"
  },
  {
    "objectID": "posts/git-ssh/index.html#git-per-repository-configuration",
    "href": "posts/git-ssh/index.html#git-per-repository-configuration",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Git Per Repository Configuration",
    "text": "Git Per Repository Configuration\nWe now get to the meat of this post, how to configure individual repositories to use specific SSH keys. This may be desirable if you have two accounts on the same forge e.g.¬†both on GitHub.com or both on GitLab.com? As of Git 2.10.0 you can configure each repository to use a specific key (source). At the command line‚Ä¶\ncd a/git/repository\ngit config core.sshCommand \"ssh -i ~/.ssh/work_ed25519 -F /dev/null\"\ngit config --local user.name \"Username\"\ngit config --local user.email \"repos@username.com\"\nThis adds the following to the repositories configuration which is stored under .git/config and you can of course enter this directly to the configuration file yourself.\n[core]\n    sshCommand = ssh -i ~/.ssh/work_ed25519 -F /dev/null\n[user]\n    name = Username\n    email = repos@username.com\nWhat is this doing? Well it‚Äôs instructing Git to run ssh using the private key file (with the -i flag to specify the identity_file) that is located at ~/.ssh/work_ed25519. Providing you have‚Ä¶\n\nAlready uploaded the public key (work_ed25519.pub) to your GitHub account.\nStored this key in a Keychain as described above.\n\n‚Ä¶you shouldn‚Äôt be prompted for a password.\nYou can now configure, on a repository basis, which SSH key is used by Git when pushing/pulling changes from the remote origin (typically a forge such as GitHub, GitLab, Codeberg or so forth). If however you have multiple projects you wish to setup with an alternative SSH key configuration it can be tedious to configure each repository. Thankfully Git >= 2.13 introduced Conditional includes to the configuration."
  },
  {
    "objectID": "posts/git-ssh/index.html#conditional-includes",
    "href": "posts/git-ssh/index.html#conditional-includes",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Conditional Includes",
    "text": "Conditional Includes\nYou global configuration is stored in ~/.gitconfig and defines key variables such user and name, the default editor and many other options, including a customised sshCommand as was added above to a local .git/config file.\nGit 2.13 introduced the aforementioned Conditional includes which works ‚Äú_by setting a includeIf.<condition>.path variable to the name of the file to be included.‚Äù. For our current case-use the <condition> we are interested in is whether the path, which is interpreted as a pattern, is a gitdir then we include what follows.\nFor example, we place all of our work related Git repositories under the ~/work/ directory and wish to use ~/.ssh/work_ed25519 for these and keep all of our personal repositories elsewhere and wish to use our main ~/.ssh/id_ed25519 key for those.\nOut ~/.gitconfig should look like\n[user]\n    name = Your Name\n    email = your.personal@email.com\n\n[includeIf \"gitdir:~/work/\"]    # Directory paths ending in '/** has the globbing wildcard '**' added by default.\n    path = ~/work/.gitconfig_work\n\n[core]\n    sshCommand = ssh -i ~/.ssh/id_ed25519 -F /dev/null\nThen our ~/work/.gitconfig_work can contain the alternative values we wish to use for all repositories under the ~/work/ directory.\n[user]\n    name = Your Name\n    email = your.work@email.com\n\n[core]\n    sshCommand = ssh -i ~/.ssh/work_ed25519 -F /dev/null"
  },
  {
    "objectID": "posts/git-ssh/index.html#commit-verification-with-ssh",
    "href": "posts/git-ssh/index.html#commit-verification-with-ssh",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Commit verification with SSH",
    "text": "Commit verification with SSH\nVerification of commits is a useful security feature, but beyond the scope of this article but as doing so with SSH keys is a recently supported feature on GitHub (see blog SSH commit verification now supported) I felt it worth mentioning."
  },
  {
    "objectID": "posts/git-ssh/index.html#links",
    "href": "posts/git-ssh/index.html#links",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Links",
    "text": "Links\n\nSSH\n\nSSH Academy\nOpenSSH Key Management, Part 1\nOpenSSH Key Management, Part 2\nOpenSSH Key Management, Part 3\n\n\n\nForges\n\nGitHub | Connect with SSH\nGitLab | Use SSH keys to communicate with GitLab\nCodeberg | Adding an SSH key to your account"
  },
  {
    "objectID": "posts/running-2022/index.html",
    "href": "posts/running-2022/index.html",
    "title": "Running in 2022",
    "section": "",
    "text": "I started running üèÉ in 2019. It never used to appeal but I wanted to get more regular exercise easily from my doorstep as despite commuting by bike for most of my life and cycling between 12-20km daily it wasn‚Äôt enough for me. Whilst I‚Äôve always cycled for commuting and cycling was delightful during the pandemic lockdowns I‚Äôve since enjoyed it less and less as it seems an increasing number of motorists seen to have little to no regard for the safety of pedestrians and cyclists. As a consequence I prefer running over cycling these days and get most of my aerobic exercise on two feet rather than two wheels."
  },
  {
    "objectID": "posts/running-2022/index.html#number-of-runs",
    "href": "posts/running-2022/index.html#number-of-runs",
    "title": "Running in 2022",
    "section": "Number of Runs",
    "text": "Number of Runs\nHow often I go running varies depending on the distance I‚Äôve done recently and more importantly whether I‚Äôm carrying an injury of some sort.\n\n\nCode\nmonth <- data |> ggplot(aes(year_month)) +\n    geom_bar() +\n    dark_theme_bw() +\n    ylab(\"Runs\") +\n    xlab(\"Month/Year\") +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(month)\n\n\n\n\nRuns by month\n\n\nCode\nweek <- data |> ggplot(aes(year_week)) +\n    geom_bar() +\n    dark_theme_bw() +\n    ylab(\"Runs\") +\n    xlab(\"Week\") +\n    scale_x_datetime(breaks = date_breaks(\"2 weeks\")) + ##, labels=date_format(\"%w\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(week)\n\n\n\n\nRuns by week\n\nRuns per by Month and Week\n\n‚Äì"
  },
  {
    "objectID": "posts/running-2022/index.html#distance",
    "href": "posts/running-2022/index.html#distance",
    "title": "Running in 2022",
    "section": "Distance",
    "text": "Distance\nI prefer longer runs, I often feel I‚Äôm only getting going and into a rhythm after 3-5km and therefore struggle to find motivation to go out for short runs. That said, whilst I can run > 21km (half-marathon) distances I don‚Äôt do so often and past experience tells me that if I run too much I end up injuring myself.\n\n\nCode\ndistance_time <- data |>\n    ggplot(aes(date, distance)) +\n    geom_line() +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Date\") +\n    # scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%b\"))\nggplotly(distance_time)\n\n\n\n\nDistance of Runs.\n\n\n\nDistance By Month\nMarch was a fallow month for running as I had a sore knee and a month of work and so I did a lot of cycling (A57 Snake Pass when it was closed, a morning jaunt to Bakewell and back) and DIY around the home and garden (finishing off a patio). I also had to ease off from mid-November and most of December due to a sore thigh.\n\n\nCode\nbar_distance_month <- data |> ggplot(aes(year_month)) +\n    geom_bar(aes(weight = distance)) +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\")  +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\"))\n    ## theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(bar_distance_month)\n\n\n\n\nBar Chart\n\n\nCode\nbox_distance_month <- data |> ggplot(aes(year_month, distance)) +\n    geom_boxplot(aes(factor(year_month), distance)) +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\")\n    ## scale_x_datetime(breaks = date_breaks(\"1 month\")) + ##, labels=date_format(\"%B\"))\n    ## theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(box_distance_month)\n\n\n\n\nBox Plot\n\n\nCode\nridge_distance_month <- data |> ggplot(aes(x=distance, y=factor(year_month), group=year_month, fill=after_stat(x))) +\n    # geom_density_ridges_gradient(scale=1, gradient_lwd=1.) +\n    geom_density_ridges_gradient(gradient_lwd=1.) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_fill_viridis_c(name=\"Distance (km)\", option=\"C\") +\n    dark_theme_bw() +\n    ylab(\"Year\") +\n    xlab(\"Distance (km)\") ## +\n    ## scale_y_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\"))\n## ggplotly(ridge_distance_month)\nridge_distance_month\n\n\nPicking joint bandwidth of 0.942\n\n\nCode\ndev.off()\n\n\nnull device \n          1 \n\n\n\n\n\nRidge Density\n\n\n\nDistance per month.\n\n\n\nDistance By Week\n\n\nCode\nbar_distance_week <- data |> ggplot(aes(year_week)) +\n    geom_bar(aes(weight = distance)) +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\")  +\n    ## scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%w\"))\n    theme(axis.text.x = element_text(angle=30, hjust=1))\n\nggplotly(bar_distance_week)\n\n\n\n\nBar Chart\n\n\nCode\nbox_distance_week <- data |> ggplot(aes(factor(year_week), distance)) +\n    geom_boxplot() +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\") +\n    ## scale_x_datetime(breaks = date_breaks(\"2 weeks\")) + ##, labels=date_format(\"%w\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(box_distance_week)\n\n\n\n\nBox Plot\n\nDistance per week.\n\n\n\nRuns v Distance\nLets look at how the number of runs per week affects the overall distance covered. Do I keep to a consistent distance if I‚Äôm doing less runs by doing longer runs? We can look at that by plotting the number of runs per week against either the total distance or the mean distance for that week. If I do fewer longer runs there should be a downwards trend, with weeks where I only run once having very high values, and weeks where I do multiple runs having very low means.\n\n\nCode\ntmp_data <- data |>\n    group_by(year_week) |>\n    summarise(runs = n(),\n              distance_total= sum(distance),\n              distance_mean = mean(distance))\nruns_distance_total<- tmp_data |>\n    ggplot(aes(runs, distance_total)) +\n    geom_point(aes(color=distance_mean, size=distance_mean, alpha=0.45)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Total Distance (km)\") +\n    xlab(\"Runs (km)\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(runs_distance_total, tooltip = c(\"year_week\", \"distance_total\", \"distance_mean\"))\n\n\n\n\nTotal Distance\n\n\nCode\nruns_distance_mean <- tmp_data |>\n    ggplot(aes(runs, distance_mean, label=year_week)) +\n    geom_point(aes(color=distance_total, size=distance_total, alpha=0.45)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Mean Distance (km)\") +\n    xlab(\"Runs (km)\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(runs_distance_mean, tooltip = c(\"year_week\", \"distance_total\", \"distance_mean\"))\n\n\n\n\nMean Distance\n\nRuns per week v Distance"
  },
  {
    "objectID": "posts/running-2022/index.html#pace",
    "href": "posts/running-2022/index.html#pace",
    "title": "Running in 2022",
    "section": "Pace",
    "text": "Pace\nI like pushing myself and going fast, I perhaps stupidly and against much perceived wisdom, think that if I‚Äôm able to hold a conversation then I‚Äôm not trying hard enough (in reality I rarely try talking as I always run on my own).\nBefore looking at pace over time its interesting to look at the relationship between distance and time. Obviously longer runs are going to take longer, but is the relationship linear or do I get slower the further I go?\n\n\nCode\ndistance_time_all <- data |>\n    ggplot(aes(distance, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Distance\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(distance_time_all)\n\n\n\n\nAll\n\n\nCode\ndistance_time_excl_outliers <- data |>\n    dplyr::filter(pace < 7) |>\n    ggplot(aes(distance, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Distance\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(distance_time_excl_outliers)\n\n\n\n\nExcluding Outliers\n\nDistance v Pace.\n\nThe relationship between distance and pace is interesting, one might expect that the pace decreases with the overall distance, but it depends on the terrain. Most of my shorter runs involved a fair proportion of uphill as I live in the bottom of a valley and my typical circuit takes me up the valley to some extent before turning around and heading back. Longer runs I would typically get out of the valley and run along fairly flat ground before heading back down and I think this is what causes the dip in the above graph (excluding outliers) in the range of 11-14km, but further distances I tire and so my pace drops.\nLooking at pace over time it increases as the year progresses but then runs are getting gradually longer, and I know I changed the route to include more hills. Strangely getting COVID at the end of August didn‚Äôt appear to negatively impact my pace although running with an injury late November/December did (I should probably have had a break but wanted to reach my goal so dialled down the frequency and distance).\n\n\nCode\nno_outliers <- data |>\n    dplyr::filter(pace < 7)\npace_timeseries<- data |>\n    ggplot(aes(date, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth(method=\"loess\", data=no_outliers) +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Date\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%b\"))\nggplotly(pace_timeseries)\n\n\n\n\nPace over time by distance.\n\n\n\nPace By Month\n\n\nCode\nbox_pace_month <- data |> ggplot(aes(factor(year_month), pace)) +\n    geom_boxplot() +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Month/Year\") ## +\n## scale_x_datetime(breaks = date_breaks(\"2 weeks\"), labels=date_format(\"%B\"))\nggplotly(box_pace_month)\n\n\n\n\nBox Plot\n\n\nCode\nridge_pace_month <- data |> ggplot(aes(x=pace, y=factor(year_month), group=year_month, fill=after_stat(x))) +\n    # geom_density_ridges_gradient(scale=1, gradient_lwd=1.) +\n    geom_density_ridges_gradient(scale=1, gradient_lwd=1.) +\n    scale_x_continuous(expand = c(0, 0)) +\n    # scale_y_discrete(expand = expansion(mult = c(0.01, 0.25))) +\n    scale_fill_viridis_c(name=\"Pace (min/km)\", option=\"C\") +\n    dark_theme_bw() +\n    ylab(\"Year\") +\n    xlab(\"Pace (min/km)\") ## +\n##     scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\"))\n## ggplotly(ridge_pace_month)\nridge_pace_month\n\n\n\n\n\nRidge Density\n\n\n\nPace per month.\n\n\n\nPace By Week\n\n\nCode\nbox_pace_week <- data |> ggplot(aes(factor(year_week), pace)) +\n    geom_boxplot() +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Month/Year\") +\n    ## scale_x_datetime(breaks = date_breaks(\"2 weeks\"), labels=date_format(\"%w\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(box_pace_week)\n\n\n\n\nPace per week."
  },
  {
    "objectID": "posts/running-2022/index.html#when-do-i-run",
    "href": "posts/running-2022/index.html#when-do-i-run",
    "title": "Running in 2022",
    "section": "When Do I Run?",
    "text": "When Do I Run?\nI‚Äôm much more a morning person and typically go out running on an empty stomach as I find it quite unpleasant to have a bellyful of food jiggling around inside me. But what days of the week and times do I actually go running? I can answer this with only a low degree of accuracy because whilst I do try to log my runs immediately after having completed them (post-stretching!) I don‚Äôt always do so and so the times below reflect the times I logged the run rather than started.\nIn the summer when it gets light early I‚Äôll sometimes go out running at 06:00 but clearly these are not reflected in the logged times.\n\n\nCode\nwhat_day_I_run <- data  |> ggplot(aes(year_day)) +\n    geom_bar() +\n    dark_theme_bw() +\n    xlab(\"Day of Week\") +\n    ylab(\"N\")\nggplotly(what_day_I_run)\n\n\n\n\nDay\n\n\nCode\nwhen_I_run <- data |> ggplot(aes(logged_at)) +\n    geom_bar() +\n    dark_theme_bw() +\n    xlab(\"Time of Day\") +\n    ylab(\"N\")\nggplotly(when_I_run)\n\n\n\n\nTime\n\n\nCode\nwhat_time_I_run_by_month <- data |> ggplot(aes(year_month, logged_at)) +\n    geom_point() +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    xlab(\"Month\") +\n    ylab(\"Time of Day\")\nggplotly(what_time_I_run_by_month)\n\n\n\n\nMonth v Time\n\n\nCode\nwhat_time_I_run_each_day <- data |> ggplot(aes(year_day, logged_at)) +\n    geom_point() +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    xlab(\"Day of Week\") +\n    ylab(\"Time of Day\")\nggplotly(what_time_I_run_each_day)\n\n\n\n\nDay v Time\n\nWhen I go running.\n\n\nDoes distance differ with time of day?\nIf I go out running in the morning I usually go further because I work five days a week and lunch-time runs have to fit within an hour. As you can see I don‚Äôt generally run in the evening as I don‚Äôt like exercising with food in my stomach.\n\n\nCode\nwhen_I_run_distance <- data |> ggplot(aes(logged_at, distance)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth() +\n    dark_theme_bw() +\n    xlab(\"Time of Day\") +\n    ylab(\"Distance\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") ## +\n    ## scale_x_datetime(breaks = date_breaks(\"1 hour\"), labels=date_format(\"%H\"))\nggplotly(when_I_run_distance)\n\n\n\n\nTime of Day v Distance.\n\n\n\n\nDoes pace differ with time of day?\nAs I‚Äôve mentioned I somewhat counter-intuitively get a faster mean pace when going further distances. Does this feature come through with the time of day and do I run faster in the morning or at other times. The graph below plots the time of day against the pace with the distance denoted by the size and colour of points.\n\n\nCode\nwhen_I_run_pace <- data |> ggplot(aes(logged_at, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth() +\n    dark_theme_bw() +\n    xlab(\"Time of Day\") +\n    ylab(\"Pace\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") ## +\n    ## scale_x_datetime(breaks = date_breaks(\"1 hour\"), labels=date_format(\"%H\"))\nggplotly(when_I_run_pace)\n\n\n\n\nWhen I go running v pace.\n\n\nI could go on and on making different types of plots but I think that is sufficient for now."
  },
  {
    "objectID": "posts/running-2022/index.html#code-folding",
    "href": "posts/running-2022/index.html#code-folding",
    "title": "Running in 2022",
    "section": "Code Folding",
    "text": "Code Folding\nIt should be possible to set options at the global level by setting the following in the site index.qmd.\nexecute:\n  code-fold: true\n  code-tools: true\n  code-link: true\nI‚Äôm using the blogging feature of Quarto but adding this to the site index.qmd didn‚Äôt work when previewed locally. I tried adding it to the YAML header for the post itself (i.e.¬†posts/running-2022/index.qmd) but no joy, the code chunks were still displayed. I could however set this on a per-chunk basis though so each code chunk carries the options.\n#| code-fold: true\n#| code-link: true\n#| code-tools: true\n#| warning: false"
  },
  {
    "objectID": "posts/running-2022/index.html#table-and-figure-caption-locations-are-should-be-configurable",
    "href": "posts/running-2022/index.html#table-and-figure-caption-locations-are-should-be-configurable",
    "title": "Running in 2022",
    "section": "Table and Figure caption locations are should be configurable",
    "text": "Table and Figure caption locations are should be configurable\nCaptions were by default underneath each picture which is perhaps ok when reading as a PDF but this is rendered as HTML and I would prefer these to be at the top so that readers see the heading as they scroll down (I‚Äôm often torn about figure headings and labels and feel they should be included in the image itself so they are retained if/when they are used elsewhere).\nFortunately you can specify the location of table and figure captions. Unfortunately this doesn‚Äôt appear to render correctly when using the blogging feature and all captions are still at the bottom.\n#| fig-cap-location: [top|bottom|margin]\n#| tbl-cap-location: [top|bottom|margin]"
  },
  {
    "objectID": "posts/running-2022/index.html#two-for-the-price-of-one",
    "href": "posts/running-2022/index.html#two-for-the-price-of-one",
    "title": "Running in 2022",
    "section": "Two for the price of one",
    "text": "Two for the price of one\nIts possible to include two or more sub-figures in a code chunk and have them both displayed.\n#| label: pace-per-month\n#| fig-cap: \"Pace per month.\"\n#| fig-subcap:\n#|    - \"Bar Chart\"\n#|    - \"Box Plot\"\n#| fig-alt: \"A plot of pace per month/week in 2022.\"\nIn the output format of the blog these do not appear side by side, but rather underneath each other."
  },
  {
    "objectID": "posts/running-2022/index.html#plotly-plays-with-quarto",
    "href": "posts/running-2022/index.html#plotly-plays-with-quarto",
    "title": "Running in 2022",
    "section": "Plotly plays with Quarto",
    "text": "Plotly plays with Quarto\nUsing the plotly R package with Quarto ‚ÄúJust Works‚Äù, the plots render nicely in the page and are zoom-able with tool-tips appearing over key points."
  },
  {
    "objectID": "posts/running-2022/index.html#some-graphs-appear-where-i-dont-expect-them-to",
    "href": "posts/running-2022/index.html#some-graphs-appear-where-i-dont-expect-them-to",
    "title": "Running in 2022",
    "section": "Some graphs appear where I don‚Äôt expect them to",
    "text": "Some graphs appear where I don‚Äôt expect them to\nAstute readers will notice that some of the ridge plot graphs appear more than once. I couldn‚Äôt work out why this was, the code does not specify that they should be shown again. For example the Ridge Density plot for total Distance by Month also appeared under the total Distance by Week. To try working around this I attempted to explicitly use dev.off() after the initial generation of the first Ridge Density Plot, but this had the undesired effect of including output from the call and an additional code-chunk. Not one I‚Äôve sorted yet. ü§î"
  },
  {
    "objectID": "posts/running-2022/index.html#emojis",
    "href": "posts/running-2022/index.html#emojis",
    "title": "Running in 2022",
    "section": "Emoji‚Äôs",
    "text": "Emoji‚Äôs\nTo include emoji‚Äôs in the Markdown it‚Äôs necessary to add the following to the header of the specific page (i.e.¬†posts/running_2022/index.qmd)\nfrom: markdown+emoji\nText based emoji names (e.g.¬†:thinking: ü§î ; :snake üêç ; :tada: üéâ) are then automatically included when rendering."
  },
  {
    "objectID": "posts/running-2022/index.html#formatting-datestimes-in-axes",
    "href": "posts/running-2022/index.html#formatting-datestimes-in-axes",
    "title": "Running in 2022",
    "section": "Formatting Dates/Times in Axes",
    "text": "Formatting Dates/Times in Axes\nI‚Äôve a few more niggles to round out such as the formatting of the months/weeks which I should probably do up-front in the dataframe rather leaving them as POSIXct objects as then the and then the ggplot2 functions scale_x_datetime() can be used directly (in some places I convert to factors which doesn‚Äôt help)."
  },
  {
    "objectID": "posts/running-2022/index.html#previewing-graphs",
    "href": "posts/running-2022/index.html#previewing-graphs",
    "title": "Running in 2022",
    "section": "Previewing Graphs",
    "text": "Previewing Graphs\nOne very nice feature I discovered recently courtesy of a short video by Bruno Rodrigues (thanks Bruno üëç) is the ability to preview graphs in the browser. This is unlikely to be something you need if you use RStudio but as with most things I use Emacs and Emacs Speaks Statistics and so normally I get an individual X-window appearing showing the plot. Instead we can use the httpgd package to start web-server that renders the images. It keeps a history of lots that have been produced and you can scroll back and forth through them.\n> httpgd::hgd()\nhttpgd server running at:\n  http://127.0.0.1:42729/live?token=beRmYcSn\nThen just create your plots and hey-presto the graphs appear at the URL. üßô"
  },
  {
    "objectID": "posts/running-2022/index.html#renv",
    "href": "posts/running-2022/index.html#renv",
    "title": "Running in 2022",
    "section": "renv",
    "text": "renv\nIn order to have this page publish correctly I had to initialise a renv within the repository and include the renv.lockfile so that the GitHub action/workflow installed all of the packages I use and their dependencies in the runner that renders the blog.\nWhilst I‚Äôm familiar with virtual environments under Python this is something relatively new to me for R. I won‚Äôt write much other than the process involved initialising the environment, installing the dependencies then updating the lockfile.\n> renv::init()\n> install.packages(c(\"dplyr\", \"ggdark\", \"ggridges\", \"hms\", \"knitr\", \"lubridate\", \"orgutils\", \"plotly\", \"readr\",\n                     \"scales\", \"stringr\"))\n> renv::snapshot()\n> q()\ngit add renv.lockfile"
  },
  {
    "objectID": "posts/cli-alternatives/index.html",
    "href": "posts/cli-alternatives/index.html",
    "title": "Linux Command Line Alternatives",
    "section": "",
    "text": "The command line is my second home when sat at a computer (Emacs is my first ;-) and the UNIX Philosophy is the key to the huge amount of highly productive tools that are available under UNIX, GNU/Linux, BSD, OSX, PowerShell etc.\nMany of these tools work and have done for many years, but there are some new alternatives that are coming through that build and modernise on these tools without breaking the core functionality. Here I detail some of the tools and why you might want to use them. Each tool has a brief introduction with some example output shown and then some aliases listed that you can drop into ~/.bash_aliases or ~/.oh-my-zsh/custom/aliases to use on your system."
  },
  {
    "objectID": "posts/cli-alternatives/index.html#alternatives",
    "href": "posts/cli-alternatives/index.html#alternatives",
    "title": "Linux Command Line Alternatives",
    "section": "Alternatives",
    "text": "Alternatives\n\nbat\nbat is ‚ÄúA cat(1) clone with wings.‚Äù. It automatically uses syntax highlighting and integrates with git if a file is version controlled to show changes and lots more. You can pipe input to it, including from e.g.¬†curl -s https://server.com/some_file\n\nExamples\n‚ù± bat pyproject.toml\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n       ‚îÇ File: pyproject.toml\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1   ‚îÇ [build-system]\n   2   ‚îÇ requires = [\n   3   ‚îÇ   \"setuptools\",\n   4   ‚îÇ   \"versioneer==0.26\",\n   5   ‚îÇ   \"wheel\"]\n   6   ‚îÇ build-backend = \"setuptools.build_meta\"\n   7   ‚îÇ\n   8   ‚îÇ [tool.black]\n   9   ‚îÇ line-length = 120\n  10   ‚îÇ target-version = ['py38', 'py39', 'py310']\n  11   ‚îÇ include = '\\.pyi?$'\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nConfiguration\nYou can generate a default configuration file with\nbat --generate-config-file\nThis will be saved at ~/.config/bat/config and you can edit it as desired.\n\n\n\ncheat\ncheat is actually a web-service that returns short ‚Äúcheats‚Äù for command line programmes which will often cover many use cases and save you having to read the rather dry man pages for functions.\n\nExamples\n‚ù± cheat cheat\n cheat:cheat\n# To see example usage of a program:\ncheat <command>\n\n# To edit a cheatsheet\ncheat -e <command>\n\n# To list available cheatsheets\ncheat -l\n\n# To search available cheatsheets\ncheat -s <command>\n\n# To get the current `cheat' version\ncheat -v\n\n tldr:cheat\n# cheat\n# Create and view interactive cheat sheets on the command-line.\n# More information: <https://github.com/cheat/cheat>.\n\n# Show example usage of a command:\ncheat command\n\n# Edit the cheat sheet for a command:\ncheat -e command\n\n# List the available cheat sheets:\ncheat -l\n\n# Search available the cheat sheets for a specified command name:\ncheat -s command\n\n# Get the current cheat version:\ncheat -v\n\n\nAliases\nYou don‚Äôt need to install anything to use this, instead define an alias for your shell (e.g.¬†in ~/.bashrc/~/.zshrc/~/.oh-my-zsh/custom/aliases.zsh)\n## Linux commands https://github.com/chubin/cheat.sheets\ncheat () {\n    curl cheat.sh/\"$@\"\n}\n\n\n\ndifftastic\ndifftastic (GitHub) is an alternative to the default GNU diff packaged with most systems. It is ‚Äúaware‚Äù of some 30 or so programming languages and will show diffs side-by-side rather than the traditional linear manner. It integrates easily with Git so when you git diff it uses difft to show the differences.\nHighly recommended, but don‚Äôt take my word for it, give it a whirl yourself.\n\n\nduf\nduf is a nice alternative to the traditional du and df commands which report disk usage and file/directory usage respectively.\n\nExamples\n‚ù± tldr duf\n\n  duf\n\n  Disk Usage/Free Utility.\n  More information: https://github.com/muesli/duf.\n\n  - List accessible devices:\n    duf\n\n  - List everything (such as pseudo, duplicate or inaccessible file systems):\n    duf --all\n\n  - Only show specified devices or mount points:\n    duf path/to/directory1 path/to/directory2 ...\n\n  - Sort the output by a specified criteria:\n  duf --sort size|used|avail|usage\n\n‚ù± duf\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ 4 local devices                                                                              ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ MOUNTED ON ‚îÇ   SIZE ‚îÇ  USED ‚îÇ  AVAIL ‚îÇ              USE%             ‚îÇ TYPE ‚îÇ FILESYSTEM     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ /          ‚îÇ  19.5G ‚îÇ  9.5G ‚îÇ   9.0G ‚îÇ [#########...........]  48.9% ‚îÇ ext4 ‚îÇ /dev/mmcblk0p2 ‚îÇ\n‚îÇ /boot      ‚îÇ 199.8M ‚îÇ 38.8M ‚îÇ 161.0M ‚îÇ [###.................]  19.4% ‚îÇ vfat ‚îÇ /dev/mmcblk0p1 ‚îÇ\n‚îÇ /home      ‚îÇ   9.3G ‚îÇ  3.7G ‚îÇ   5.0G ‚îÇ [########............]  40.4% ‚îÇ ext4 ‚îÇ /dev/mmcblk0p3 ‚îÇ\n‚îÇ /mnt/usb   ‚îÇ   4.5T ‚îÇ  3.2T ‚îÇ   1.1T ‚îÇ [##############......]  71.3% ‚îÇ ext4 ‚îÇ /dev/sda1      ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ 6 special devices                                                                                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ MOUNTED ON     ‚îÇ   SIZE ‚îÇ   USED ‚îÇ  AVAIL ‚îÇ              USE%             ‚îÇ TYPE     ‚îÇ FILESYSTEM ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ /dev           ‚îÇ   3.7G ‚îÇ     0B ‚îÇ   3.7G ‚îÇ                               ‚îÇ devtmpfs ‚îÇ dev        ‚îÇ\n‚îÇ /dev/shm       ‚îÇ   3.9G ‚îÇ     0B ‚îÇ   3.9G ‚îÇ                               ‚îÇ tmpfs    ‚îÇ tmpfs      ‚îÇ\n‚îÇ /run           ‚îÇ   3.9G ‚îÇ 812.0K ‚îÇ   3.9G ‚îÇ [....................]   0.0% ‚îÇ tmpfs    ‚îÇ run        ‚îÇ\n‚îÇ /run/user/1001 ‚îÇ 789.3M ‚îÇ  20.0K ‚îÇ 789.3M ‚îÇ [....................]   0.0% ‚îÇ tmpfs    ‚îÇ tmpfs      ‚îÇ\n‚îÇ /run/user/966  ‚îÇ 789.3M ‚îÇ  24.0K ‚îÇ 789.2M ‚îÇ [....................]   0.0% ‚îÇ tmpfs    ‚îÇ tmpfs      ‚îÇ\n‚îÇ /tmp           ‚îÇ   3.9G ‚îÇ   4.0K ‚îÇ   3.9G ‚îÇ [....................]   0.0% ‚îÇ tmpfs    ‚îÇ tmpfs      ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\n\nfd\nfd is an alternative to find that is easier to use. It is ‚Äúopinionated‚Äù (i.e.¬†decisions have been made about default options that you may not agree with) but purportedly covers ~80% of use cases. It works directly with regular expressions.\n\nExamples\n‚ù± tldr fd\n\n  fd\n\n  An alternative to `find`.\n  Aims to be faster and easier to use than `find`.\n  More information: https://github.com/sharkdp/fd.\n\n  - Recursively find files matching the given pattern in the current directory:\n    fd pattern\n\n  - Find files that begin with \"foo\":\n    fd '^foo'\n\n  - Find files with a specific extension:\n    fd --extension txt\n\n  - Find files in a specific directory:\n    fd pattern path/to/directory\n\n  - Include ignored and hidden files in the search:\n    fd --hidden --no-ignore pattern\n\n  - Execute a command on each search result returned:\n    fd pattern --exec command\n\n\n\njq\njq is to JSON (JavaScript Object Notation) what awk/grep/sed is to text files. It allows parsing, searching and selecting of JSON files, which if you‚Äôve not encountered them before take a bit of getting used to.\n\nExamples\nDetails of using jq are really beyond the scope of this short article, like awk its almost a language in itself.\n‚ù± tldr jq\n\n  jq\n\n  A command-line JSON processor that uses a domain-specific language.\n  More information: https://stedolan.github.io/jq/manual/.\n\n  - Execute a specific expression (print a colored and formatted json):\n    cat path/to/file.json | jq '.'\n\n  - Execute a specific script:\n    cat path/to/file.json | jq --from-file path/to/script.jq\n\n  - Pass specific arguments:\n    cat path/to/file.json | jq --arg \"name1\" \"value1\" --arg \"name2\" \"value2\" ... '. + $ARGS.named'\n\n  - Print specific keys:\n    cat path/to/file.json | jq '.key1, .key2, ...'\n\n  - Print specific array items:\n    cat path/to/file.json | jq '.[index1], .[index2], ...'\n\n  - Print all array items/object keys:\n    cat path/to/file.json | jq '.[]'\n\n  - Add/remove specific keys:\n    cat path/to/file.json | jq '. +|- {\"key1\": \"value1\", \"key2\": \"value2\", ...}'\n\n\n\nlsd\nlsd is lsDeluxe and is very similar to exa but with a few additions such as icons.\n\nExamples\n‚ù± l\n.rw-r--r-- neil neil  144 B  Sun Aug 14 19:56:53 2022 ÔÄñ #.gitlab-ci.yml#\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:21:25 2022 ÔÑï .\ndrwxrwxr-x root users 4.0 KB Tue Aug 30 20:46:37 2022 ÔÑï ..\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:21:56 2022 Óóª .git\ndrwxr-xr-x neil neil  4.0 KB Sun Aug 14 21:51:03 2022 ÓóΩ .github\n.rw-r--r-- neil neil  613 B  Sun Aug 14 21:44:38 2022 Ôáì .gitignore\n.rw-r--r-- neil neil  151 B  Sun Aug 14 19:56:13 2022 Ôäñ .gitlab-ci.yml\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:21:25 2022 ÔÑï .quarto\n.rw-r--r-- neil neil  386 B  Thu Sep 15 22:05:23 2022 Óòã _quarto.yaml\n.rw-r--r-- neil neil  263 B  Sun Aug 14 10:59:13 2022 ÔÄñ _quarto.yml~\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:05:24 2022 ÔÑï _site\n.rw-r--r-- neil neil  1.1 KB Thu Sep 15 22:05:23 2022 ÔÄñ about.qmd\n.rw-r--r-- neil neil  455 B  Sun Aug 14 11:02:13 2022 ÔÄñ about.qmd~\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:05:23 2022 ÔÑï img\n.rw-r--r-- neil neil  185 B  Sun Aug 14 22:22:04 2022 ÔÄñ index.qmd\n.rw-r--r-- neil neil  191 B  Sun Aug 14 10:59:13 2022 ÔÄñ index.qmd~\n.rw-r--r-- neil neil   34 KB Sun Aug 14 21:14:38 2022 Óòä LICENSE\n.rw-r--r-- neil neil  1.7 KB Thu Sep 15 22:05:23 2022 ÔÄñ links.qmd\n.rw-r--r-- neil neil  237 B  Thu Sep 15 21:46:30 2022 ÔÄñ links.qmd~\ndrwxr-xr-x neil neil  4.0 KB Wed Sep 14 20:24:25 2022 ÔÑï posts\n.rw-r--r-- neil neil  378 B  Thu Aug 25 23:20:16 2022 Óòâ README.md\n.rw-r--r-- neil neil   13 B  Sun Aug 14 21:58:38 2022 ÔÖú requirements.txt\n.rw-r--r-- neil neil   17 B  Sun Aug 14 21:24:35 2022 Óùâ styles.css\ndrwxr-xr-x neil neil  4.0 KB Thu Aug 25 23:20:16 2022 ÔÑï www\n\n\nAliases\nalias ls='lsd'\nalias l='ls -lha'\nalias lla='ls -la'\nalias lt='ls --tree'\n\n\n\ntldr\ntldr is very similar to cheat in that it shows short, simple examples of using a command. There are a number of different clients written in C, Node and Python as well as a few others. It depends on jq so you will have to install that if you want to use yq.\n\nExamples\n‚ù± tldr tldr\n\n  tldr\n\n  Display simple help pages for command-line tools from the tldr-pages project.\n  More information: https://tldr.sh.\n\n  - Print the tldr page for a specific command (hint: this is how you got here!):\n    tldr command\n\n  - Print the tldr page for a specific subcommand:\n    tldr command-subcommand\n\n  - Print the tldr page for a command for a specific [p]latform:\n    tldr -p android|linux|osx|sunos|windows command\n\n  - [u]pdate the local cache of tldr pages:\n    tldr -u\n\n\n\nyq\nyq is to YAML (YAML Ain‚Äôt Markup Language) what jq is to JSON. Written in Python it allows fast and efficient parsing, searching and selecting of YAML files.\n\nExamples\n‚ù± tldr yq\n\n  yq\n\n  A lightweight and portable command-line YAML processor.\n  More information: https://mikefarah.gitbook.io/yq/.\n\n  - Output a YAML file, in pretty-print format (v4+):\n    yq eval path/to/file.yaml\n\n  - Output a YAML file, in pretty-print format (v3):\n    yq read path/to/file.yaml --colors\n\n  - Output the first element in a YAML file that contains only an array (v4+):\n    yq eval '.[0]' path/to/file.yaml\n\n  - Output the first element in a YAML file that contains only an array (v3):\n    yq read path/to/file.yaml '[0]'\n\n  - Set (or overwrite) a key to a value in a file (v4+):\n    yq eval '.key = \"value\"' --inplace path/to/file.yaml\n\n  - Set (or overwrite) a key to a value in a file (v3):\n    yq write --inplace path/to/file.yaml 'key' 'value'\n\n  - Merge two files and print to stdout (v4+):\n    yq eval-all 'select(filename == \"path/to/file1.yaml\") * select(filename == \"path/to/file2.yaml\")' path/to/file1.yaml path/to/file2.yaml\n\n  - Merge two files and print to stdout (v3):\n    yq merge path/to/file1.yaml path/to/file2.yaml --colors"
  },
  {
    "objectID": "posts/cli-alternatives/index.html#installation",
    "href": "posts/cli-alternatives/index.html#installation",
    "title": "Linux Command Line Alternatives",
    "section": "Installation",
    "text": "Installation\nMost of these programmes will be available in your systems package manager, if they are not you should consult the project page directly for install instructions.\n\nLinux\n# Gentoo\nemerge -av bat duf fd jq lsd tldr yq\n\n# Arch\npacman -Syu bat duf fd jq lsd tldr yq\n\n# Ubuntu\nsudo apt-install bat duf fd jq lsd tldr yq\n\n\nOSX\nbrew install bat duf fd jq lsd tldr yq\n\n\nWindows\nWARNING None of these have been tested I do not have access to a Windows system running PowerShell. They use Scoop a command-line installer for Windows.\nscoop install lsd"
  },
  {
    "objectID": "posts/cli-alternatives/index.html#links",
    "href": "posts/cli-alternatives/index.html#links",
    "title": "Linux Command Line Alternatives",
    "section": "Links",
    "text": "Links\n\nbat\ncheat\nduf\nexa\nfd\njq\nlsd\ntldr\nyq"
  },
  {
    "objectID": "posts/pre-commit-updates/index.html",
    "href": "posts/pre-commit-updates/index.html",
    "title": "Pre-Commit : Customising and Updating",
    "section": "",
    "text": "Pre-commit is a tool for running hooks prior to making commits to your Git history. If you‚Äôre not familiar with it then you may want to read the earlier post Pre-Commit : Protecting your future self. This article discusses updating pre-commit and is prompted by a change in the flake8 repository."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#pre-commit-hooks",
    "href": "posts/pre-commit-updates/index.html#pre-commit-hooks",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Pre-commit hooks",
    "text": "Pre-commit hooks\nA lot of the power of pre-commit comes from the vast array of hooks that are available that users make available. These are included under repos: section of the .pre-commit-config.yaml and typically require a minimum of the repo: and the rev: to use and then optionally a hooks: section. The sample-config that pre-commit will auto-generate looks like‚Ä¶\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\nAfter finding a repository and hook that you wish to use hooks repository you need to add it to your .pre-commit-config.yaml. Here we add the pylint repository and whilst it only has one hook we explicitly add it.\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n-   repo: https://github.com/PyCQA/pylint\n    rev: v2.15.5\n    hooks:\n    -   id: pylint\nIf a repository has more than one hook available then it can be enabled by listing its id: as is the case in the hooks above for the pre-commit-hooks repository."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#local-hooks",
    "href": "posts/pre-commit-updates/index.html#local-hooks",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Local Hooks",
    "text": "Local Hooks\nIn some instances the provisioned repositories do not always meet the requirements. One example of this is the pylint action which parses the code-base to detect errors using pylint. Typically most Python packages have their own dependencies but because the Pylint action pulls down and uses its own virtual environment these packages are not installed. As a consequence pylint reports a lot of import-error as its unable to import the required dependencies.\nThe solution to this is to write a local hook, which instead of defining a GitHub repository as the repo: uses the value local. Thus to run pylint in a local environment from pre-commit you would add the following to your .pre-commit-config.yaml\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n# -   repo: https://github.com/PyCQA/pylint\n#     rev: v2.15.5\n#     hooks:\n#     -   id: pylint\n-   repo: local\n    hooks:\n    -   id: pylint\n        name: PyLint\n        entry: python -m pylint.__main__\n        language: system\n        files: \\.py$\nFor this to work you would have to ensure that you have a virtual environment activated that includes the package dependencies, including pylint, when you make you git commit so that pre-commit can find and import all the required packages."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#updating-pre-commit",
    "href": "posts/pre-commit-updates/index.html#updating-pre-commit",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Updating pre-commit",
    "text": "Updating pre-commit\nAfter adding a new repo and hook it will not be immediately ready to use as the environment has not been initialised. You can wait until your next commit or force this with the autoupdate option. This will update all repositories that are defined in your configuration.\n$ pre-commit autoupdate\nUpdating https://github.com/pre-commit/pre-commit-hooks ... updating v3.2.0 -> v4.3.0.\nUpdating https://github.com/PyCQA/pylint ... [INFO] Initializing environment for https://github.com/PyCQA/pylint.\nalready up to date."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#repository-changes",
    "href": "posts/pre-commit-updates/index.html#repository-changes",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Repository Changes",
    "text": "Repository Changes\nSometimes, albeit rarely, repositories change their location as was the case recently when flake8 moved from GitLab to GitHub. As a consequence any pre-commit that uses flake8 repo/hook and configured to run in Continuous Integration pipelines failed as it was unable to download and run the flake8 environment. The solution is simply to update the repo:.\nBefore this change the entry for flake8 looked like‚Ä¶\n-   repo: https://gitlab.com/pycqa/flake8.git\n    rev: 3.9.2\n    hooks:\n    -   id: flake8\n        additional_dependencies: [flake8-print]\n        args: [\"topostats\", \"tests\"]\n        types: [python]\nTo update to use the new repository it should point to github.com as shown below.\n-   repo: https://github.com/pycqa/flake8.git\n    rev: 3.9.2\n    hooks:\n    -   id: flake8\n        additional_dependencies: [flake8-print]\n        args: [\"topostats\", \"tests\"]\n        types: [python]\nAfter making this change you have to pre-commit autoupdate to force downloading and updating from the new source, otherwise your existing older revision will be used locally."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#links",
    "href": "posts/pre-commit-updates/index.html#links",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Links",
    "text": "Links\n\nPre-Commit : Protecting your future self\nPre-commit\nPre-commit hooks\npylint\nflake8"
  },
  {
    "objectID": "posts/linting/index.html",
    "href": "posts/linting/index.html",
    "title": "Linting - What is all the fluff about?",
    "section": "",
    "text": "NB This article originally appeared on RSE University of Sheffield but is updated here.\nIf you‚Äôve been dabbling in programming for a while you may have heard of ‚Äúlinting your code‚Äù which is a process of static code analysis to remove the ‚Äúfluff‚Äù from your code. Just as physically linting your clothes removes unwanted fluff, linting your code removes ‚Äúfluff‚Äù and can help‚Ä¶\nThis helps reduce the technical debt which impacts the amount of time required for maintenance and further development of a code base. The main focus of this article is the use of linting to ensure consistent coding style, it focuses on Python under Linux but similar tools are available for other operating systems and languages."
  },
  {
    "objectID": "posts/linting/index.html#style-matters",
    "href": "posts/linting/index.html#style-matters",
    "title": "Linting - What is all the fluff about?",
    "section": "Style Matters",
    "text": "Style Matters\nWhat has style got to do with writing code? Trends come and go in fashion but coding styles are meant to be relatively static and not change with the season, although they can and do evolve over time. This is because using a consistent and widely used style when writing code makes it easier for other people, often your future self, to read and understand the code you have written. If code is easier to understand then its easier to modify, update, extend, improve and in general maintain.\nA useful insight from Gudio van Rossum, the creator of Python is that ‚Äúcode is read much more often than it is written‚Äù and so it should be easy to understand and not obfuscate its intent. Python is quite good for this as it is an expressive language which encourages coders to be explicit when naming variables, functions, classes and so forth so that their purpose and intention is clear, although the same is true of most modern languages. However, going a step further and using consistent styles to format and layout code helps enhance this."
  },
  {
    "objectID": "posts/linting/index.html#linting-in-python",
    "href": "posts/linting/index.html#linting-in-python",
    "title": "Linting - What is all the fluff about?",
    "section": "Linting in Python",
    "text": "Linting in Python\nThe most widely used Python style is defined in the long established PEP 8: The Style Guide for Python Code. There are a number of tools available that will lint your Python code for you and most integrate with your IDE, whether that is Visual Studio Code, PyCharm or Emacs. Some of the formatting and linting tools available for Python are‚Ä¶\n\nPylint - checks for errors in Python code, tries to enforce a coding standard and looks for code smells.\nYAPF - takes the code and reformats it to the best formatting that conforms to the style guide.\nBlack - The Uncompromising Code Formatter\nFlake8 - Your Tool For Style Guide Enforcement\nProspector - Python Static Analysis\nmypy - Optional Static Typing for Python\n\nHere we will work through linting and formatting the simple file below (available as a download here) using PyLint and Black.\nimport numpy as np\nfrom pathlib import Path\nfrom typing import Union\nimport csv\n\ndef save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -> None:\n    \"\"\"Save a list of random numbers (floats) to the given file.\n\n    The stated number of random numbers will be saved to the given target file, if the directory structure\n    doesn't exist it will be created. Output will by default be over-written.\n    Parameters\n    ----------\n    size : int\n        Number of random numbers to generate\n    seed: int\n        Seed for random number generation\n    save_as : Union[str, Path]\n        Directory/file to save numbers to.\n    \"\"\"\n    rng = np.random.default_rng()\n    random_numbers = rng.random(size)\n\n    with Path(save_as).open('w') as out:\n        writer = csv.write(out)\n        writer.writerows(random_numbers)\n\nLinting with PyLint\nWe will lint this file using Pylint to find out what errors there are and how its style can be improved to conform with PEP8 guidelines.\nFirst you need to install pylint, typically in your virtual environment.\npip install pylint\nPylint can be configured using a ~/.pylintrc file in your home directory and over time this will grow as you customise your configuration but for now we will make one simple change from the default which is to increase the accepted line length. Create the file and save it with the following content.\n[FORMAT]\n## Maximum number of characters on a single line.\nmax-line-length=120\nOpen a terminal and navigate to the location you saved the example file save_random_numbers.py activate the virtual environment you installed pylint under if its not already being used and then type the following to run Pylint against your code‚Ä¶\npylint save_random_numbers.py\nYou should see output similar to the following‚Ä¶\n ‚ù± pylint save_random_numbers.py\n************* Module save_random_numbers\nsave_random_numbers.py:1:0: C0114: Missing module docstring (missing-module-docstring)\nsave_random_numbers.py:5:66: E0602: Undefined variable 'Union' (undefined-variable)\nsave_random_numbers.py:5:35: W0613: Unused argument 'seed' (unused-argument)\nsave_random_numbers.py:2:0: C0411: standard import \"from pathlib import Path\" should be placed before \"import numpy as np\" (wrong-import-order)\nsave_random_numbers.py:3:0: C0411: standard import \"import csv\" should be placed before \"import numpy as np\" (wrong-import-order)\n\n-------------------------------------------------------------------\nYour code has been rated at 0.00/10\nThe output tells us which module has been inspected on the first line. Each subsequent line indicates\n\nThe file.\nThe line the problem has been encountered on.\nThe column.\nA somewhat cryptic error code and then a message about the problem\nA more descriptive generic message associated with the error code.\n\nAt the moment we are only looking at one file, but when using PyLint against larger code bases this information is vital in helping direct you to the location of code that needs changing. At the end PyLint rates your code, ideally you should aim to get a score of 10.0/10.\nThe messages are quite informative, taking each in turn we can work through resolving them.\n\nMissing module docstring (missing-module-docstring)\nEach Python module should have a docstring as the very first line that describes what it does. In this example it might be considered superflous but its good practice to get in the habit of writing these as it comes in useful when documentation is automatically generated from the docstrings in the code. To fix it we can add a short docstring at the top.\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport numpy as np\nfrom pathlib import Path\n\n\nUndefined variable 'Union' (undefined-variable)\nThis error arises because the type hint uses Union but it hasn‚Äôt been imported. It‚Äôs from the typing module so we can import it.\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport numpy as np\nfrom pathlib import Path\nfrom typing import Union\n\n\nUnused argument 'seed' (unused-argument)\nThis is very useful to be informed about because the seed argument, according to the docstring, is meant to be used in the call to the random number generator and ensures we will get the same set of random numbers generated each time we call the function with that seed, however, as Pylint has informed us we haven‚Äôt actually used it within the save_random_number() function. We can correct that by adding it when we instantiate the random number generator.\nrng = np.random.default_rng(seed=seed)\n\n\nstandard import \"from pathlib import Path\" should be placed before \"import numpy as np\" (wrong-import-order)\nThis message, like the one that follows it, is telling us that the order in which we have imported modules is incorrect, because the PEP8 guide recommends that core modules, which both csv and pathlib are, should be imported before other modules. We can correct this by changing the order (and because we have added an import from the typing module which is also a core module we move that too).\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport csv\nfrom pathlib import Path\nfrom typing import Union\n\nimport numpy as np\nOnce corrected your file should look like this‚Ä¶\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport csv\nfrom pathlib import Path\nfrom typing import Union\nimport numpy as np\n\ndef save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -> None:\n    \"\"\"Save a list of random numbers (floats) to the given file.\n\n    The stated number of random numbers will be saved to the given target file, if the directory structure\n    doesn't exist it will be created. Output will by default be over-written.\n\n    Parameters\n    ----------\n    size : int\n        Number of random numbers to generate\n    seed: int\n        Seed for random number generation\n    save_as : Union[str, Path]\n        Directory/file to save numbers to.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    random_numbers = rng.random(size)\n\n    with Path(save_as).open('w') as out:\n        writer = csv.write(out)\n        writer.writerows(random_numbers)\n‚Ä¶and you can now run PyLint against it to see if you‚Äôve improved your score.\n ‚ù± pylint save_random_numbers.py\n************* Module save_random_numbers\nsave_random_numbers.py:7:66: E1136: Value 'Union' is unsubscriptable (unsubscriptable-object)\n\n------------------------------------------------------------------\nYour code has been rated at 5.00/10 (previous run: 4.00/10, +1.00)\nThat is an improvement in score (of +1.00) but we now have another error telling us that E1136: Value 'Union' is unsubscriptable (unsubscriptable-object). You are unlikely to know what all the error codes mean, but there are a few handy on-line lists all PyLint codes or all PyLint messages and what they are telling you are worth consulting (The Little Book of Python Anti-Patterns is also useful). In this instance PyLint has returned a false-positive because Union can and should be subscripted here because it means the argument can be either a string (str) or a pathlib Path (Path). So how do we get around this complaint?\nYou can disable PyLint from complaining about specific error codes/messages on a per-file basis by adding a line that disables them. You can use either codes or messages (the bit in the brackets at the end of the line, in this case unsubscriptable-object) and it is advisable to use the message form as it is more informative to those who read your code subequently.\nIf we add the following line it prevents PyLint from reporting the specific error‚Ä¶\nimport numpy as np\n\n# pylint: disable=unsubscriptable-object\n\ndef save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -> None:\n‚Ä¶running PyLint against our code again we get a much better score.\n ‚ù± pylint save_random_numbers_tidy.py\n\n-------------------------------------------------------------------\nYour code has been rated at 10.00/10 (previous run: 5.00/10, +5.00)\n\n\n\nConfiguring PyLint\nThe last error we encountered is something that is likely to crop up again if you use Typehints liberally throughout your Python code (and I would encourage you to do so). Rather than having to remember to disable the error in each file/module we create we can configure PyLint via its configuration file ~/.pylintrc to always ignore this error. To do so add the following‚Ä¶\n[MESSAGES CONTROL]\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).\ndisable=unsubscriptable-object\nFor more on configuriong PyLint refer to the documentation and also details of how to integrate with your editor and IDE\n\n\nAutomated Formatting with Black\nBlack is The Uncompromising Code Formatter and is very strict about the way in which it formats code. This could be a good or bad thing depending on your point of view, but it does result in highly consistent code when applied to all files. It formats files in place, so be mindful of this if you run it against one of your files it will change it.\nInstall black in your virtual environment and make a backup of your save_random_number.py file that you have just tidied up with linting.\npip install black\ncp save_random_numbers.py tidy_save_random_numbers.py\nTo run black against your code pass it the input file, it will re-write it and you can then compare it against the backup you just made‚Ä¶\nblack save_random_numbers.py\n‚ù± diff save_random_numbers.py tidy_save_random_numbers.py\n5,8c5\n<\n< def save_random_numbers(\n  <     size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\"\n  < ) -> None:\n---\n> def save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -> None:\n27c24\n<     with Path(save_as).open(\"w\") as out:\n---\n>     with Path(save_as).open('w') as out:\nIn this instance Black hasn‚Äôt changed much but it has reformatted the def save~randomnumbers~(...) line and moved the with Path() line as a consequence."
  },
  {
    "objectID": "posts/linting/index.html#when-to-lint",
    "href": "posts/linting/index.html#when-to-lint",
    "title": "Linting - What is all the fluff about?",
    "section": "When to Lint",
    "text": "When to Lint\nIt is worth linting your code from the outset of a project as not only does it result in a consistent style across your code base it also avoids the problem that can arise when applying linting retrospectively. If an existing code base has linting applied then the git blame, which indicates who the last person to edit a section was, then resides with the person who applied the linting, rather than the original author of the code. Its possible though that the person who applied the linting knows very little about the underlying functionality of the code but they may receive questions about it if they are indicated as the last person to have modified particular lines.\nFortunately there are a number of ways to automate and integrate linting into your workflow."
  },
  {
    "objectID": "posts/linting/index.html#automating-linting",
    "href": "posts/linting/index.html#automating-linting",
    "title": "Linting - What is all the fluff about?",
    "section": "Automating Linting",
    "text": "Automating Linting\n\nIDE Integration\nWhen programming it is really useful to use an Integrated Development Environment (IDE) as most allow the integration of linting tools and apply them to your code automatically, whether its using PyLint, YAPF, Black or otherwise. Setup and configuration is beyond the scope of this article but some links are provided to useful resources to get you started.\n\n\nVSCode\nVSCode supports linting in most languages, and both Python and R are supported along with other languages.\n\n\nPyCharm\nPyCharm supports automated formatting of code, for more information please refer to Reformat and rearrange code | PyCharm.\n\n\nEmacs\nThere are various options available for linting within Emacs, which you use depends on your preferences but LSP mode integrates with YAPF (via yapfify), Flake8 (via flycheck) and Black (via blacken)."
  },
  {
    "objectID": "posts/linting/index.html#git-integration",
    "href": "posts/linting/index.html#git-integration",
    "title": "Linting - What is all the fluff about?",
    "section": "Git Integration",
    "text": "Git Integration\nIf you are using an IDE then if configured correctly your code should be linted automatically for you, but an additional step that can capture anything that hasn‚Äôt been correctly formatted is to use a git hook to run linting on your code prior to making commits. There is git-pylint-commit-hook available on PyPi which runs automatically when you make commits to .py files."
  },
  {
    "objectID": "posts/linting/index.html#continuous-integration",
    "href": "posts/linting/index.html#continuous-integration",
    "title": "Linting - What is all the fluff about?",
    "section": "Continuous Integration",
    "text": "Continuous Integration\nIncluding a linting stage in your Continuous Integration (CI) pipeline pays dividends as we all make mistakes and sometimes forget to lint our code before making pushes."
  },
  {
    "objectID": "posts/linting/index.html#megalinter",
    "href": "posts/linting/index.html#megalinter",
    "title": "Linting - What is all the fluff about?",
    "section": "Megalinter",
    "text": "Megalinter\nPerhaps not necessary for everyone but worth mentioning the beast that is MegaLinter which will lint code across multiple languages and integrates easily into your pipeline (GitHub Action, CI on GitLab, Jenkins etc.). A useful article on doing so is Limit your technical debt and secure your code base using MegaLinter."
  },
  {
    "objectID": "posts/linting/index.html#pre-commit",
    "href": "posts/linting/index.html#pre-commit",
    "title": "Linting - What is all the fluff about?",
    "section": "Pre-commit",
    "text": "Pre-commit\nPre-commit is a Python package that adds a set of configurable hooks for linting your code, and not just Python, using a Git pre-commit hook. Hooks are run conditional on certain changes in states, in this case code that is run before commits are made. It creates a virtual Python Environment and installs the required packages there to lint your code. More will be written on this in a subsequent post."
  },
  {
    "objectID": "posts/linting/index.html#links",
    "href": "posts/linting/index.html#links",
    "title": "Linting - What is all the fluff about?",
    "section": "Links",
    "text": "Links\n\nPython\n\nFlake8 - Your Tool For Style Guide Enforcement\nBlack - The Uncompromising Code Formatter\nLinting Python in Visual Studio Code\nPylint - Overview of all Pylint messages\n\n\n\nR\n\nGitHub - r-lib/lintr: Static Code Analysis for R\nIntroduction to R: Linting R (and R Markdown)\n\n\n\nC++\n\ncpplint"
  },
  {
    "objectID": "posts/pre-commit/index.html",
    "href": "posts/pre-commit/index.html",
    "title": "Pre-Commit : Protecting your future self",
    "section": "",
    "text": "Pre-commit is a powerful tool for executing a range of hooks prior to making commits to your Git history. This is useful because it means you can automatically run a range of linting tools on your code across an array of languages to ensure your code is up-to-scratch before you make the commit."
  },
  {
    "objectID": "posts/pre-commit/index.html#background",
    "href": "posts/pre-commit/index.html#background",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Background",
    "text": "Background\nPre-commit is written in Python but that isn‚Äôt a limitation as it will lint YAML, JSON, C, JavaScript, Go, Rust, TOML, Terraform, Jupyter Notebooks, and so on. The list of supported hooks is vast.\nFor those unfamiliar with version control and Git in particular this will likely all sound alien. If you are new to the world of version control and Git I can highly recommend the Git & Github through GitKraken Client - From Zero to Hero! course offered by the Research Software Engineering at the University of Sheffield and developed by Alumni Anna Krystalli.\n\nWhat is a ‚Äúhook‚Äù?\nIn computing a ‚Äúhook‚Äù refers to something that is run prior to or in response to a requested action. In the context of the current discussion we are talking about hooks that relate to actions undertaken in Git version control and specifically actions that are run before a ‚Äúcommit‚Äù is made.\nWhen you have initialised a directory to be under Git version control the settings and configuration are stored in the .git/ sub-directory. There is the .git/config file for the repositories configuration but also the .git/hooks/ directory that is populated with a host of *.sample files with various different names that give you an in-road into what different hooks you might want to run. Its worth spending a little time reading through these if you haven‚Äôt done so yet as they provide useful examples of how various hooks work.\n\n\nWhy pre-commit hooks?\nTypically when writing code you should lint your code to ensure it conforms to agreed style guides and remove any ‚Äúcode smells‚Äù that may be lingering (code that violates design principles). It won‚Äôt guarantee that your code is perfect but its a good starting point to improving it. People who write a lot of code have good habits of doing these checks manually prior to making commits. Experienced coders will have configured their Integrated Development Environment (IDE) to apply many such ‚Äúhooks‚Äù on saving a file they have been working on.\nAt regular points in your workflow you save your work and check it into Git by making a commit and that is where pre-commit comes in to play because it will run all the hooks it has been configured to run against the files you are including in your commit. If any of the hooks fail then your commit is not made. In some cases pre-commit will automatically correct the errors (e.g.¬†removing trailing white-space; applying black formatting if configured) but in others you have to correct them yourself before a commit can be successfully made.\nInitially this can be jarring, but it saves you, and more importantly those who you are asking to review your code, time and effort. Your code meets the required style and is a little bit cleaner before being sent out for review. Long term linting your code is beneficial (see Linting - What is all the fluff about?)."
  },
  {
    "objectID": "posts/pre-commit/index.html#installation",
    "href": "posts/pre-commit/index.html#installation",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Installation",
    "text": "Installation\nPre-commit is written in Python and so you will need Python installed on your system in order to use it. Aside from that there is little else extra that is required to be manually installed as pre-commit installs virtual environments specific for each enabled hook.\nMost systems provide pre-commit in their package management system but typically you should install pre-commit within your virtual environment or under your user account.\npip install pre-commit\nconda install -c conda-forge pre-commit\nIf you are working on a Python project then you should include pre-commit as a requirement (either in requirements-dev.txt) or under the dev section of [options.extras_require] in your setup.cfg as shown below.\n[options.extras_require]\ndev =\n  pre-commit\n  pytest\n  pytest-cov"
  },
  {
    "objectID": "posts/pre-commit/index.html#configuration",
    "href": "posts/pre-commit/index.html#configuration",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Configuration",
    "text": "Configuration\nConfiguration of pre-commit is via a file in the root of your Git version controlled directory called .pre-commit-config.yaml. This file should be included in your Git repository, you can create a blank file or pre-commit can generate a sample configuration for you.\n# Empty configuration\ntouch .pre-commit-config.yaml\n# Auto-generate basic configuration\npre-commit sample-config > .pre-commit-config.yaml\ngit add .pre-commit-config.yaml\n\nHooks\nEach hook is associated with a repository (repo) and a version (rev) within it. Many are available from the https://github.com/pre-commit/pre-commit-hooks. The default set of pre-commit hooks might look like the following.\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n      rev: v4.3.0 # Use the ref you want to point at\n      hooks:\n          - id: trailing-whitespace\n            types: [file, text]\n          - id: check-docstring-first\n          - id: check-case-conflict\n          - id: end-of-file-fixer\n            types: [python]\n          - id: requirements-txt-fixer\n          - id: check-yaml\n\n\nHooks from External Repositories\nSome hooks are available from dedicated repositories, for example the following runs Black, Flake8 and Pylint on your code and should follow under the above (with the same level of indenting to be valid YAML).\n  - repo: https://github.com/psf/black\n    rev: 22.6.0\n    hooks:\n        - id: black\n          types: [python]\n\n  - repo: https://gitlab.com/pycqa/flake8.git\n    rev: 3.9.2\n    hooks:\n        - id: flake8\n          additional_dependencies: [flake8-print]\n          types: [python]\n  - repo: https://github.com/pycqa/pylint\n    rev: v2.15.3\n    hooks:\n        - id: pylint\nAn extensive list of supported hooks is available. It lists the repository from which the hook is derived along with its name.\n\n\nLocal Hooks\nYou can also define new hook and configure them under the - repo: local.\n  - repo: local\n    hooks:\n      - id: <id>\n        name: <descriptive name>\n        language: python\n        entry:\n        types: [python]\nFor some examples of locally defined hooks see the Pandas .pre-commit-config.yaml."
  },
  {
    "objectID": "posts/pre-commit/index.html#usage",
    "href": "posts/pre-commit/index.html#usage",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Usage",
    "text": "Usage\nBefore pre-commit will run you need to install it within your repository. This puts the file .git/hooks/pre-commit in place that contains the hooks you have configured to run. To install this you should have your .pre-commit-config.yaml in place and then run the following.\npre-commit install\nOnce installed and configured there really isn‚Äôt much to be said for using pre-commit, just make commits and before you can make a successful commit pre-commit must run with all the hooks you have configured passing. By default pre-commit only runs on files that are staged and ready to be committed, if you have unstaged files these will be stashed prior to running the pre-commit hook and restored afterwards. Should you wish to run these manually without making a commit then, after activating a virtual environment if you are using one, simply make a git commit or you can run.\npre-commit run\nIf any of the configured hooks fail then the commit will not be made. Some hooks such as black may reformat files in place and you can then make another commit recording those changes and the hook should pass. Its important to pay close attention to the output.\nIf you want to run a specific hook you simply add the <id> after run.\npre-commit run <id>\nOr if you want to force running against all files (except unstaged ones) you can do so.\npre-commit run --all-files # Across all files/hooks\nAnd these two options can be combined to run a specific hook against all files.\npre-commit run <id> --all-files\nYou may find that you wish to switch branches to work on another feature or fix a bug but that your current work doesn‚Äôt pass the pre-commit and you don‚Äôt wish to sort that out immediately. The solution to this is to use git stash to temporarily save your current uncommitted work and restore the working directory and index to its previous state. You are then free to switch branches and work on another feature or fix a bug, commit and push those changes and then switch back.\nImagine you are working on branch a but are asked to fix a bug on branch b. You go to commit your work but find that a does not pass pre-commit but you wish to work on b anyway. Starting on branch a you stash your changes, switch branches, make and commit your changes to branch b then switch back to a and unstash your work there.\ngit stash\ngit checkout b\n... # Work on branch b\ngit add <changed_files_on_branch_b>\ngit commit -m \"Fixing bug on branch b\"\ngit push\ngit checkout a\ngit stash apply"
  },
  {
    "objectID": "posts/pre-commit/index.html#updating",
    "href": "posts/pre-commit/index.html#updating",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Updating",
    "text": "Updating\nYou can update hooks locally by running pre-commit autoupdate. This will update your .pre-commit-config.yaml with the latest version of repositories you have configured and these will run both locally and if you use CI/CD as described below. However this will not update any packages that are part of the - repo: local that you may have implemented and it is your responsibility to handle these."
  },
  {
    "objectID": "posts/pre-commit/index.html#pre-commit-cicd",
    "href": "posts/pre-commit/index.html#pre-commit-cicd",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Pre-commit CI/CD",
    "text": "Pre-commit CI/CD\nIdeally contributors will have setup their system to work with pre-commit and be running such checks prior to making pushes. It is however useful to enable running pre-commit as part of your Continuous Integration/Development pipeline (CI/CD). This can be done with both GitLab and GitHub although similar methods are available for many continuous integration systems.\n\nGitHub\nGitHub actions reside in the .github/workflows/ directory of your project. A simple pre-commit action is available on the Marketplace at pre-commit/action. Copy this template to .github/workflows/pre-commit.yml and include it in your Git repository.\ngit add .github/workflows/pre-commit.yml\ngit commit -m \"Adding pre-commit GitHub Action\" && git push\n\n\nGitLab\nIf you use GitLab the following article describes how to configure a CI job to run as part of your repository.\n\nHow to use pre-commit to automatically correct commits and merge requests with GitLab CI"
  },
  {
    "objectID": "posts/pre-commit/index.html#links",
    "href": "posts/pre-commit/index.html#links",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Links",
    "text": "Links\n\nPre-commit\nSupported hooks\nGitHub Action\nGitLab CI"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Neil and work as a Research Software Engineer at the University of Sheffield. My education started with BSc in Zoology and Genetics followed by a MSc in Genetic Epidemiology. My academic career started with just over a decade as a Genetics Statistician where I learnt GNU/Linux system administration and reproducible research practices before shifting to Medical Statistics and working on Clinical Trials for a similar amount of time and taught myself R before a stint as a Data Scientist in industry company where I not only learnt Python but about good software development practices.\nHere you‚Äôll find posts about Research Software Engineering, Git/GitHub/GitLab, GNU/Linux (Gentoo, Arch, OpenWRT), Python, Bash, R, Emacs, Org-mode, Statistics, Genetics, Evolution and more.\nWhen not working I enjoy climbing, running, cycling, cooking, hiking, photography, gardening and spending time with my family."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "I‚Äôve a few other sites‚Ä¶\n\n\n\nSite\nDescription\n\n\n\n\nkimura\nA Dokuwiki site where I keep notes.\n\n\nFlickr\nPhotography (mostly landscape, climbing and cats).\n\n\nneil-snaps.co.uk\nWhere I fail to monetise my photography.\n\n\nSheffieldBoulder.uk\nAnother Dokuwiki site detailing artificial boulders around Sheffield.\n\n\nGitLab\nGit repos.\n\n\nGitHub (Personal)\nGit repos.\n\n\nGitHub (Work)\nGit repos."
  }
]