[
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "I’ve a few other sites…\n\n\n\nSite\nDescription\n\n\n\n\nkimura\nA Dokuwiki site where I keep notes.\n\n\nFlickr\nPhotography (mostly landscape, climbing and cats).\n\n\nneil-snaps.co.uk\nWhere I fail to monetise my photography.\n\n\nSheffieldBoulder.uk\nAnother Dokuwiki site detailing artificial boulders around Sheffield.\n\n\nSoftware Design Patterns\nA “sub-blog” on Software Design Patterns.\n\n\nGitLab\nGit repos.\n\n\nGitHub (Personal)\nGit repos.\n\n\nGitHub (Work)\nGit repos.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGitLab CI - Automatic Publishing to PyPI\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-commit and R Packaging\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Useful Hooks\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nSphinx Documentation\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPython Packaging\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nBrowser Extensions\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit.ci : Integrating Pre-Commit into CI/CD\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nRunning in 2022\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nWho’s to Blame\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Customising and Updating\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nLinux Command Line Alternatives\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Protecting your future self\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nGit : Custom SSH credentials for git repositories\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nLinting - What is all the fluff about?\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ns-rse",
    "section": "",
    "text": "GitLab CI - Automatic Publishing to PyPI\n\n\n\n\n\n\n\npython\n\n\ndocumentation\n\n\npackaging\n\n\ngitlab\n\n\nci\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-commit and R Packaging\n\n\n\n\n\n\n\nquarto\n\n\nR\n\n\ngit\n\n\npre-commit\n\n\ngithub actions\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Useful Hooks\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\npre-commit\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nSphinx Documentation\n\n\n\n\n\n\n\nquarto\n\n\npython\n\n\ndocumentation\n\n\nsphinx\n\n\ngithub actions\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPython Packaging\n\n\n\n\n\n\n\nquarto\n\n\npython\n\n\npackaging\n\n\nsetuptools\n\n\ngithub actions\n\n\npypi\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nBrowser Extensions\n\n\n\n\n\n\n\nbrowser\n\n\nfirefox\n\n\nopera\n\n\nvivaldi\n\n\nextensions\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit.ci : Integrating Pre-Commit into CI/CD\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\npre-commit\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nRunning in 2022\n\n\n\n\n\n\n\nquarto\n\n\nrunning\n\n\nemacs\n\n\nliterate programming\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nWho’s to Blame\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\nblame\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Customising and Updating\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\npre-commit\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nLinux Command Line Alternatives\n\n\n\n\n\n\n\ncode\n\n\nlinux\n\n\nbash\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Protecting your future self\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nlinting\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\npre-commit\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nGit : Custom SSH credentials for git repositories\n\n\n\n\n\n\n\nssh\n\n\ngit\n\n\ngithub\n\n\ngitlab\n\n\nkeychain\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nLinting - What is all the fluff about?\n\n\n\n\n\n\n\ncode\n\n\nlinting\n\n\npython\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/browser-extensions/index.html",
    "href": "posts/browser-extensions/index.html",
    "title": "Browser Extensions",
    "section": "",
    "text": "Most people use web-browsers a fair bit. There are a number of extensions available which make their use more stream-lined and efficient. This post covers those that I use. If there is something that you use regularly and find useful I’d love to hear about it (see links at top of page)."
  },
  {
    "objectID": "posts/browser-extensions/index.html#browser-choice",
    "href": "posts/browser-extensions/index.html#browser-choice",
    "title": "Browser Extensions",
    "section": "Browser Choice",
    "text": "Browser Choice\nI deliberately eschew Chrome and even its open-source relative Chromium because of the tracking built into the system. Instead I use Firefox as my main browser and for work I use Opera. I’ve also dabbled with Vivaldi which I quite like and should use more.\nMost of the plugins discussed here work across browsers, although for Opera and Vivaldi it is often the case of installing the Chrome extensions which work because of the development/toolkit on which the browser is based."
  },
  {
    "objectID": "posts/browser-extensions/index.html#pluginsextensions",
    "href": "posts/browser-extensions/index.html#pluginsextensions",
    "title": "Browser Extensions",
    "section": "Plugins/Extensions",
    "text": "Plugins/Extensions\nPlugins/extensions add additional functionality to your browser. I use many for blocking adverts and trackers and I list those below and find they make browsing cleaner and faster (I also block many such sites via my router but that is a separate post). But protecting your privacy is not the only purpose of extensions, many can streamline your browser usage and workflow and that is the main focus of this article.\nFirefox Extensions are available at Firefox Extensions, as mentioned those for Opera and Vivaldi are typically installed via the Chrome Web Store.\n\nBibItNow\nThis is invaluable if you undertake any academic work and use citations. Once installed it adds a short-cut for generating BibTex and other citation formats from the page being visited. This can then be copy and pasted into your database with a few keystrokes.\nIt is possible to customise the fields that are included e.g. by default Abtract is not included, but its something I like to include in my citation database for a quick overview of what a paper is about.\n\n\nCopy URL to Clipboard\nLinks are the blood of the internet and when reading and taking notes I like to link to the source I am using. This plugin makes it a doddle and will create a link to the page that is being viewed (or highlighted text) to a range of formats including Markdown, Org-mode, LaTeX, reStructuredText.\nBinding each link type to specific keys means its incredibly easy to copy and paste links from browser to Emacs (where I do most of my writing).\n\n\nUnpaywall\nMany papers are, unfairly given the research that paid for them is often from the public purse, behind PayWalls. Thankfully with the rise of pre-print servers such as arXiv, biorXiv, F1000 and the forthcoming Octopus pre-prints and alternatives to pay-walled articles are available and Unpaywall is a plugin that automatically finds them for you. Install it and when you visit an articles page if its behind a paywall but available freely elsewhere a green-symbol with an unlocked padlock appears on the right-hand side of your browser. If its not available this is a locked padlock on a grey background.\n\n\nBrowserPass\nEveryone should use a Password Manager of some description, I use Pass: The Standard Unix Password Manager and to get it to work seamlessly with my browsers I use BrowserPass. It requires a little configuration so read the GitHub page carefully but once working it is seamless. I visit a web-site and because I organise my passwords to include the URL all I need to do is use Ctrl-Shift-f and if my GPG key is unlocked the password is entered for me. My GPG key is unlocked using my Yubikey so if this isn’t plugged in and unlocked I’m prompted to do so. Makes logging in to web-sites so much faster.\n\n\norg-capture\nProbably only useful if you use the amazing “Capture” web-site, title and selected text to Emacs in Org-mode via org-protocol.\n\n\nGitLab Notify\nGet notifications from GitLab in your browser.\n\n\nOctotree\nNot used this much as I only discovered it recently whilst working on this article but it improves navigation of GitHub repositories. The main, free, feature that is of most use is a sidebar to aid navigation of a repositories code. This can be pinned if required."
  },
  {
    "objectID": "posts/browser-extensions/index.html#keyboard-shortcuts",
    "href": "posts/browser-extensions/index.html#keyboard-shortcuts",
    "title": "Browser Extensions",
    "section": "Keyboard Shortcuts",
    "text": "Keyboard Shortcuts\nBinding actions you take with extensions to keyboard short-cuts can save a considerable amount of time, particularly if you are a heavy typist, as it saves the small amount of time taken to move the hand to the mouse, locate the pointer and move it to where it needs to be.\nNot every extension has shortcuts associated with it, but for those that do in Firefox you can configure this by going to Add-ons and themes and at the top-right of the page listing the installed extensions, adjacent to Manage Your Extensions is a cog. Left-click on this once and a menu appears and at the bottom you can select Manage Extension Shortcuts. This allows you to bind “key-chords” (combinations of keys) to each plugins action. If there are conflicts (i.e. the same key-binding is bound to two actions) then these are highlighted and can be corrected."
  },
  {
    "objectID": "posts/browser-extensions/index.html#summary",
    "href": "posts/browser-extensions/index.html#summary",
    "title": "Browser Extensions",
    "section": "Summary",
    "text": "Summary\nThese are but a few of the vast array of productivity extensions you can make use of. Which you find useful and would use will be dependent on your browser usage and work\n\nProductivity Extensions\n\n\n\nExtension\nDescription\nShortcut\n\n\n\n\nBibItNow!\nCreates BibTex and other citation formats from web-pages. Really useful when browsing for journal articles, books and other sites that you want to add to your citation database.\nAlt-C\n\n\nCopy URL to Clipboard\nCopy the URL of a page along with its title (or selected text) to any number of different link formats.\nAlt-m (Markdown); Alt-o (Org-mode).\n\n\nUnpaywall\nAutomatically provides links to free versions of pay-walled journal articles and books.\nNot Required\n\n\nBrowserPass\nAuto-fill website login details stored in your Pass: The Standard Unix Password Manager.\nCtrl+Shift-F\n\n\norg-capture\n“Capture” web-site, title and selected text to Emacs via org-protocol.\nCtrl-Shift-L\n\n\nGitLab Notify\nGet notifications from GitLab in your browser.\nNot Available\n\n\nSci-Hub Now!\nAccess papers on Sci-Hub.\nNot Available\n\n\n\n\n\nPrivacy Protecting Extensions\nI’ve not gone into detail about the privacy protecting extensions I use but have listed them below. There is overlap/redundancy in what I’m using but that’s not necessarily a bad thing. I do find it breaks some sites (e.g. Amazon) but that isn’t necessarily a bad thing as it encourages me to shop more ethically and I can always switch browsers if needs be.\n\n\n\nExtension\nDescription\n\n\n\n\nClearURLs\nAutomatically remove tracking elements from URLs to help protect your privacy.\n\n\nCookieBlock\nAutomating Cookie Consent and GDPR Violation Detection.\n\n\nDecentralEyes\nProtects you against tracking through “free”, centralized, content delivery. It prevents a lot of requests from reaching networks like Google Hosted Libraries, and serves local files to keep sites from breaking. Complements regular content blockers.\n\n\nDuckDuckGo Privacy Essentials\nTracker blocking, cookie protection, DuckDuckGo private search, email protection, HTTPS upgrading and more.\n\n\nHTTPS Everywhere\nFrom the Electronic Frontier Foundation, retired 2023 as most browser can be set up to use HTTPS by Default\n\n\nI don’t care about cookies 3.4.6\nGet rid of cookie warnings from almost all websites.\n\n\nTemporary Containers\nEnhance your privacy in Firefox with Temporary COntainers\n\n\nUTM Remover\nRemove Google Analytics UTM tracking parameters fromURLS for privacy.\n\n\nUntrackMe\nRemoves parts of URLs that track you (also worth enabling Do Not Track).\n\n\nuBlock Origin\nBlock adverts"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html",
    "href": "posts/pre-commit-ci/index.html",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "",
    "text": "NB If you’ve not read it already I would recommend reading my previous post on using pre-commit as the contents described herein assume that you are already using pre-commit in your development.\nHaving pre-commit setup locally to run before making commits is great. Typically code lives in a “forge” such as GitHub or GitLab and as pre-commit is run on each commit you shouldn’t have any problems when you come to git push your code to the remote origin repository (i.e. the repository hosted on GitHub/GitLab) as all pre-commit checks will have to have passed before this will take place.\nBut what if for some reason you disabled pre-commit just to make some changes rather than addressing the failed linting or test? Or if you work on an open-source project and someone else contributes how can you ensure that their contributed code meets the code-style chosen by the project and that all tests pass in light of the changes that are being introduced?"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#continuous-integration-continuous-delivery-cicd",
    "href": "posts/pre-commit-ci/index.html#continuous-integration-continuous-delivery-cicd",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "Continuous Integration / Continuous Delivery (CI/CD)",
    "text": "Continuous Integration / Continuous Delivery (CI/CD)\nThe solution to this is Continuous Integration/Continuous Delivery (CI/CD) which runs various hooks on GitHub/GitLab etc. in response to specific tasks/actions that occur on the repository. The exact name or system used depends on the forge, on GitHub these are GitHub Actions (see also Actions Marketplace) whilst on GitLab uses Pipelines. There are even standalone systems which integrate with both such as the popular Jenkins.\nBy employing pre-commit as part of your CI/CD pipeline you ensure code meets the standards (linting, tests etc.) you wish contributions to meet before it is merged into your main/master branch`\nThese work by running processes under certain conditions, for example on a push to the main branch or a tag that begins with v, and they might run processes such as running the test suite for your project to ensure all tests pass, build web-pages or build the package for deployment to a repository (e.g. PyPI). They are really useful and flexible systems and can be leveraged to run pre-commit on your code when Pull Requests (PR) are made to ensure the PR passes the various hooks. Ultimately a PR results in a commit to master/main and so its logically consistent that Pull Requests should pass pre-commit prior to being merged.\nUnder any system you could write your own hook to run pre-commit but there is an even easier and more efficient solution if you use GitHub in the form of pre-commit.ci."
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#github-and-pre-commit.ci",
    "href": "posts/pre-commit-ci/index.html#github-and-pre-commit.ci",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "GitHub and pre-commit.ci",
    "text": "GitHub and pre-commit.ci\nCurrently pre-commit.ci only supports GitHub although support of other systems is in the pipeline. pre-commit.ci doesn’t need any configuration beyond your already existing .pre-commit-config.yaml (see Pre-commit : Protecting Your Future Self). Where a pre-commit hook corrects formatting issues as is the case with some of the defaults such as trailing-whitespace or check-yaml, or if you are using Python linters such as black or ruff which fix errors, pre-commit.ci can commit these changes and push them back to the Pull Request automatically. In a similar vein it will also routinely update the rev used in your .pre-commit-config.yaml, commit the change and push it back to your repository.\nIt is also really fast because pre-commit.ci keeps the virtual environments that are used in tests cached whereas if you wrote your own action to run this the GitHub runner that is spun up to run GitHub Actions would have to download all of these each time the action is run is they are not persistent.\nUse of pre-commit.ci is free for open-source repositories and there are paid options for private or organisation repositories.\n\nBenefits of pre-commit.ci\n\nSupports GitHub but more to come in the future.\nZero configuration, just need .pre-commit-config.yaml.\nCorrects & commits formatting issues automatically without need for developer to reformat.\nAutomatically updates .pre-commit-config.yaml for you (e.g. new rev).\nFaster than your own GitHub Action.\nFree for open source repositories (paid for version for private/organisation repositories).\n\n\n\nConfiguration (.pre-commit-config.yaml)\nWhilst not required it is possible to configure the behaviour of pre-commit.ci by adding a ci: section to your .pre-commit-config.yaml. The fields are fairly self-explanatory as the example below shows. Its possible to toggle whether to autofix_prs and to set the autofix_commit_msg. The autoupdate_schedule can be set to weekly, monthly or quarterly along with a custom autoupdate_commit_msg. Finally you can optionally disable some hooks from being run only in pre-commit.ci.\nci:\n  autofix_prs: true\n  autofix_commit_msg: '[pre-commit.ci] Fixing issues with pre-commit'\n  autoupdate_schedule: weekly\n  autoupdate_commit_msg: '[pre-commit.ci] pre-commit automatically updated revs.'\n  skip: [pylint] # Optionally list ids of hooks to skip on CI\n\n\nSetup\nSetup is relatively straight-forward, head to https://pre-commit.ci and sign-in with your GitHub account and grant pre-commit.ci access to your account.\n\n\n\nPre-commit CI\n\n\nOnce you have granted access you can choose which repositories pre-commit.ci has access to. It is possible to grant access to all repositories but I would recommend doing so on a per-repository basis so you know and are in control of what is happening across your repositories. If you have administration rights to organisation repositories these should be listed in the “Select repositories” pull-down menu.\n\n\n\nGranting pre-commit.ci access to GitHub\n\n\n\n\npre-commit.ci jobs\nWhen logged into pre-commit.ci using your GitHub account you are presented with a page similar to the following which lists the accounts and any organisations that you have authorised pre-commit.ci to access.\n\n\n\nPre-commit.ci account access\n\n\nYou can follow the links through to view the history of jobs run by pre-commit.ci and whether they pass or fail. The page shows the current status and provides both Markdown and reStructured Text code for adding badges to your source documents (e.g. the Markdown badge can be added to your repositories top-level README.md and the badge will be displayed on GitHub)\n\n\n\nPre-commit.ci jobs pass\n\n\nYou can click through and see the results of a given run and when they pass they look similar to the output you would have seen when making commits locally.\n\n\n\nPre-commit.ci jobs pass\n\n\nBut sometimes things will fail as shown below where the trailing-whitespace hook failed and the file was modified. But since pre-commit.ci corrects and pushes such changes automatically you can see at the bottom that these changes were pushed to the Pull Request from which the originated.\n\n\n\nPre-commit.ci jobs fail"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#gitlab",
    "href": "posts/pre-commit-ci/index.html#gitlab",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "GitLab",
    "text": "GitLab\nAs pre-commit.ci doesn’t (yet) support GitLab integrating pre-commit into your GitLab Pipeline is a little more involved. What follows is based on the excellent post on StackOverflow describing how to achieve this integration.\nYou should already have a valid .pre-commit-config.yaml in place (if not work through Pre-commit : Protecting your future self (blog-post)). To enable pre-commit on your GitLab Pipeline you need to to have a pipeline in place. This is a file in the root of your repository called .gitlab-ci.yml. You need to add the following to this file…\nvariables:\n  # since we're not using merge request pipelines in this example,\n  # we will configure the pre-commit job to run on branch pipelines only.\n  # If you ARE using merge request pipelines, you can omit this section\n  PRE_COMMIT_DEDUPLICATE_MR_AND_BRANCH: false\n  PRE_COMMIT_AUTO_FIX_BRANCH_ONLY: true\n\ninclude:\n  - remote: 'https://gitlab.com/yesolutions/gitlab-ci-templates/raw/main/templates/pre-commit-autofix.yaml'\nThis uses the pre-commit-autofix.yaml from yesolutions to run pre-commit and as the configuration shows automatically apply fixes pre-commit makes to your code. There are more options available for configuring this pipeline and they are documented here.\nBecause you are allowing a third-party pipeline to access your repository when pushing the changes pre-commit makes back to your repository for this to work you must create a project access token. Under the repositories Settings > Access Tokens you can create a new token with an expiry date. You must then create a CI/CD variable called PRE_COMMIT_ACCESS_TOKEN with this token as a value.\nOnce you have done this your CI/CD pipeline should show at the very start the .pre stage…\n\n\n\nGitLab pre-commit pipeline.\n\n\n…and you can click through on this to see the details of the pipeline. Note that it takes a while to run as it has to download and intialise all of the environments for each configured hook unlike pre-commit.ci (this is akin to writing your own GitHub Action to run pre-commit which would also have to download and initialise the environments).\n\n\n\nSuccess! GitLab pre-commit hooks pass!"
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#summary",
    "href": "posts/pre-commit-ci/index.html#summary",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "Summary",
    "text": "Summary\nThis article has covered\n\nWhy to integrate pre-commit into your Continuous Integration/Delivery pipeline.\nWhat the pre-commit.ci service is and the benefits it provides.\nHow to integrate pre-commit.ci with GitHub repositories.\nHow to integrate pre-commit with GitLab repositories.\n\nBy automating linting and testing in this manner you improve and shorten the feedback loop for developers and contributors which frees up more time and focus on the code itself."
  },
  {
    "objectID": "posts/pre-commit-ci/index.html#links",
    "href": "posts/pre-commit-ci/index.html#links",
    "title": "Pre-Commit.ci : Integrating Pre-Commit into CI/CD",
    "section": "Links",
    "text": "Links\n\nPre-commit : Protecting your future self (blog-post)- pre-requisite reading if you are not already using pre-commit\nPre-commit : Protecting your future self (slides) - slides from a talk given at Research Computing at the University of Leeds that extended the above blog post to cover the material in this post (hit s to see the “speaker notes” 😉).\npre-commit\npre-commit.ci\nHow to use pre-commit to automatically correct commits and merge requests with GitLab CI - Stack Overflow"
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html",
    "href": "posts/pre-commit-hooks/index.html",
    "title": "Pre-Commit : Useful Hooks",
    "section": "",
    "text": "I’m a big fan of pre-commit and have written about it before (see posts on pre-commit, pre-commit CI and pre-commit updating). This post discusses some of the hooks that I use and how to configure them."
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#python-linting",
    "href": "posts/pre-commit-hooks/index.html#python-linting",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Python Linting",
    "text": "Python Linting\n\nRuff\nruff is a Python linter written in Rust which means its considerably faster than many native linters. It aims for parity with Flake8 and covers a lot of the linting that PyLint undertakes too. Its configured via pyproject.toml which makes incorporating it into your Python Package simple.\nrepos:\n  - repo: https://github.com/charliermarsh/ruff-pre-commit\n    rev: v0.0.191\n    hooks:\n      - id: ruff\nConfiguration is, as noted, via pyproject.toml and you may find the post on Python Packaging worth reading to understand more on this.\n[tool.ruff]\nexclude = []\n# per-file-ignores = []\nline-length = 120\ntarget-version = \"py310\"\n\n# Allow autofix for all enabled rules (when `--fix`) is provided.\nfixable = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"R\", \"S\", \"W\", \"U\"]\nunfixable = []\n\n\nBlack\nBlack is an opinionated formatter for Python that is PEP8 compliant. By using black to format your code you end up with a consistent style across the code base and commit changes end up being minimal. This helps speed up code-review of pull-requests.\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 22.10.0\n    hooks:\n      - id: black\n        types: [python]\nConfiguration is, as noted, via pyproject.toml and you may find the post on Python Packaging worth reading to understand more on this.\n[tool.black]\nline-length = 120\ntarget-version = [\"py38\", \"py39\", \"py310\"]\ninclude = \"\\\\.pyi?$\"\n\n\npydocstyle\nYou can check your docstrings are correctly written using the pydocstyle hook.\nIts pretty straight-forward to use and accepts arguments so you can pass all the command line options you might want to use into the hook when it runs. It supports three different doc string styles, pep257, numpy and google.\n    - repo  https://github.com/pycqa/pydocstyle\n        rev: 6.3.0  # pick a git hash / tag to point to\n        hooks:\n        - id: pydocstyle\n        args:\n        - --convention=numpy\n        # Optionally ignore rules\n        - --ignore=D101,D2\nAlternatively you can add configuration options to your projects pyproject.toml under a [tool.pydocstyle] section.\n[tool.pydocstyle]\nconvention = \"numpy\"\nignore = [\n  \"D101\",\n  \"D2\"\n]"
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#markdown-linting",
    "href": "posts/pre-commit-hooks/index.html#markdown-linting",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Markdown Linting",
    "text": "Markdown Linting\nmarkdownlint-cli2 is a useful and highly configurable hook for linting Markdown and CommonMark. I wanted to use it on this blog though which is written using Quarto and therefore uses PandocMarkdown with files that have extension .qmd. I therefore enable the hook in .pre-commit-config.yaml with a configuration file specified\nrepos:\n- repo: https://github.com/DavidAnson/markdownlint-cli2\n  rev: v0.6.0\n  hooks:\n    - id: markdownlint-cli2\n      args: [.markdownlin-cli2.yaml]\n..and add a sample configuration file (e.g. .mardownlint-cli2.yaml although other formats such as JSON can be used) is shown below and markdownlint-cli2 picks this up automatically.\n# Configuration\nconfig:\n  # MD013 - line-length\n  line_length:\n    line_length: 120\n    code_blocks: false\n    tables: false\n  html:\n    allowed_elements:\n      - div\n\n# Globs\nglobs:\n  - \"**/*.qmd\"\n  - \"*.qmd\"\n\n# Fix any fixable errors\nfix: false"
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#emacs-lisp",
    "href": "posts/pre-commit-hooks/index.html#emacs-lisp",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Emacs Lisp",
    "text": "Emacs Lisp\nAs I use Emacs I have recourse to write some Emacs Lisp and so its useful to applying formatting to my .el files before committing them. lisp-format does the job nicely.\nrepos:\n  - repo: https://github.com/eschulte/lisp-format\n    rev: 088c8f78ca41204b44f2636275517ac09a2de6a9\n    hooks:\n      - id: lisp-format\n        name: formatter of lisp code\n        description: Run lisp-format against lisp files\n        language: script\n        files: \\.(lisp|cl|asd|scm|el)$\n        entry: lisp-format -i"
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#conclusion",
    "href": "posts/pre-commit-hooks/index.html#conclusion",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Conclusion",
    "text": "Conclusion\nThere are a lot of hooks out there to be used with pre-commit and incorporated into your Continuous Integration pipeline with pre-commit.ci. Which you find useful will depend to a large extent on the languages that you are using for any given project. Here I’ve focused mainly on common tools for Python Packages, Markdown and Lisp but you can find hooks for Docker, Ansible, Rust, Go, JavaScript, C++ and many more, there is even gitlint which lints your commit messages! Checkout the long list of available hooks and try some out."
  },
  {
    "objectID": "posts/pre-commit-hooks/index.html#links",
    "href": "posts/pre-commit-hooks/index.html#links",
    "title": "Pre-Commit : Useful Hooks",
    "section": "Links",
    "text": "Links\n\nRelated pre-commit posts\n\npre-commit\npre-commit CI\npre-commit updating\n\n\n\nPre-commit hooks\n\npre-commit (pre-commit hooks)\npre-commit.ci\nruff\nmarkdownlint-cli2\nBlack\nlisp-format\ngitlint"
  },
  {
    "objectID": "posts/python-packaging/index.html",
    "href": "posts/python-packaging/index.html",
    "title": "Python Packaging",
    "section": "",
    "text": "This post describes steps in creating a Python package. If you are looking for information on installing packages this is done using Python PIP.\nPython packaging is in a constant state of flux. There is the official Python Packaging User Guide and the Python Packaging Authority (PyPA) which is probably the best resource to read but things change, and often quickly. The focus here is on the PyPA Setuptools using pyproject.toml which works with Python >= 3.7, but you may wish to consider other packages such as Poetry or PDM which offer some advantages but with additional frameworks to learn.\nA few examples of Python packages that I have packaged are listed below, most have also been released to PyPI."
  },
  {
    "objectID": "posts/python-packaging/index.html#package-structure",
    "href": "posts/python-packaging/index.html#package-structure",
    "title": "Python Packaging",
    "section": "Package Structure",
    "text": "Package Structure\nYou should place your code within a Git version controlled directory for your project. It is then normal to place all files in an organised hierarchy with a sub-directory of the same name for Python code, known as a \"flat\" structure and tests under tests directory. It is possible to have more than one directory containing code but for now I'm sticking to the flat structure.\n.\n    ├── ./build\n    ├── ./dist\n    ├── ./\n    ├── ./my_package\n    ├── ./my_package/__init__.py\n    ├── ./my_package/module_a.py\n    ├── ./my_package/module_b.py\n    ├── ./my_package/something/module_c.py\n    └── ./tests\n        ├── ./tests/conftest.py\n        ├── ./tests/resources\n        ├── ./tests/test_module_a.py\n        ├── ./tests/test_module_b.py\n        └── ./tests/something/test_module_c.py\n\n__init__.py\nIn older versions of Python (<3.3) a __init__.py was required in every directory and sub-directory that was to be a module/sub-module. In more recent versions of Python (>\\=3.3) they are not essential though as Python uses namespace packages. But in most cases its simpler to include such a file in the top level of your directory. __init__.py files can be completely empty or they can contain code that is used throughout your package, such as setting up a logger."
  },
  {
    "objectID": "posts/python-packaging/index.html#configuration-pyproject.toml",
    "href": "posts/python-packaging/index.html#configuration-pyproject.toml",
    "title": "Python Packaging",
    "section": "Configuration pyproject.toml",
    "text": "Configuration pyproject.toml\nPackage configuration has been and is in a state of flux, there was originally setup.py which was then complemented and gradually replaced by setup.cfg. The new method on the block though is pyproject.toml which, with a little tweaking and judicious choice of packages can handle everything.\nSetuptools is shifting towards using pyproject.toml and whilst it is still under development its already highly functional. It’s written in Tom's Obvious Minimal Language and isn't too dissimilar in structure to setup.cfg.\nA useful reference for writing your configuration in pyproject.toml is Configuring setuptools using pyproject.toml files. It is based around PEP 621 – Storing project metadata in pyproject.toml | peps.python.org.\nA bare-bones pyproject.toml file should reside in the top level of your directory with the following (NB This includes the minimum versions and setuptools_scm extension for dynamically setting package version)…\n\nbuild-system\n[build-system]\nrequires = [\"setuptools>=65.6.3\", \"setuptools_scm[tools]>=6.2\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\nTraditionally configuration of meta-data such as author, code repository and license was made via setup.py but you can either specify some (or most) of this in pyproject.toml or a concurrent setup.cfg.\n\n\nproject\nThis is the main body of the project description detailing name, authors, description, readme, license, keywords, classifiers, dependencies and version amongst other things.\nThe type of license you have chosen to apply to your package. For guidance see Choose an Open Source License.\nThe README of your package which may be in Markdown or Restructured Text.\nSets the components of your package which are set dynamically. In this example we only set the version dynamically using setuptools_scm.\nThe dependencies are those that are required for running the code. They should not include packages that are required for development (e.g. black. flake8, ruff, pre-comit, pylint etc.), nor those required for testing (e.g. pytest, pytest-regtest, pytest-cov etc.), documentation (e.g. Sphinx, numpydoc, sphinx_markdown_table, sphinx-autodoc-typehints, sphinxcontrib-mermaid etc.) as these are defined in a separate section.\n[project]\nname = \"my_package\"\nauthors = [\n  {name = \"Author 1\", email=\"author1@somewhere.com\"},\n  {name = \"Author 2\", email=\"author2@somewhere.com\"},\n  {name = \"Author 3\", email=\"author3@somewhere.com\"},\n]\ndescription = \"A package that does some magic!\"\nlicense = \"GNU GPLv3 only\"\nreadme = \"README.md\"\ndynamic = [\"version\"]\ndependencies = [\n  \"numpy\",\n  \"pandas\",\n  \"tqdm\",\n]\nAll other sections are considered subsections, either of project or tool and are defined under their own heading with [project|tool].<package>[.<options>].\n\nproject.urls\nThese are important as they define where people can find the Source, Documentation and Bug_Tracker amongst other things. There may be more fields that can be configured here but I've not used the yet. Substitute these to reflect where your package is hosted, your username and the package name.\n[project.urls]\nSource = \"https://gitlab.com/username/my_package\"\nBug_Tracker = \"https://gitlab.com/username/my_package/issues\"\nDocumentation = \"https://username.gitlab.com/my_package\"\n\n\nproject.optional-dependencies\nThis is where you list dependencies that are not required for running a package but are required for different aspects such as development, documentation, publishing to PyPI, additional Notebooks and so forth, the options are limitless.\n[project.optional-dependencies]\ndev = [\n  \"black\",\n  \"flake8\",\n  \"Flake8-pyproject\",\n  \"pre-commit\",\n  \"pylint\",\n  \"ruff\",\n]\ndocs = [\n  \"Sphinx\",\n  \"myst-parser\",\n  \"numpydoc\",\n  \"pydata_sphinx_theme\",\n  \"sphinx-autodoc-typehints\",\n  \"sphinx_markdown_tables\",\n  \"sphinxcontrib-mermaid\",\n]\npypi = [\n  \"build\",\n  \"pytest-runner\",\n  \"setuptools-lint\",\n  \"setuptools_scm\",\n  \"twine\",\n  \"wheel\"\n]\ntest = [\n \"pytest\",\n \"pytest-cov\",\n]\nnotebooks = [\n  \"ipython\",\n  \"ipywidgets\",\n  \"jupyter_contrib_nbextensions\",\n  \"jupyterthemes\",\n]\n\n\nproject.scripts (Entry Points)\nEntry points or scripts are a neat method of providing a simple command line interface to your package that links directly into a specific module to provide a command line interface to your programme.\nThese are defined under project.scripts section.\n[project.scripts]\ntcx2gpx = \"tcx2gpx:process\"\n\n\n\ntool\n\ntool.setuptools\nsetuptools is perhaps the most common package for configuring Python packages and is the one that is being exposed here. Its configuration is multi-level depending on which component you are configuring.\n\ntool.setuptools.packages.find\nUses the find utility to search for packages to include, based on my understanding it looks for __init__.py in a directory and includes it (see above note about these no longer being required in every directory). Typically you would want to exclude tests/ from a package you are making as most users won’t need to run the test suite (if they do they would clone from the source repository).\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"tcx2gpx\"]\nexclude = [\"tests\"]\n\n\n\ntool.setuptools.package-data\nThis allows additional, non .py files to be included, they are listed on a per package basis and are a table (in toml parlance, list in Python terms).\n  [tool.setuptools.packages-data]\n  tcx2gpx = [\"*.yaml\", \"*.json\"]\n\n\ntool.pytest\n[tool.pytest.ini_options]\nminversion = \"7.0\"\naddopts = \"--cov --mpl\"\ntestpaths = [\n    \"tests\",\n]\nfilterwarnings = [\n    \"ignore::DeprecationWarning\",\n    \"ignore::UserWarning\"\n]\n\n\ntool.black\n[tool.black]\nline-length = 120\ntarget-version = [\"py38\", \"py39\", \"py310\", \"py311\"]\nexclude = '''\n\n(\n  /(\n      \\.eggs         # exclude a few common directories in the\n    | \\.git          # root of the project\n    | \\.venv\n  )/\n)\n'''\n\n\ntool.flake8\nThe developers of Flake8 will not be supporting pyproject.toml for configuration. This is a shame but a work around is available in the form of Flake8-pyproject. Make sure to add this to your requirements section to ensure it is installed when people use pre-commit.\n[tool.flake8]\nignore = ['E231', 'E241']\nper-file-ignores = [\n    '__init__.py:F401',\n]\nmax-line-length = 120\ncount = true\n\n\ntool.setuptools_scm\nsetuptools_scm is a simple to use extension to setuptools that dynamically sets the package version based on the version control data. It is important to note that by default setuptools_scm will attempt to bump the version of the release. The following configuration forces the use of the current git tag.\n[tool.setuptools_scm]\nwrite_to = \"tcx2gpx/_version.py\"\nversion_scheme = \"post-release\"\nlocal_scheme = \"no-local-version\"\ngit_describe_command = \"git describe --tags\"\n\n\ntool.ruff\nruff is a Python linter written in Rust which is therefore very fast. It provides the same functionality as black, flake8 and pylint and can auto-correct many issues if configured to do so. A GitHub Actions is also available. I'd recommend checking it out.\n[tool.ruff]\nfixable = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"R\", \"S\", \"W\", \"U\"]\nunfixable = []"
  },
  {
    "objectID": "posts/python-packaging/index.html#versioning",
    "href": "posts/python-packaging/index.html#versioning",
    "title": "Python Packaging",
    "section": "Versioning",
    "text": "Versioning\nTypically the version is defined in the __version__ variable/object in the top-level __init__.py or as a value in [metadata] of either setup.cfg or pyproject.toml but this has some downsides in that you have to remember to update the string manually when you are ready for a release and it doesn't tie in with using tags in Git to tag versions of your commits.\nIt is worth taking a moment to read and understand about Semantic Versioning which you are likely to use when tagging versions of your software to work with setuptools_scm.\n\nSetuptools-scm\nsetuptools_scm is simpler to setup and use than versioneer as it relies solely on configuration via pyproject.toml rather than being dependent on now deprecated setup.py.\nAs shown above you should have set the minimum versions of \"setuptools>=45\" and \"setuptools_scm[toml]>=6.2\", dynamic = [\"version\"] under project and set the write_to = \"pkg/_version.py\" (NB substitute pkg for your package directory, whether its src or the package name).\n[build-system]\nrequires = [\"setuptools>=65.6.3\", \"setuptools_scm[toml]>=6.2\"]\n\n[project]\ndynamic = [\"version\"]\n\n[tool.setuptools_scm]\nwrite_to \"pkg/_version.py\"\nversion_scheme = \"post-release\"\nlocal_scheme = \"no-local-version\"\ngit_describe_command = \"git describe --tags\"\n\nIncluding Version in Sphinx Documentation\nIf you have Sphinx documentation you can add the following to docs/conf.py\nfrom importlib.metadata import version\nrelease = version(\"myproject\")\nversion = \".\".join(release.split(\".\")[:2])"
  },
  {
    "objectID": "posts/python-packaging/index.html#building-your-package",
    "href": "posts/python-packaging/index.html#building-your-package",
    "title": "Python Packaging",
    "section": "Building your Package",
    "text": "Building your Package\n\nGenerate Distribution Archive\nIn your package directory you can create a distribution of your package with the latest versions of setuptools and wheel. To do this in your virtual environment run the following. The documentation for how to do this is at Building and Distributing Packages with Setuptools.\n[build-system]\nrequires = [\n  \"setuptools >= 65.6.3\",\n  \"wheel\",\n]\nbuild-backend = \"setuptools.build_meta\"\nThe package can now be built locally with…\npython -m pip install --upgrade setuptools wheel\npython -m build --no-isolation\n…and the resulting package will be generated in the dist/ directory."
  },
  {
    "objectID": "posts/python-packaging/index.html#publishing-to-pypi",
    "href": "posts/python-packaging/index.html#publishing-to-pypi",
    "title": "Python Packaging",
    "section": "Publishing to PyPI",
    "text": "Publishing to PyPI\nBefore pushing the package to the main PyPi server it is prudent to test things out on TestPyPI first. You must first generate an API Token from your account settings page. It needs a name and the scope should be `Entire account (all projects)`. This token will be shown once so do not navigate away from the page until you have copied it.\nYou use twine to upload the package and should create a .pypirc file in the root of the package directory that contains your API key and the username __token__. For the TestPyPI server it follows the following format.\n[testpypi]\n  username = __token__\n  password = pypi-dfkjh9384hdszfkjnkjahkjfhd3YAJKSHE0089asdf0lkjsjJLLS_-0942358JKHDKjhkljna39o854yurlaoisdvnzli8yw459872jkhlkjsdfkjhasdfadsfasdf\nOnce this is in place you are ready to use twine to upload the package using the configuration file you have just created.\ntwine upload --config-file ./.pypirc --repository testpypi dist/*\n\nTesting Download\nAfter having uploaded your package to the TestPyPI server you should create a clean virtual environment and try installing the package from where you have just uploaded it. You can do this using pip and the --index-url and --extra-index-url, the former installs your package from TestPyPI, the later installs dependencies from PyPI.\npip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ your-package\nOnce installed you can try running the code, scripts or notebooks associated with the package as you would normally.\n\n\nRepeat for PyPI\nOnce you are happy this is working you can repeat the process on the main PyPI server. You can add the token that you generate to /.pypirc under a separate heading.\n[testpypi]\n  username = __token__\n  password = pypi-dfkjh9384hdszfkjnkjahkjfhd3YAJKSHE0089asdf0lkjsjJLLS_-0942358JKHDKjhkljna39o854yurlaoisdvnzli8yw459872jkhlkjsdfkjhdfJZZZZZF\n[pypi]\n  username = __token__\n  password = pypi-dfkjh9384hdszfkjnkjahkjfhd3YAJKSHE0089asdf0lkjsjJLLS_-0942358JKHDKjhkljna39o854yurlaoisdvnzli8yw459872jkhlkjsdfkjhdfJZZZZZF\n\n\nGitHub Action\nManually uploading is somewhat time consuming and tedious. Fortunately though with setuptools_scm in place and tokens generated we can automate the process of building and uploading packages to PyPI using the GitHub Action gh-action-pypi-publish (read more about GitHub Actions). You will have already generated a PYPI token (and similarly one for test PyPI) and these can stored on the projects GitHub account under Settings > Secrets > Actions with the names PYPI_API_TOKEN and TEST_PYPI_API_TOKEN respectively. You can then add the following GitHub Action under .github/workflow/pypi.yaml.\nname: Publish package to PyPi\n\non:\n  push:\ntags:\n  - v*\njobs:\n  build-release:\nruns-on: ubuntu-latest\nname: Publish package to PyPi\nsteps:\n  - uses: actions/checkout@v3\n    with:\n      fetch-depth: 0\n  - name: Setup Python\n    uses: actions/setup-python@v4.3.0\n    with:\n      python-version: 3.9\n      cache: 'pip'\n  - name: Installing the package\n    run: |\n      pip3 install .\n      pip3 install .[pypi]\n  - name: Build package\n    run: |\n      python -m build --no-isolation\n  - name: Publish package to PyPI\n    uses: pypa/gh-action-pypi-publish@release/v1\n    with:\n      user: __token__\n      password: ${{ secrets.PYPI_API_TOKEN }}\n\n\nReleasing via GitHub\nWith setuptools_scm in place and a GitHub Action setup and configured it is now possible to make a release to PyPI via GitHub Releases.\n\nGo to the Releases page (its linked from the right-hand side of the front-page).\nDraft a New release.\nCreate a new tag using semantic versioning and select “Create new tag v#.#.# on publish”.\nClick the \"Generate Release Notes\" button, this adds all the titles for all Pull Requests, I'll often remove all these but leave the link to the ChangeLog that is generated for the release.\nWrite your release notes.\nSelect \"Set as latest release\".\nSelect \"Create a discussion for this releases\" and select \"Announcements\".\nClick on \"Publish Release\"."
  },
  {
    "objectID": "posts/python-packaging/index.html#packaging-frameworks",
    "href": "posts/python-packaging/index.html#packaging-frameworks",
    "title": "Python Packaging",
    "section": "Packaging Frameworks",
    "text": "Packaging Frameworks\nThere are some frameworks that are meant to ease the pain of this process and make it easier. I'm yet to test these for two reasons. Firstly I wanted to understand what is going on rather than learn another framework. Secondly it was an additional framework to learn.\n\nPDM\nPDM (Python package and Dependency Manager) handles all stages of setting up and creating a package and managing its dependencies. In essence its a tool for interactively generating the configuration files described above. I've not yet.\n\n\nPoetry\nPoetry is another package for managing packaging and dependencies. Again, I've not yet used it."
  },
  {
    "objectID": "posts/python-packaging/index.html#links",
    "href": "posts/python-packaging/index.html#links",
    "title": "Python Packaging",
    "section": "Links",
    "text": "Links\n\nPyPA : Building and Distributing Packages with Setuptools\nPyPA : Specifications\nPackaging Python Projects\nPython package structure information — pyOpenSci Python Packaging Guide\nPackaging Data files in a Python Distribution\nPDM - Python package and Dependency Manager\nWhy you shouldn't invoke setup.py directly\npython-versioneer/python-versioneer: version-string management for VCS-controlled trees\npypa/setuptoolsscm: the blessed package to manage your versions by scm tags\nrye one-shop-stop for Python"
  },
  {
    "objectID": "posts/running-2022/index.html",
    "href": "posts/running-2022/index.html",
    "title": "Running in 2022",
    "section": "",
    "text": "I started running 🏃 in 2019. It never used to appeal but I wanted to get more regular exercise easily from my doorstep as despite commuting by bike for most of my life and cycling between 12-20km daily it wasn’t enough for me. Whilst I’ve always cycled for commuting and cycling was delightful during the pandemic lockdowns I’ve since enjoyed it less and less as it seems an increasing number of motorists seen to have little to no regard for the safety of pedestrians and cyclists. As a consequence I prefer running over cycling these days and get most of my aerobic exercise on two feet rather than two wheels."
  },
  {
    "objectID": "posts/running-2022/index.html#background",
    "href": "posts/running-2022/index.html#background",
    "title": "Running in 2022",
    "section": "Background",
    "text": "Background\nHaving worked as a statistician (of different sorts) over a number of years I also like data and analysing it. I therefore record my runs (and occasional cycles). A long time ago I used to use Endomondo but didn’t like the idea of sharing my personal location data with a random company so went private and now use the privacy respecting OpenTracks1.\nI use Emacs for most of my work, often using Org-mode which offers literate programming via Org-Babel and so my logged runs/cycles/hikes/climbs are summarised captured into Org files and I’ve written a literate document to process these in R and output to HTML which I host on my VPS. I can view my progress online.\nAt some point early in 2022 I decided to set myself an arbitrary goal of running at least 1200km in the year. It seemed a nice round number at 100km/month. This post summarises the data collected during that period and serves as a learning exercise for me to refresh/improve my R and Quarto knowledge. I used to use R when I worked as a Medical Statistician but for the last five years or so I’ve mainly used Python for work reasons. I’m keen to use Quarto as its a very neat literate programming framework (this blog is written in Quarto). The code chunks are hidden by default but can be easily expanded if you wish to look at them. I’ve summarised some of the features I’ve learnt about Quarto and I’ve explained how I use Emacs Org-mode to capture my runs."
  },
  {
    "objectID": "posts/running-2022/index.html#data",
    "href": "posts/running-2022/index.html#data",
    "title": "Running in 2022",
    "section": "Data",
    "text": "Data\nMy data is captured using org-capture in an ASCII text file (for more on this setup see below), but its wrapped in an org-mode table. For the purpose of this post I have imported, filtered and redacted some of the data saved this to a .RData file having so that it works with the GitHub Action that produces the blog. The following code chunk determines where to load the file from. If I’m working on the file locally the original data is loaded and parsed before saving to .RData. In GitHub pages, the Action (CI/CD) pipeline won’t have access to the original instead it load the .RData file and its good to go with generating graphs and renders correctly.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| label: load-data\nlibrary(dplyr)\nlibrary(ggdark)\nlibrary(ggplot2)\nlibrary(ggridges)\nlibrary(hms)\nlibrary(knitr)\nlibrary(lubridate)\nlibrary(orgutils)\nlibrary(plotly)\nlibrary(readr)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(stringr)\n\ncurrent_env <- Sys.getenv()\ncurrent_env[\"hostname\"] <- Sys.info()[\"nodename\"]\n\nif(current_env[\"hostname\"] == \"kimura\") {\n    # If at home we load and parse the data\n    remove_days <- c(\"Mon \" = \"\", \"Tue \" = \"\", \"Wed \" = \"\", \"Thu \" = \"\", \"Fri \" = \"\", \"Sat \" = \"\", \"Sun \" = \"\")\n    training_dir <- paste(current_env[\"HOME\"], \"org/training/\", sep=\"/\")\n    running_log <- paste(training_dir, \"log/running.org\", sep=\"\")\n\n    data <- orgutils::readOrg(running_log, table.name=\"running-log\")\n    data <- dplyr::tibble(data)\n    data <- data |>\n        mutate(distance = as.double(stringr::str_replace(Distance, \"km\", \"\")),\n               date = stringr::str_replace(Date, \"<\", \"\"),\n               date = stringr::str_replace(date, \">\", \"\"),\n               date = stringr::str_replace(date, \"\\\\[\", \"\"),\n               date = stringr::str_replace(date, \"\\\\]\", \"\"),\n               date = stringr::str_replace_all(date, remove_days),\n               year_month_day = stringr::str_extract(date, \"[0-9]+-[0-9]+-[0-9]+\"),\n               time = stringr::str_replace(Time, \"min \\\\+ \", \" \"),\n               time = stringr::str_replace(time, \"s\", \"\")) |>\n        dplyr::filter(year_month_day>= ymd(\"2022-01-01\")) |>\n        tidyr::separate(time, c(\"min\", \"sec\")) |>\n        mutate(date = lubridate::ymd_hm(date),\n               year_month = floor_date(date, \"month\"),\n               year_week = floor_date(date, \"week\"),\n               year_day = lubridate::wday(date, week_start=1, label=TRUE, abbr=FALSE),\n               logged_at = hms::as_hms(date),\n               min = as.integer(min),\n               sec = as.integer(sec),\n               hour = floor(min / 60),\n               min = min - (hour * 60),\n               time = lubridate::hms(paste(hour, min, sec, sep=\":\")),\n               time_sec = lubridate::period_to_seconds(time),\n               time_min = lubridate::period_to_seconds(time) / 60,\n               pace = as.numeric(time) / (60 * distance)) |>\n        dplyr::select(-c(Date, Route, Time, Pace, year_month_day, hour, min, sec))\n    # readr::write_csv(data, file=\"running_2022.csv\")\n    save(data, file=\"data.RData\")\n\n} else {\n    # Otherwise we load parsed data.\n    # data <- readr::read_csv(file=\"running_2022.csv\", col_names = TRUE)\n    load(\"data.RData\")\n}\n\nsummary_data <- data  |>\n     dplyr::select(distance, pace, time)  |>\n     dplyr::summarise(across(c(distance, pace, time),\n                             list(sum=sum,\n                                  mean=mean,\n                                  sd=sd,\n                                  median=median,\n##                                  quantile=quantile,\n                                  min=min,\n                                  max=max),\n                             .names = \"{.col}_{.fn}\"))"
  },
  {
    "objectID": "posts/running-2022/index.html#summary",
    "href": "posts/running-2022/index.html#summary",
    "title": "Running in 2022",
    "section": "Summary",
    "text": "Summary\nIn total in 2022 I went out running 146 times and covered a total distance of 1377.2km. Individual runs ranged from 6.43km to 32.62km (mean : 9.4328767km; standard deviation : 3.2130937km; median : 8.69km). The mean pace across all runs was 5.2192349min/km (standard deviation 0.265666; fastest : 4.754306min/km; slowest 7.1990599min/km). The total time I spent running was 4513 (mean : 30.9109589; standard deviation 1249.9327749; median : 30).\nThe longest run I did was the Edale Skyline ( 32.62km in 0.9833333mins) although I started in Hope and went anti-clockwise rather than the “race” circuit of starting in Edale and ascending Ringing Roger then going clockwise. This meant I had slightly less ascent to do. However by around 28-29km coming off of Kinder and passing Hope Cross my legs (well the tendons going into my groin!) were complaining a lot and so I basically walked the remainder up to Win Hill and very tentatively jogged back down into Hope. This data point is a bit of an outlier and so is excluded from some plots. For some reason this value isn’t correctly shown in the table below.\nTODO - Calculate the inter-quartile range, tricky with across() used above? 🤔\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| label: summary-data\n##| tbl-cap-location: top\n##| tbl-cap: Summary statistics for run distances, pace and time.\n\nsummary_data |>\n    reshape2::melt()  |>\n    tidyr::separate(variable, c(\"Metric\", \"Statistic\")) |>\n    dplyr::mutate(Metric = dplyr::recode(Metric, distance=\"Distance (km)\", time=\"Time (min)\", pace=\"Pace (min/km)\"),\n                  Statistic = dplyr::recode(Statistic, sum=\"Total\", mean=\"Mean\", sd=\"SD\", min=\"Min.\", max=\"Max.\",\n                                       median=\"Median\"))  |>\n    tidyr::pivot_wider(names_from=Metric, values_from=value) |>\n    knitr::kable(digits=3)\n\n\n\n\nStatistic\nDistance (km)\nPace (min/km)\nTime (min)\n\n\n\n\nTotal\n1377.200\n762.008\n4513.000\n\n\nMean\n9.433\n5.219\n30.911\n\n\nSD\n3.213\n0.266\n1249.933\n\n\nMedian\n8.690\n5.193\n30.000\n\n\nMin.\n6.430\n4.754\n0.000\n\n\nMax.\n32.620\n7.199\n59.000\n\n\n\n\n\nFor some reason\n\nNumber of Runs\nHow often I go running varies depending on the distance I’ve done recently and more importantly whether I’m carrying an injury of some sort.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| label: number-of-runs\n##| fig-cap-location: top\n##| fig-cap: \"Runs per by Month and Week\"\n##| fig-subcap:\n##|    - \"Runs by month\"\n##|    - \"Runs by week\"\n##| fig-alt: \"A plot of runs per month/week in 2022.\"\nmonth <- data |> ggplot(aes(year_month)) +\n    geom_bar() +\n    dark_theme_bw() +\n    ylab(\"Runs\") +\n    xlab(\"Month/Year\") +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(month)\n\n\n\n\nweek <- data |> ggplot(aes(year_week)) +\n    geom_bar() +\n    dark_theme_bw() +\n    ylab(\"Runs\") +\n    xlab(\"Week\") +\n    scale_x_datetime(breaks = date_breaks(\"2 weeks\")) + ##, labels=date_format(\"%w\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(week)\n\n\n\n\n\n–\n\n\nDistance\nI prefer longer runs, I often feel I’m only getting going and into a rhythm after 3-5km and therefore struggle to find motivation to go out for short runs. That said, whilst I can run > 21km (half-marathon) distances I don’t do so often and past experience tells me that if I run too much I end up injuring myself.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: distance-time-series\n##| fig-cap: \"Distance of Runs.\"\n##| fig-alt: \"A plot of distance of runs in 2022.\"\ndistance_time <- data |>\n    ggplot(aes(date, distance)) +\n    geom_line() +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Date\") +\n    # scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%b\"))\nggplotly(distance_time)\n\n\n\n\n\n\nDistance By Month\nMarch was a fallow month for running as I had a sore knee and a month of work and so I did a lot of cycling (A57 Snake Pass when it was closed, a morning jaunt to Bakewell and back) and DIY around the home and garden (finishing off a patio). I also had to ease off from mid-November and most of December due to a sore thigh.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: true\n##| fig-cap-location: top\n##| label: distance-per-month\n##| fig-cap: \"Distance per month.\"\n##| fig-subcap:\n##|    - \"Bar Chart\"\n##|    - \"Box Plot\"\n##|    - \"Ridge Density\"\n##| fig-alt: \"A plot of distance per month/week in 2022.\"\nbar_distance_month <- data |> ggplot(aes(year_month)) +\n    geom_bar(aes(weight = distance)) +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\")  +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\"))\n    ## theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(bar_distance_month)\n\n\n\n\nbox_distance_month <- data |> ggplot(aes(year_month, distance)) +\n    geom_boxplot(aes(factor(year_month), distance)) +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\")\n    ## scale_x_datetime(breaks = date_breaks(\"1 month\")) + ##, labels=date_format(\"%B\"))\n    ## theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(box_distance_month)\n\n\n\n\nridge_distance_month <- data |> ggplot(aes(x=distance, y=factor(year_month), group=year_month, fill=after_stat(x))) +\n    # geom_density_ridges_gradient(scale=1, gradient_lwd=1.) +\n    geom_density_ridges_gradient(gradient_lwd=1.) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_fill_viridis_c(name=\"Distance (km)\", option=\"C\") +\n    dark_theme_bw() +\n    ylab(\"Year\") +\n    xlab(\"Distance (km)\") ## +\n    ## scale_y_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\"))\n### ggplotly(ridge_distance_month)\nridge_distance_month\n\n\n\ndev.off()\n\nnull device \n          1 \n\n\n\n\nDistance By Week\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: distance-per-week\n##| fig-cap: \"Distance per week.\"\n##| fig-subcap:\n##|    - \"Bar Chart\"\n##|    - \"Box Plot\"\n##| fig-alt: \"Plots of distance per month in 2022.\"\nbar_distance_week <- data |> ggplot(aes(year_week)) +\n    geom_bar(aes(weight = distance)) +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\")  +\n    ## scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%w\"))\n    theme(axis.text.x = element_text(angle=30, hjust=1))\n\nggplotly(bar_distance_week)\n\n\n\n\nbox_distance_week <- data |> ggplot(aes(factor(year_week), distance)) +\n    geom_boxplot() +\n    dark_theme_bw() +\n    ylab(\"Distance (km)\") +\n    xlab(\"Month/Year\") +\n    ## scale_x_datetime(breaks = date_breaks(\"2 weeks\")) + ##, labels=date_format(\"%w\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(box_distance_week)\n\n\n\n\n\n\n\nRuns v Distance\nLets look at how the number of runs per week affects the overall distance covered. Do I keep to a consistent distance if I’m doing less runs by doing longer runs? We can look at that by plotting the number of runs per week against either the total distance or the mean distance for that week. If I do fewer longer runs there should be a downwards trend, with weeks where I only run once having very high values, and weeks where I do multiple runs having very low means.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: runs-distance\n##| fig-cap: \"Runs per week v Distance\"\n##| fig-subcap:\n##|    - Total Distance\n##|    - Mean Distance\n##| fig-alt: \"A plot of number of runs per week and the distanace covered in 2022.\"\ntmp_data <- data |>\n    group_by(year_week) |>\n    summarise(runs = n(),\n              distance_total= sum(distance),\n              distance_mean = mean(distance))\nruns_distance_total<- tmp_data |>\n    ggplot(aes(runs, distance_total)) +\n    geom_point(aes(color=distance_mean, size=distance_mean, alpha=0.45)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Total Distance (km)\") +\n    xlab(\"Runs (km)\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(runs_distance_total, tooltip = c(\"year_week\", \"distance_total\", \"distance_mean\"))\n\n\n\n\nruns_distance_mean <- tmp_data |>\n    ggplot(aes(runs, distance_mean, label=year_week)) +\n    geom_point(aes(color=distance_total, size=distance_total, alpha=0.45)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Mean Distance (km)\") +\n    xlab(\"Runs (km)\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(runs_distance_mean, tooltip = c(\"year_week\", \"distance_total\", \"distance_mean\"))\n\n\n\n\n\n\n\n\nPace\nI like pushing myself and going fast, I perhaps stupidly and against much perceived wisdom, think that if I’m able to hold a conversation then I’m not trying hard enough (in reality I rarely try talking as I always run on my own).\nBefore looking at pace over time its interesting to look at the relationship between distance and time. Obviously longer runs are going to take longer, but is the relationship linear or do I get slower the further I go?\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: distance-v-time\n##| fig-cap: \"Distance v Pace.\"\n##| fig-subcap:\n##|    - All\n##|    - Excluding Outliers\n##| fig-alt: \"Distance v Pace for runs in 2022.\"\ndistance_time_all <- data |>\n    ggplot(aes(distance, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Distance\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(distance_time_all)\n\n\n\n\ndistance_time_excl_outliers <- data |>\n    dplyr::filter(pace < 7) |>\n    ggplot(aes(distance, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Distance\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\")\nggplotly(distance_time_excl_outliers)\n\n\n\n\n\nThe relationship between distance and pace is interesting, one might expect that the pace decreases with the overall distance, but it depends on the terrain. Most of my shorter runs involved a fair proportion of uphill as I live in the bottom of a valley and my typical circuit takes me up the valley to some extent before turning around and heading back. Longer runs I would typically get out of the valley and run along fairly flat ground before heading back down and I think this is what causes the dip in the above graph (excluding outliers) in the range of 11-14km, but further distances I tire and so my pace drops.\nLooking at pace over time it increases as the year progresses but then runs are getting gradually longer, and I know I changed the route to include more hills. Strangely getting COVID at the end of August didn’t appear to negatively impact my pace although running with an injury late November/December did (I should probably have had a break but wanted to reach my goal so dialled down the frequency and distance).\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: pace-overtime\n##| fig-cap: \"Pace over time by distance.\"\n##| fig-alt: \"A plot of distance per month/week in 2022.\"\nno_outliers <- data |>\n    dplyr::filter(pace < 7)\npace_timeseries<- data |>\n    ggplot(aes(date, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth(method=\"loess\", data=no_outliers) +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Date\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") +\n    scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%b\"))\nggplotly(pace_timeseries)\n\n\n\n\n\n\nPace By Month\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: pace-per-month\n##| fig-cap: \"Pace per month.\"\n##| fig-subcap:\n##|    - \"Box Plot\"\n##|    - \"Ridge Density\"\n##| fig-alt: \"A plot of pace per month/week in 2022.\"\nbox_pace_month <- data |> ggplot(aes(factor(year_month), pace)) +\n    geom_boxplot() +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Month/Year\") ## +\n### scale_x_datetime(breaks = date_breaks(\"2 weeks\"), labels=date_format(\"%B\"))\nggplotly(box_pace_month)\n\n\n\n\nridge_pace_month <- data |> ggplot(aes(x=pace, y=factor(year_month), group=year_month, fill=after_stat(x))) +\n    # geom_density_ridges_gradient(scale=1, gradient_lwd=1.) +\n    geom_density_ridges_gradient(scale=1, gradient_lwd=1.) +\n    scale_x_continuous(expand = c(0, 0)) +\n    # scale_y_discrete(expand = expansion(mult = c(0.01, 0.25))) +\n    scale_fill_viridis_c(name=\"Pace (min/km)\", option=\"C\") +\n    dark_theme_bw() +\n    ylab(\"Year\") +\n    xlab(\"Pace (min/km)\") ## +\n###     scale_x_datetime(breaks = date_breaks(\"1 month\"), labels=date_format(\"%B\"))\n### ggplotly(ridge_pace_month)\nridge_pace_month\n\n\n\n\n\n\nPace By Week\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: pace-per-week\n##| fig-cap: \"Pace per week.\"\n##| fig-alt: \"Plots of pace per month in 2022.\"\nbox_pace_week <- data |> ggplot(aes(factor(year_week), pace)) +\n    geom_boxplot() +\n    dark_theme_bw() +\n    ylab(\"Pace (min/km)\") +\n    xlab(\"Month/Year\") +\n    ## scale_x_datetime(breaks = date_breaks(\"2 weeks\"), labels=date_format(\"%w\")) +\n    theme(axis.text.x = element_text(angle=30, hjust=1))\nggplotly(box_pace_week)\n\n\n\n\n\n\n\n\nWhen Do I Run?\nI’m much more a morning person and typically go out running on an empty stomach as I find it quite unpleasant to have a bellyful of food jiggling around inside me. But what days of the week and times do I actually go running? I can answer this with only a low degree of accuracy because whilst I do try to log my runs immediately after having completed them (post-stretching!) I don’t always do so and so the times below reflect the times I logged the run rather than started.\nIn the summer when it gets light early I’ll sometimes go out running at 06:00 but clearly these are not reflected in the logged times.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: when-I-run\n##| fig-cap: \"When I go running.\"\n##| fig-subcap:\n##|    - \"Day\"\n##|    - \"Time\"\n##|    - \"Month v Time\"\n##|    - \"Day v Time\"\n##| fig-alt: \"When I go running\"\nwhat_day_I_run <- data  |> ggplot(aes(year_day)) +\n    geom_bar() +\n    dark_theme_bw() +\n    xlab(\"Day of Week\") +\n    ylab(\"N\")\nggplotly(what_day_I_run)\n\n\n\n\nwhen_I_run <- data |> ggplot(aes(logged_at)) +\n    geom_bar() +\n    dark_theme_bw() +\n    xlab(\"Time of Day\") +\n    ylab(\"N\")\nggplotly(when_I_run)\n\n\n\n\nwhat_time_I_run_by_month <- data |> ggplot(aes(year_month, logged_at)) +\n    geom_point() +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    xlab(\"Month\") +\n    ylab(\"Time of Day\")\nggplotly(what_time_I_run_by_month)\n\n\n\n\nwhat_time_I_run_each_day <- data |> ggplot(aes(year_day, logged_at)) +\n    geom_point() +\n    geom_smooth(method=\"loess\") +\n    dark_theme_bw() +\n    xlab(\"Day of Week\") +\n    ylab(\"Time of Day\")\nggplotly(what_time_I_run_each_day)\n\n\n\n\n\n\nDoes distance differ with time of day?\nIf I go out running in the morning I usually go further because I work five days a week and lunch-time runs have to fit within an hour. As you can see I don’t generally run in the evening as I don’t like exercising with food in my stomach.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: when-I-run-distance\n##| fig-cap: \"Time of Day v Distance.\"\n##| fig-alt: \"Time of day I go running v distance\"\nwhen_I_run_distance <- data |> ggplot(aes(logged_at, distance)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth() +\n    dark_theme_bw() +\n    xlab(\"Time of Day\") +\n    ylab(\"Distance\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") ## +\n    ## scale_x_datetime(breaks = date_breaks(\"1 hour\"), labels=date_format(\"%H\"))\nggplotly(when_I_run_distance)\n\n\n\n\n\n\n\nDoes pace differ with time of day?\nAs I’ve mentioned I somewhat counter-intuitively get a faster mean pace when going further distances. Does this feature come through with the time of day and do I run faster in the morning or at other times. The graph below plots the time of day against the pace with the distance denoted by the size and colour of points.\n\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n##| fig-cap-location: top\n##| label: when-I-run-pace\n##| fig-cap: \"When I go running v pace.\"\n##| fig-alt: \"Time of day I go running v pace\"\nwhen_I_run_pace <- data |> ggplot(aes(logged_at, pace)) +\n    geom_point(aes(color=distance, size=distance)) +\n    geom_smooth() +\n    dark_theme_bw() +\n    xlab(\"Time of Day\") +\n    ylab(\"Pace\") +\n    scale_color_gradientn(colors = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n    theme(legend.position = \"right\") ## +\n    ## scale_x_datetime(breaks = date_breaks(\"1 hour\"), labels=date_format(\"%H\"))\nggplotly(when_I_run_pace)\n\n\n\n\n\nI could go on and on making different types of plots but I think that is sufficient for now."
  },
  {
    "objectID": "posts/running-2022/index.html#capturing-data-in-emacs-org-mode",
    "href": "posts/running-2022/index.html#capturing-data-in-emacs-org-mode",
    "title": "Running in 2022",
    "section": "Capturing Data in Emacs Org-mode",
    "text": "Capturing Data in Emacs Org-mode\nI thought it would be worthwhile including a section on how I capture data. Its not perfect (yet!) because as mentioned I use OpenTracks to log my runs. This saves data to a GPX file on finishing on my phone and I use SyncThing to back these up automatically to my server using a run condition so it only tries to do so when the phone is connected to my home WiFi network and the phone is charging. At some point I will take advantage of this and develop a dashboard that loads the GPX data and allows interactive exploration as well as summarising the data similar to that which I’ve presented here. But life is busy so for now I manually capture the summary statistics using Org-mode and Org Capture templates.\nI have a file ~/path/to/running.org which contains an Org table as shown below.\n##+CAPTION: Running Log\n##+NAME: running-log\n| Date                   | Route                     | Distance | Time         | Pace                           | Notes     |\n|------------------------+---------------------------+----------+--------------+--------------------------------+-----------|\n| [2022-12-25 Sun 10:17] | From here to there.       | 9.17km   | 48min + 06s  | 5 min / km + 14.721919 s / km  | Felt ok   |\n| [2022-12-21 Wed 13:20] | A to B via C              | 9.81km   | 49min + 14s  | 5 min / km + 1.1213048 s / km  | No falls! |\n| [2022-12-17 Sat 09:08] | There and back            | 15.05km  | 79min + 03s  | 5 min / km + 15.149502 s / km  | Not bad   |\n|------------------------+---------------------------+----------+--------------+--------------------------------+-----------|\n##+TBLFM: $5=uconvert($4/$3, (min+s)/km);L\nThe layout is I think self-explanatory and each time I want to log a run I could open the file C-x C-f ~/path/to/running.org, navigate to the top of the file and start entering details manually in a new row. But that is a bit slow and instead I wrote an Org-capture rule to capture runs (and many other things). Writing these was initially a bit tricky (as I’m a slow learner) but I now understand the structure and can quickly add new entries to capture items where I want them to be, saving me time in the long run.\n(use-package org-capture\n         :ensure nil\n         :after org-gtd\n         :config\n         (setq org-default-notes-file (concat org-directory \"/notes.org\"))\n         (setq org-capture-templates\n           '((\"e\" \"Exercise\")\n             (\"er\" \"Logging a run\" table-line (file+olp \"~/path/to/running.org\")\n              \"| %U | %? | km | min + s | | |\" :prepend t)\n             (\"ec\" \"Logging a cycle\" table-line (file+olp \"~/path/to/cycling.org\")\n              \"| %U | %? | km | min + s | | |\" :prepend t)\n             (\"eh\" \"Logging a hike\" table-line (file+olp \"~/path/to/hiking.org\")\n              \"| %U | %? | km | m | min + s| |\" :prepend t)\n             (\"em\" \"Weight & Waist/Hip\" table-line (file+olp \"~/path/to/metrics_weight.org\")\n              \"| %U | %? | | | |\" :prepend t)\n             (\"es\" \"Steps\" table-line (file+olp \"~/path/to/metrics_steps.org\")\n              \"| %t | %? |\" :prepend t)\n             (\"eb\" \"Blood\" table-line (file+olp \"~/path/to/metrics_blood.org\")\n              \"| %U | %? | | | | | |\" :prepend t))))\nTo enter Org-capture its C-c c this brings up a menu for all of the capture templates I’ve defined…\nSelect a capture template\n=========================\n\n[E]  Email\n[a]  Agenda\n[e]  Exercise\n[w]  Work\nExercise is entered by pressing e as defined on the line (\"e\" \"Exercise\"). I then see a sub-menu…\nSelect a capture template\n=========================\n\ne [r]  Logging a run\ne [c]  Logging a cycle\ne [h]  Logging a hike\ne [m]  Weight & Waist/Hip\ne [s]  Steps\ne [b]  Blood\n…and I can choose what activity to log, hit r for a run and a new buffer appears, the date and time is entered automatically because that field is set to be %U in the template. The cursor is located in the Route column because the field content is %? which means user input is required. The Distance, Time and Notes fields are also completed although the Pace field should be left blank since the formula at the bottom of the table (#+TBLFM: $5=uconvert($4/$3, (min+s)/km);L) calculates this automatically on saving.\nCapture buffer, Finish 'C-c C-c', refile 'C-c C-w', abort 'C-c C-k'\n| [2022-12-27 Tue 11:29] | Out for a run | 10.8km | 54min + 12s | | Fun run in the snow. |\nOnce all the fields are completed press C-c C-c to save the changes and the row is added to the table in the file ~/path/to/running.org. This file forms part of my training.org (its pulled into the main document training.org using #+INCLUDE: ~/path/to/running.org), but the data is used and processed using R. How does the data get from the org-formatted table into the R session as a data frame for summarising and using? This is part of the amazing magic that is Org-babel for literate programming. A source code chunk can be defined with a :var option which refers to the table you want to include. In this my source block does some processing of the table to tidy up the dates into something R understands and is shown below.\n##+begin_src R :session *training-R* :eval yes :exports none :var running_table=running-log  :colnames nil :results output silent\n  running_table %<>% mutate(distance = as.double(str_replace(Distance, \"km\", \"\")),\n             time = str_replace(Time, \"min \\\\+ \", \" \"),\n             time = str_replace(time, \"s\", \"\"),\n             Date = str_extract(Date, \"[0-9]+-[0-9]+-[0-9]+\"),\n             date = ymd(Date),\n             year = floor_date(date, \"year\"),\n             year_month = floor_date(date, \"month\"),\n             year_week = floor_date(date, \"week\")) %>%\n      separate(time, c(\"min\", \"sec\")) %>%\n      mutate(min = as.integer(min),\n             sec = as.integer(sec),\n             hour = floor(min / 60),\n             min = min - (hour * 60),\n             # time = chron(time=paste(hour, min, sec, sep=\":\")),\n             time = hms(paste(hour, min, sec, sep=\":\")),\n             pace = as.numeric(time) / (60 * distance)) %>%\n             # pace = Pace) %>%\n      select(-c(Date, Distance, Time, Pace, hour, min, sec))\n##+end_src\nThe key to getting the Org-mode table (which has the #+NAME: running-log) into the R session (which is set to :session *training-R* and is evaluated :eval yes) is the option :var running_table=running-log which makes the table available in the R session as the dataframe running_table. As you’ll see from the very first code chunk at the top of this post because this document is written in Quarto I instead use the orgutils::readOrg() package/function to read the table into R directly."
  },
  {
    "objectID": "posts/running-2022/index.html#quarto---things-ive-learnt",
    "href": "posts/running-2022/index.html#quarto---things-ive-learnt",
    "title": "Running in 2022",
    "section": "Quarto - Things I’ve learnt",
    "text": "Quarto - Things I’ve learnt\nSome things I’ve learnt about Quarto whilst preparing this document.\n\nCode Folding\nIt should be possible to set options at the global level by setting the following in the site index.qmd.\nexecute:\n  code-fold: true\n  code-tools: true\n  code-link: true\nI’m using the blogging feature of Quarto but adding this to the site index.qmd didn’t work when previewed locally. I tried adding it to the YAML header for the post itself (i.e. posts/running-2022/index.qmd) but no joy, the code chunks were still displayed. I could however set this on a per-chunk basis though so each code chunk carries the options.\n##| code-fold: true\n##| code-link: true\n##| code-tools: true\n##| warning: false\n\n\nTable and Figure caption locations are should be configurable\nCaptions were by default underneath each picture which is perhaps ok when reading as a PDF but this is rendered as HTML and I would prefer these to be at the top so that readers see the heading as they scroll down (I’m often torn about figure headings and labels and feel they should be included in the image itself so they are retained if/when they are used elsewhere).\nFortunately you can specify the location of table and figure captions. Unfortunately this doesn’t appear to render correctly when using the blogging feature and all captions are still at the bottom.\n##| fig-cap-location: [top|bottom|margin]\n##| tbl-cap-location: [top|bottom|margin]\n\n\nTwo for the price of one\nIts possible to include two or more sub-figures in a code chunk and have them both displayed.\n##| label: pace-per-month\n##| fig-cap: \"Pace per month.\"\n##| fig-subcap:\n##|    - \"Bar Chart\"\n##|    - \"Box Plot\"\n##| fig-alt: \"A plot of pace per month/week in 2022.\"\nIn the output format of the blog these do not appear side by side, but rather underneath each other.\n\n\nPlotly plays with Quarto\nUsing the plotly R package with Quarto “Just Works”, the plots render nicely in the page and are zoom-able with tool-tips appearing over key points.\n\n\nSome graphs appear where I don’t expect them to\nAstute readers will notice that some of the ridge plot graphs appear more than once. I couldn’t work out why this was, the code does not specify that they should be shown again. For example the Ridge Density plot for total Distance by Month also appeared under the total Distance by Week. To try working around this I attempted to explicitly use dev.off() after the initial generation of the first Ridge Density Plot, but this had the undesired effect of including output from the call and an additional code-chunk. Not one I’ve sorted yet. 🤔\n\n\nEmoji’s\nTo include emoji’s in the Markdown it’s necessary to add the following to the header of the specific page (i.e. posts/running_2022/index.qmd)\nfrom: markdown+emoji\nText based emoji names (e.g. :thinking: 🤔 ; :snake 🐍 ; :tada: 🎉) are then automatically included when rendering."
  },
  {
    "objectID": "posts/running-2022/index.html#r---things-ive-forgotten-remembered",
    "href": "posts/running-2022/index.html#r---things-ive-forgotten-remembered",
    "title": "Running in 2022",
    "section": "R - Things I’ve forgotten remembered",
    "text": "R - Things I’ve forgotten remembered\nIts been a few years since I used R on a daily basis. As a consequence I thought I’d forgotten a bunch of things I used to know how to do, but this exercise has reminded me that I perhaps have some vestigial remnants of my old knowledge lingering. Some things I had to look up and, unsurprisingly, the available tools, functions/methods have evolved in that time (viz. tidyr is now more generic and simpler than reshape2).\n\nFormatting Dates/Times in Axes\nI’ve a few more niggles to round out such as the formatting of the months/weeks which I should probably do up-front in the dataframe rather leaving them as POSIXct objects as then the and then the ggplot2 functions scale_x_datetime() can be used directly (in some places I convert to factors which doesn’t help).\n\n\nPreviewing Graphs\nOne very nice feature I discovered recently courtesy of a short video by Bruno Rodrigues (thanks Bruno 👍) is the ability to preview graphs in the browser. This is unlikely to be something you need if you use RStudio but as with most things I use Emacs and Emacs Speaks Statistics and so normally I get an individual X-window appearing showing the plot. Instead we can use the httpgd package to start web-server that renders the images. It keeps a history of lots that have been produced and you can scroll back and forth through them.\n> httpgd::hgd()\nhttpgd server running at:\n  http://127.0.0.1:42729/live?token=beRmYcSn\nThen just create your plots and hey-presto the graphs appear at the URL. 🧙\n\n\nrenv\nIn order to have this page publish correctly I had to initialise a renv within the repository and include the renv.lockfile so that the GitHub action/workflow installed all of the packages I use and their dependencies in the runner that renders the blog.\nWhilst I’m familiar with virtual environments under Python this is something relatively new to me for R. I won’t write much other than the process involved initialising the environment, installing the dependencies then updating the lockfile.\n> renv::init()\n> install.packages(c(\"dplyr\", \"ggdark\", \"ggridges\", \"hms\", \"knitr\", \"lubridate\", \"orgutils\", \"plotly\", \"readr\",\n                     \"scales\", \"stringr\"))\n> renv::snapshot()\n> q()\ngit add renv.lockfile"
  },
  {
    "objectID": "posts/running-2022/index.html#conclusion",
    "href": "posts/running-2022/index.html#conclusion",
    "title": "Running in 2022",
    "section": "Conclusion",
    "text": "Conclusion\nIts been a fun exercise digging back into R and learning more about Quarto. If you’ve some data and an urge to summarise it I’d recommend having a play, although since Quarto supports more than just R you could alternatively use Python packages such as Pandas and Matplotlib to achieve the same. Here’s to more running 🏃 and maybe finding the time to write the Shiny dashboard to do a lot of this automatically and on the fly as I log runs in 2023 and beyond."
  },
  {
    "objectID": "posts/running-2022/index.html#links",
    "href": "posts/running-2022/index.html#links",
    "title": "Running in 2022",
    "section": "Links",
    "text": "Links\n\nTraining Summary\nEmacs\nOrgMode\nOrg-babel\nR\nEmacs Speaks Statistics\nQuarto\nquarto-emacs mode\n\nReturn to the top!"
  },
  {
    "objectID": "posts/pre-commit-updates/index.html",
    "href": "posts/pre-commit-updates/index.html",
    "title": "Pre-Commit : Customising and Updating",
    "section": "",
    "text": "Pre-commit is a tool for running hooks prior to making commits to your Git history. If you’re not familiar with it then you may want to read the earlier post Pre-Commit : Protecting your future self. This article discusses updating pre-commit and is prompted by a change in the flake8 repository."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#pre-commit-hooks",
    "href": "posts/pre-commit-updates/index.html#pre-commit-hooks",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Pre-commit hooks",
    "text": "Pre-commit hooks\nA lot of the power of pre-commit comes from the vast array of hooks that are available that users make available. These are included under repos: section of the .pre-commit-config.yaml and typically require a minimum of the repo: and the rev: to use and then optionally a hooks: section. The sample-config that pre-commit will auto-generate looks like…\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\nAfter finding a repository and hook that you wish to use hooks repository you need to add it to your .pre-commit-config.yaml. Here we add the pylint repository and whilst it only has one hook we explicitly add it.\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n-   repo: https://github.com/PyCQA/pylint\n    rev: v2.15.5\n    hooks:\n    -   id: pylint\nIf a repository has more than one hook available then it can be enabled by listing its id: as is the case in the hooks above for the pre-commit-hooks repository."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#local-hooks",
    "href": "posts/pre-commit-updates/index.html#local-hooks",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Local Hooks",
    "text": "Local Hooks\nIn some instances the provisioned repositories do not always meet the requirements. One example of this is the pylint action which parses the code-base to detect errors using pylint. Typically most Python packages have their own dependencies but because the Pylint action pulls down and uses its own virtual environment these packages are not installed. As a consequence pylint reports a lot of import-error as its unable to import the required dependencies.\nThe solution to this is to write a local hook, which instead of defining a GitHub repository as the repo: uses the value local. Thus to run pylint in a local environment from pre-commit you would add the following to your .pre-commit-config.yaml\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n# -   repo: https://github.com/PyCQA/pylint\n#     rev: v2.15.5\n#     hooks:\n#     -   id: pylint\n-   repo: local\n    hooks:\n    -   id: pylint\n        name: PyLint\n        entry: python -m pylint.__main__\n        language: system\n        files: \\.py$\nFor this to work you would have to ensure that you have a virtual environment activated that includes the package dependencies, including pylint, when you make you git commit so that pre-commit can find and import all the required packages."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#updating-pre-commit",
    "href": "posts/pre-commit-updates/index.html#updating-pre-commit",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Updating pre-commit",
    "text": "Updating pre-commit\nAfter adding a new repo and hook it will not be immediately ready to use as the environment has not been initialised. You can wait until your next commit or force this with the autoupdate option. This will update all repositories that are defined in your configuration.\n$ pre-commit autoupdate\nUpdating https://github.com/pre-commit/pre-commit-hooks ... updating v3.2.0 -> v4.3.0.\nUpdating https://github.com/PyCQA/pylint ... [INFO] Initializing environment for https://github.com/PyCQA/pylint.\nalready up to date."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#repository-changes",
    "href": "posts/pre-commit-updates/index.html#repository-changes",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Repository Changes",
    "text": "Repository Changes\nSometimes, albeit rarely, repositories change their location as was the case recently when flake8 moved from GitLab to GitHub. As a consequence any pre-commit that uses flake8 repo/hook and configured to run in Continuous Integration pipelines failed as it was unable to download and run the flake8 environment. The solution is simply to update the repo:.\nBefore this change the entry for flake8 looked like…\n-   repo: https://gitlab.com/pycqa/flake8.git\n    rev: 3.9.2\n    hooks:\n    -   id: flake8\n        additional_dependencies: [flake8-print]\n        args: [\"topostats\", \"tests\"]\n        types: [python]\nTo update to use the new repository it should point to github.com as shown below.\n-   repo: https://github.com/pycqa/flake8.git\n    rev: 3.9.2\n    hooks:\n    -   id: flake8\n        additional_dependencies: [flake8-print]\n        args: [\"topostats\", \"tests\"]\n        types: [python]\nAfter making this change you have to pre-commit autoupdate to force downloading and updating from the new source, otherwise your existing older revision will be used locally."
  },
  {
    "objectID": "posts/pre-commit-updates/index.html#links",
    "href": "posts/pre-commit-updates/index.html#links",
    "title": "Pre-Commit : Customising and Updating",
    "section": "Links",
    "text": "Links\n\nPre-Commit : Protecting your future self\nPre-commit\nPre-commit hooks\npylint\nflake8"
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html",
    "href": "posts/gitlab-ci-pypi/index.html",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "",
    "text": "I’ve written previously on Python Packaging and in that article included details of how to automate publishing to PyPI from GitHub. This article details how to automatically publish your package to PyPI from GitLab."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#repository-configuration",
    "href": "posts/gitlab-ci-pypi/index.html#repository-configuration",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "Repository Configuration",
    "text": "Repository Configuration\n\nCI Variables\nThe environment variables $TWINE_USERNAME (__token__) and $TWINE_PASSWORD which will be the token you generate for publishing on PyPI or Test PyPI. These are saved under the repository _Settings > CI/CD > Varialbes_ section and how to create and save these is described below.\n\n\nProtecting Tags\nThis really stumped me I could build and push automatically from the master branch but could not use the - if $CI_COMMIT_TAG condition to publish commits that were tagged. I wrote a post on the GitLab Forums asking how to do this and posted it to Mastodon asking if anyone had any ideas. I got two replies (one from @manu_faktur@mastodon.social and one from @diazona@techhub.social) both asking if I’d protected the tags on my repository.\nI had no idea that you could protect tags on GitLab (or GitHub for that matter) so looked up the documentation on Protected tags and sure enough this was possible. After setting a wildcard to protect my tags the pypi CI job defined below worked as expected and built on tagged commits."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#ci-configuration",
    "href": "posts/gitlab-ci-pypi/index.html#ci-configuration",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "CI Configuration",
    "text": "CI Configuration\n\nCI\nGitLabs CI/CD is configured via a YAML file .gitlab-ci.yaml in the root of your project folder, a useful reference for writing these files is the .gitlab-ci.yml reference.\nAn example file from the tcx2gpx package is shown below (see here).\nThis defines the following…\n\nimage - the use of a Docker Python 3.11 image for running the pipeline.\nvariables - Configures pre-commit to run and automatically fix issues found on pull requests.\nstages - the subsequent stages to run (NB the debug stage which prints the environment variables is commented out).\npylint - runs linting on Python 3.10 and 3.11.\npytest - Runs tests on Python 3.10 and 3.11.\npages - Builds the documentation pages.\npypi - Builds and uploads the package to PyPI if the commit has a tag associated.\n\n````yaml image: python:3.11\nvariables: # since we’re not using merge request pipelines in this example, # we will configure the pre-commit job to run on branch pipelines only. # If you ARE using merge request pipelines, you can omit this section PRE_COMMIT_AUTO_FIX: ‘1’ PRE_COMMIT_DEDUPLICATE_MR_AND_BRANCH: ‘false’ PRE_COMMIT_AUTO_FIX_BRANCH_ONLY: ‘false’\nbefore_script: - python –version - pip install ."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#testing",
    "href": "posts/gitlab-ci-pypi/index.html#testing",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "Testing",
    "text": "Testing\nNow that you are setup you can test your configuration. To do so you need to first use the API key from the Test PyPI server that you created as the value for $TWINE_PASSWORD (see above) and set the repository twine --repository option to testpypi. Your pypi stage should look like the following…\npypi:\n    stage: pypi\n    rules:\n        - if: $CI_COMMIT_TAG\n    script:\n        - pip install .[pypi]\n        - pip install build\n        - python -m build\n        - twine upload --non-interactive --repository testpypi dist/*\nOnce this is set create a tag for the current commit using the Code > Tags settings from the left menu of your repository and then the New tag button on the top right. The tag you create should match the wild card pattern you have set for protecting tags and it should comply to the Public version identifiers specified in PEP440. Once created you will have triggered your Pipeline and you can check this by navigating to CI/CD > Pipelines and then viewing it. The pypi job should complete and you should be able to navigate to your package on Test PyPI. You can find it under your account settings.\nIf you find there is a problem you will have to correct it and either delete the tag you created and try again or increment the version. PyPI, and in turn Test PyPI which is a mirror with the same functionality, does not permit uploading packages with a version that already exists."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#publishing-to-pypi",
    "href": "posts/gitlab-ci-pypi/index.html#publishing-to-pypi",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "Publishing to PyPI",
    "text": "Publishing to PyPI\nOnce you have successfully published to the Test PyPI you are ready to publish to PyPI. There three things you need to do.\n\nDelete the existing tag, if you want to apply the same tag to publish to PyPI you can do so.\nModify the repository option to point to PyPI --repository pypi (or remove it, the default is PyPI).\nChange the key stored in the $TWINE_PASSWORD to that which you generated for PyPI instead of the one used for testing with Test PyPI.\n\nOnce you have done so you can create a new tag and the upload will be made to PyPI.\n\nReleases\nAn alternative way to apply tags to commits is to make a Releases. In creating a release you apply a tag to the current commit. In addition GitLab will build and compress snapshot of the files and you can add Release Notes detailing what has changed."
  },
  {
    "objectID": "posts/gitlab-ci-pypi/index.html#links",
    "href": "posts/gitlab-ci-pypi/index.html#links",
    "title": "GitLab CI - Automatic Publishing to PyPI",
    "section": "Links",
    "text": "Links\n\nPython Packaging\n\nPyPA : Building and Distributing Packages with Setuptools\nPyPA : Specifications\nPackaging Python Projects\nPython package structure information — pyOpenSci Python Packaging Guide\n\n\n\nGitLab Documentation\n\nUse CI/CD to build your application | GitLab\nThe .gitlab-ci.yml file | GitLab +\nProtected tags"
  },
  {
    "objectID": "posts/pre-commit-r/index.html",
    "href": "posts/pre-commit-r/index.html",
    "title": "Pre-commit and R Packaging",
    "section": "",
    "text": "This post is aimed at getting you up and running with the R precommit Package. This shouldn’t be confused with the Python pre-commit Package although as you might suspect they are closely related.\nThe R package (precommit) consists of a number of R specific hooks that are run by pre-commit before commits are made and check various aspects of your code for compliance with certain style and coding standards (mostly aspects of R packages and I’ll be posting more on R packaging in due course).\nA major part of this post is about getting things setup on Windows, I’ve only given a light overview of some of the hooks and common problems encountered as I’ve gone about using and learning R packaging because not everyone uses Windows.\nMost of the work on the R package is by Lorenz Walthert, if you find it useful consider sponsoring his work, these things take a lot of time and effort and whilst they can be used for free are worth supporting."
  },
  {
    "objectID": "posts/pre-commit-r/index.html#pre-commit",
    "href": "posts/pre-commit-r/index.html#pre-commit",
    "title": "Pre-commit and R Packaging",
    "section": "pre-commit",
    "text": "pre-commit\nI love using pre-commit in my development pipelines and have blogged about it a few times already. It saves so much hassle (once you are used to it) not just for yourself but also your collaborators who are reviewing your Pull Requests. The R precommit package comes with a set of hooks that can be enabled and configured individually. I’ve recently and reason to start making and R package and as I’ve not used R much for a few years and this was my first time attempting to develop a package, I decided to use the hooks to impose the various style standards and checks that are expected.\nI opted to enable all of the hooks. I’ve not covered them all here in detail (yet) but describe some of them below and show how to use some additional hooks from the usethis package too.\n\ncodemetar\nThere is a hook for checking the Codemeta, which is in JSON-LD format is created correctly. The R package codemetar facilitates creating this and pulls metadata from the DESCRIPTION, README.Rmd and other aspects of your package to format them in JSON Codemeta. It comes with a handy function to write the file for you, so after installing you can just run codemetar::write_codemeta() which will create the codemeta.json for you. Remember to run this each and every time you update and of the files from which the metadata is created (although keep an eye on #491 which suggests updating automatically)\n\n\nroxygenize\nRoxygen2 is a package for making the documentation to go with your package, it does this by parsing the documentation strings (“docstrings” for short) that you adorn your functions with that describe the arguments and show example usages. This hook requires additional configuration in .pre-commit-config.yaml as you have to install your package dependencies. Fortunately there is a helper function in the precommit package so you can just run precommit::snippet_generate(\"additional-deps-roxygenize\") and it will output the YAML that you need to add to your .pre-commit-config.yaml. It might look something like the following.\n    hooks:\n    - id: no-debug-statement\n    - id: roxygenize\n      additional_dependencies:\n        -    data.table\n        -    dplyr\n        -    dtplyr\n        -    duckdb\n        -    IMD\n        -    lubridate\n        -    stringr\n\n\nstyle-files\nThe style-files hook runs the styler package against your code to ensure it follows the tidyverse style guide by default, although it can be configured to use a custom style guide of your own creation.\n\n\nlintr\nThe lintr package lints your code automatically. It can be configured by adding a .lintr configuration file to your repository, a simple example is shown below. Note the indented closing parenthesis is important you get a complaint about that and any other formatting issues.\nlinters: linters_with_defaults(\n         line_length_linter(120),\n         object_name_linter = NULL,\n         object_usage_linter = NULL\n  )\n\n\nspell-check\nThis is a useful hook that checks your spelling and adds unusual words to a custom dictionary inst/WORDLIST.\n\n\ndeps-in-desc\nThis hook ensures that all dependencies that are loaded by your package are listed in the DESCRIPTION file so that when the package is installed the necessary dependencies are also pulled in, fairly essential.."
  },
  {
    "objectID": "posts/pre-commit-r/index.html#usethis-package",
    "href": "posts/pre-commit-r/index.html#usethis-package",
    "title": "Pre-commit and R Packaging",
    "section": "usethis package",
    "text": "usethis package\nThe usethis package is a compliment to the devtools package that has a lot of very useful helper functions. Some of these enable additional pre-commit hooks whilst others enable GitHub actions, which are part of Continuous Integration piplelines and I would highly recommend enabling them.\n\nREADME.Rmd\nThe user_readme_rmd() function automatically generates a README.Rmd template and will also create a pre-commit hook that keeps it synchronised with README.md whenever you update it. This is useful because the later, plain-markdown, file is automatically rendered by GitHub/GitLab/Codeberg as your repositories front-page.\n\n\nuse_github_action()\nInvoking use_github_action() within your package repository will prompt you for the type of action you wish to add to it. There are, as of writing, three options.\n    > use_github_action()\n    Which action do you want to add? (0 to exit)\n    (See <https://github.com/r-lib/actions/tree/v2/examples> for other options)\n\n    1: check-standard: Run `R CMD check` on Linux, macOS, and Windows\n    2: test-coverage: Compute test coverage and report to https://about.codecov.io\n    3: pr-commands: Add /document and /style commands for pull requests\nSelecting one will write a file to /.github/workflows/<FILENAME>.yaml and then print out code to add a badge to your repository.\nSelection: 1\n    ✔ Adding '*.html' to '.github/.gitignore'\n    ✔ Creating '.github/workflows/'\n    ✔ Saving 'r-lib/actions/examples/check-standard.yaml@v2' to '.github/workflows/R-CMD-check.yaml'\n    • Learn more at <https://github.com/r-lib/actions/blob/v2/examples/README.md>.\n    • Copy and paste the following lines into 'README.Rmd':\n      <!-- badges: start -->\n      [![R-CMD-check](https://github.com/CUREd-Plus/cuRed/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/CUREd-Plus/cuRed/actions/workflows/R-CMD-check.yaml)\n      <!-- badges: end -->\n      [Copied to clipboard]\n\n\nBadges\nMost of the GitHub Action functions described above include output that can be copy and pasted into README.Rmd to include badges in your GitHub front page. Again the usethis has you covered and can generate the necessary code for the different badges it supports."
  },
  {
    "objectID": "posts/pre-commit-r/index.html#gotchas",
    "href": "posts/pre-commit-r/index.html#gotchas",
    "title": "Pre-commit and R Packaging",
    "section": "Gotchas",
    "text": "Gotchas\nWhen starting out I found that I regularly didn’t pass the pre-commit hooks first time. This can be jarring and confusing to start with but its not something to worry about, they are there to ensure your code and package meet the standards required. If you ever come to submit to CRAN you will be grateful to have adhered to these standards.\nBelow I detail common “gotchas” I encountered when developing the package, what they mean and how to resolve them.\n\nThe following spelling errors were found:\nThe spell-check hook will fail if you’ve introduced new words that aren’t in standard dictionaries with messages similar to the those shown below. Sometimes these will be new words, sometimes they might be catching tpyos you have made. In the example below famiy should be family so you need to correct the source of the tpyo (and you’re told where this is, in this case it was line 27 of CITATION.cff), or if the new word should be added to the dictionary you will have to stage the updated inst/WORDLIST file for inclusion in your commit.\nspell-check..............................................................Failed\n- hook id: spell-check\n- exit code: 1\n- files were modified by this hook\n\nℹ Using R 4.3.1 (lockfile was generated with R 4.2.1)\nℹ Using R 4.3.1 (lockfile was generated with R 4.2.1)\nThe following spelling errors were found:\n  WORD    FOUND IN\nfamiy   CITATION.cff:27\nAll spelling errors found were copied to inst/WORDLIST assuming they were not spelling errors and will be ignored in the future. Please  review the above list and for each word that is an actual typo:\n - fix it in the source code.\n - remove it again manually from inst/WORDLIST to make sure it's not\n   ignored in the future.\n Then, try committing again.\nError: Spell check failed\nExecution halted\n\n\n! codemeta.json is out of date\nIf you modify the DESCRIPTION or CITATION.cff then the codemeta-description-updated hook will fail with error messages similar to the following.\ncodemeta-description-updated.............................................Failed\n- hook id: codemeta-description-updated\n- exit code: 1\n\nℹ Using R 4.3.1 (lockfile was generated with R 4.2.1)\nℹ Using R 4.3.1 (lockfile was generated with R 4.2.1)\nError:\n! codemeta.json is out of date; please re-run codemetar::write_codemeta().\nBacktrace:\n    ▆\n 1. └─rlang::abort(\"codemeta.json is out of date; please re-run codemetar::write_codemeta().\")\nExecution halted\nThis means yo need to update the codemeta.json with\ncodemetar::write_codemeta()\n\n\nWarning: Undocumented code objects:\nIf this error arises its because there is a .Rd file missing. You can generate these by ensuring you have the appropriate docstring definition prior to your function and then use the roxygen2::reoxygenise() function to generate the documentation automatically. Don’t forget to git stage and git commit the files to your repository, pushing if needed (e.g. a Continuous Integration pipeline is failing)."
  },
  {
    "objectID": "posts/pre-commit-r/index.html#windows",
    "href": "posts/pre-commit-r/index.html#windows",
    "title": "Pre-commit and R Packaging",
    "section": "Windows",
    "text": "Windows\nI haven’t used Windows for about 23 years but I often have colleagues who do and that was the case with the R package that I have started developing so I needed to get all members of the team up and running with the precommit R package/pipeline.\nWindows doesn’t come with Python by default, but pre-commit is written in Python and so an environment is required in order to run the above pre-commit hooks. There are many options for this, including using Windows Subsystem for Linux (WSL). I opted to try the solution provided in the precommit vignette. This shows how to use the reticulate package which acts as a glue between R and Python, to handle installing a Miniconda environment and setting up precommit/pre-commit.\nThe following runs you through the things you need to install (R, RStudio, GitBash), setting up GitHub with SSH keys and enabling precommit for your R package locally.\n\nInstall R\nWhen installing the defaults are fine, request admin permissions if required.\n\n\nInstall Rstudio\nDefaults are fine, request admin permissions if required.\n\n\nInstall GitBash\nDuring installation you’ll be asked a number of questions, if you’re unsure how to respond to any of them the following provides guidance.\n\nText Editor - Configure with your choice of editor, obviously you’ll want to have Emacs available and select that! 😉\nAdjust your PATH environment - At the bare minimum go with the Recommended option and allow Git from the command line and also from 3rd-party software. Optionally I would recommend the third option of Use Git and optional UNIX tools from the Command Prompt, particularly if you are either a) familiar with UNIX commands or b) not at all familiar with them (as you won’t have to re-learn the Windows commands, just learn the Bash commands they are more widely applicable).\nUse Bundled SSH library - Use the bundled SSH library.\nUse Bundled OpenSSL library - Use the bundled OpenSSL library.\nCheckout Windows-style, commit Unix-style line endings - This is fine, it just changes the internal representation of the carriage return to be more universal.\nUse MinTTY - The default terminal of MSYS2 is fine and more functional than the Windows’ default console window.\nDefault Merge behaviour - The default (fast-forward or merge) this is fine.\nChoose a credential helper - Select None here, we will let RStudio manage these.\nConfigure Extra Options - Defaults are fine.\nConfiguring experimental options - No need to enable any of these.\n\n\nConfigure Git\nStart a GitBash shell and configure your email address and name.\ngit config --global user.email \"you@example.com\"\ngit config --global user.name \"Your Name\"\n\n\n\nConfigure RStudio/GitHub with SSH keys\n\nStart RStudio\nCreate SSH key - Navigate to Tools > General Options > Git/SVN > Create SSH Key and under SSH key type select the default (ED25519) this is a very secure elliptic curve algorithm and is supported by GitHub. Use a secure password (i.e. long), do not change the location it is created at.\nOnce created select View public key and use Ctrl + c to copy this to your clipboard.\nNavigate to GitHub and login then click on your avatar in the top right and select Settings > SSH and GPG keys > New SSH Key.\nGive the key a name and paste into the box below where indicated/instructed then click on Add SSH key.\n\n\n\nClone Repository\nIts likely that you will have an existing repository that you wish to work on with this pipeline, if so you will have to clone it locally so you can work on it with the precommit pipeline. The following assumes you have added your SSH key to your GitHub account as described above.\n\nNavigate to the repository you wish to clone (e.g. https://github.com/CUREd-Plus/cuRed/) and click on the Code button then select SSH under the Local tab in the box that appears.\nClick on the box that has two small squares to the right of some text to copy the URL to clipboard.\nReturn to RStudio and start a new project with File > New Project > Version Control > Git and paste the URL into the Repository URL. Select a location to clone to under Create project as subdirectory of:, e.g. c:/Users/<username>/work/cuRed (replacing <username> with your username).\nIf prompted for password enter it. If asked to answer Yes\\/No answer Yes and then if prompted to Store password for this session answer Yes.\nYou should now have cloned the repository and have a project to work on.\n\n\n\nInstall pre-commit\nAs mentioned above pre-commit refers to two things, primarily it is the Python package pre-commit that does all the work of running Linting, Tests etc. before making commits. It also refers to an R package precommit (note the omission of the hyphen -) that works with the Python package to enable use of various R packages that carry out such checks. Because it is a Python package it needs a Python Virtual Environment to run. This may sound unfamiliar but don’t worry the R precommit package and documentation guides you through doing so, what follows is a rehash of the official documentation.\n\nInstall precommit and reticulate\nFrom RStudio install the remotes and reticulate package, then install the most recent version of precommit directly from GitHub.\ninstall.packages(c(\"remotes\", \"reticulate\"))\nremotes::install_github(\"lorenzwalthert/precommit\")\n\n\nInstall Miniconda environment\nYou can now use reticulate to install a Miniconda virtual environment framework for R to run Python packages (i.e. pre-commit).\noptions(timeout=600)\nreticulate::install_miniconda()\n\n\nInstall pre-commit framework\nThis step now installs the Python package pre-commit within a new Miniconda virtual environment (by default r-precommit). There will be a fair bit of output here as all the dependencies in Python for pre-commit are downloaded.\nprecommit::install_precommit()\nprecommit::autoupdate()\n\n\nUse precommit with the existing project\nYou should have cloned the repository you wish to enable precommit to use (see above). You now need to enable precommit for this local copy of the repository. This will place a script in .git/hooks/pre-commit that says which Miniconda environment to use (r-precommit) and will activate this whenever a commit is made, the install_hooks = TRUE ensures that the R specific hooks and their required environments are installed (under \\~/.config/pre-commit/).\nMake sure you have opened the .Rproj file in RStudio, this ensures you are within the project directory that you want to install precommit to (alternatively used setwd()).\nprecommit::use_precommit(install_hooks = TRUE)"
  },
  {
    "objectID": "posts/pre-commit-r/index.html#links",
    "href": "posts/pre-commit-r/index.html#links",
    "title": "Pre-commit and R Packaging",
    "section": "Links",
    "text": "Links\n\nR Packages book by Hadley Wickham and Jenny Bryan\nHappy Git and GitHub for the userR by Jenny Bryan\n\n\nR Packages\n\ndevtools\nusethis\nroxygen2\nR precommit\n(GitHub | lorenzwalthert/precommit check the Issues, can be useful for troubleshooting.\n\n\n\nPython\n\nMiniconda\n\n\n\nPre-commit\n\npre-commit\npre-commit.ci\nns-rse | pre-commit blog posts I’ve made about pre-commit."
  },
  {
    "objectID": "posts/cli-alternatives/index.html",
    "href": "posts/cli-alternatives/index.html",
    "title": "Linux Command Line Alternatives",
    "section": "",
    "text": "The command line is my second home when sat at a computer (Emacs is my first ;-) and the UNIX Philosophy is the key to the huge amount of highly productive tools that are available under UNIX, GNU/Linux, BSD, OSX, PowerShell etc.\nMany of these tools work and have done for many years, but there are some new alternatives that are coming through that build and modernise on these tools without breaking the core functionality. Here I detail some of the tools and why you might want to use them. Each tool has a brief introduction with some example output shown and then some aliases listed that you can drop into ~/.bash_aliases or ~/.oh-my-zsh/custom/aliases to use on your system."
  },
  {
    "objectID": "posts/cli-alternatives/index.html#alternatives",
    "href": "posts/cli-alternatives/index.html#alternatives",
    "title": "Linux Command Line Alternatives",
    "section": "Alternatives",
    "text": "Alternatives\n\nbat\nbat is “A cat(1) clone with wings.”. It automatically uses syntax highlighting and integrates with git if a file is version controlled to show changes and lots more. You can pipe input to it, including from e.g. curl -s https://server.com/some_file\n\nExample - bat\n❱ bat pyproject.toml\n───────┬──────────────────────────────────────────────\n       │ File: pyproject.toml\n───────┼──────────────────────────────────────────────\n   1   │ [build-system]\n   2   │ requires = [\n   3   │   \"setuptools\",\n   4   │   \"versioneer==0.26\",\n   5   │   \"wheel\"]\n   6   │ build-backend = \"setuptools.build_meta\"\n   7   │\n   8   │ [tool.black]\n   9   │ line-length = 120\n  10   │ target-version = ['py38', 'py39', 'py310']\n  11   │ include = '\\.pyi?$'\n───────┴──────────────────────────────────────────────\n\n\nConfiguration\nYou can generate a default configuration file with\nbat --generate-config-file\nThis will be saved at ~/.config/bat/config and you can edit it as desired.\n\n\n\ncheat\ncheat is actually a web-service that returns short “cheats” for command line programmes which will often cover many use cases and save you having to read the rather dry man pages for functions.\n\nExample - cheat\n❱ cheat cheat\n cheat:cheat\n# To see example usage of a program:\ncheat <command>\n\n# To edit a cheatsheet\ncheat -e <command>\n\n# To list available cheatsheets\ncheat -l\n\n# To search available cheatsheets\ncheat -s <command>\n\n# To get the current `cheat' version\ncheat -v\n\n tldr:cheat\n# cheat\n# Create and view interactive cheat sheets on the command-line.\n# More information: <https://github.com/cheat/cheat>.\n\n# Show example usage of a command:\ncheat command\n\n# Edit the cheat sheet for a command:\ncheat -e command\n\n# List the available cheat sheets:\ncheat -l\n\n# Search available the cheat sheets for a specified command name:\ncheat -s command\n\n# Get the current cheat version:\ncheat -v\n\n\nAliases - cheat\nYou don’t need to install anything to use this, instead define an alias for your shell (e.g. in ~/.bashrc/~/.zshrc/~/.oh-my-zsh/custom/aliases.zsh).\n## Linux commands https://github.com/chubin/cheat.sheets\ncheat () {\n    curl cheat.sh/\"$@\"\n}\n\n\n\ndifftastic\ndifftastic (GitHub) is an alternative to the default GNU diff packaged with most systems. It is “aware” of some 30 or so programming languages and will show diffs side-by-side rather than the traditional linear manner. It integrates easily with Git so when you git diff it uses difft to show the differences.\nHighly recommended, but don’t take my word for it, give it a whirl yourself.\n\n\nduf\nduf is a nice alternative to the traditional du and df commands which report disk usage and file/directory usage respectively.\n\nExamples\n❱ tldr duf\n\n  duf\n\n  Disk Usage/Free Utility.\n  More information: https://github.com/muesli/duf.\n\n  - List accessible devices:\n    duf\n\n  - List everything (such as pseudo, duplicate or inaccessible file systems):\n    duf --all\n\n  - Only show specified devices or mount points:\n    duf path/to/directory1 path/to/directory2 ...\n\n  - Sort the output by a specified criteria:\n  duf --sort size|used|avail|usage\n\n❱ duf\n╭──────────────────────────────────────────────────────────────────────────────────────────────╮\n│ 4 local devices                                                                              │\n├────────────┬────────┬───────┬────────┬───────────────────────────────┬──────┬────────────────┤\n│ MOUNTED ON │   SIZE │  USED │  AVAIL │              USE%             │ TYPE │ FILESYSTEM     │\n├────────────┼────────┼───────┼────────┼───────────────────────────────┼──────┼────────────────┤\n│ /          │  19.5G │  9.5G │   9.0G │ [#########...........]  48.9% │ ext4 │ /dev/mmcblk0p2 │\n│ /boot      │ 199.8M │ 38.8M │ 161.0M │ [###.................]  19.4% │ vfat │ /dev/mmcblk0p1 │\n│ /home      │   9.3G │  3.7G │   5.0G │ [########............]  40.4% │ ext4 │ /dev/mmcblk0p3 │\n│ /mnt/usb   │   4.5T │  3.2T │   1.1T │ [##############......]  71.3% │ ext4 │ /dev/sda1      │\n╰────────────┴────────┴───────┴────────┴───────────────────────────────┴──────┴────────────────╯\n╭───────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ 6 special devices                                                                                 │\n├────────────────┬────────┬────────┬────────┬───────────────────────────────┬──────────┬────────────┤\n│ MOUNTED ON     │   SIZE │   USED │  AVAIL │              USE%             │ TYPE     │ FILESYSTEM │\n├────────────────┼────────┼────────┼────────┼───────────────────────────────┼──────────┼────────────┤\n│ /dev           │   3.7G │     0B │   3.7G │                               │ devtmpfs │ dev        │\n│ /dev/shm       │   3.9G │     0B │   3.9G │                               │ tmpfs    │ tmpfs      │\n│ /run           │   3.9G │ 812.0K │   3.9G │ [....................]   0.0% │ tmpfs    │ run        │\n│ /run/user/1001 │ 789.3M │  20.0K │ 789.3M │ [....................]   0.0% │ tmpfs    │ tmpfs      │\n│ /run/user/966  │ 789.3M │  24.0K │ 789.2M │ [....................]   0.0% │ tmpfs    │ tmpfs      │\n│ /tmp           │   3.9G │   4.0K │   3.9G │ [....................]   0.0% │ tmpfs    │ tmpfs      │\n╰────────────────┴────────┴────────┴────────┴───────────────────────────────┴──────────┴────────────╯\n\n\n\nfd\nfd is an alternative to find that is easier to use. It is “opinionated” (i.e. decisions have been made about default options that you may not agree with) but purportedly covers ~80% of use cases. It works directly with regular expressions.\n\nExample - fd\n❱ tldr fd\n\n  fd\n\n  An alternative to `find`.\n  Aims to be faster and easier to use than `find`.\n  More information: https://github.com/sharkdp/fd.\n\n  - Recursively find files matching the given pattern in the current directory:\n    fd pattern\n\n  - Find files that begin with \"foo\":\n    fd '^foo'\n\n  - Find files with a specific extension:\n    fd --extension txt\n\n  - Find files in a specific directory:\n    fd pattern path/to/directory\n\n  - Include ignored and hidden files in the search:\n    fd --hidden --no-ignore pattern\n\n  - Execute a command on each search result returned:\n    fd pattern --exec command\n\n\n\njq\njq is to JSON (JavaScript Object Notation) what awk/grep/sed is to text files. It allows parsing, searching and selecting of JSON files, which if you’ve not encountered them before take a bit of getting used to.\n\nExample - jq\nDetails of using jq are really beyond the scope of this short article, like awk its almost a language in itself.\n❱ tldr jq\n\n  jq\n\n  A command-line JSON processor that uses a domain-specific language.\n  More information: https://stedolan.github.io/jq/manual/.\n\n  - Execute a specific expression (print a colored and formatted json):\n    cat path/to/file.json | jq '.'\n\n  - Execute a specific script:\n    cat path/to/file.json | jq --from-file path/to/script.jq\n\n  - Pass specific arguments:\n    cat path/to/file.json | jq --arg \"name1\" \"value1\" --arg \"name2\" \"value2\" ... '. + $ARGS.named'\n\n  - Print specific keys:\n    cat path/to/file.json | jq '.key1, .key2, ...'\n\n  - Print specific array items:\n    cat path/to/file.json | jq '.[index1], .[index2], ...'\n\n  - Print all array items/object keys:\n    cat path/to/file.json | jq '.[]'\n\n  - Add/remove specific keys:\n    cat path/to/file.json | jq '. +|- {\"key1\": \"value1\", \"key2\": \"value2\", ...}'\n\n\n\nlsd\nlsd is lsDeluxe and is very similar to exa but with a few additions such as icons.\n\nExample - lsd\n❱ l\n.rw-r--r-- neil neil  144 B  Sun Aug 14 19:56:53 2022  #.gitlab-ci.yml#\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:21:25 2022  .\ndrwxrwxr-x root users 4.0 KB Tue Aug 30 20:46:37 2022  ..\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:21:56 2022  .git\ndrwxr-xr-x neil neil  4.0 KB Sun Aug 14 21:51:03 2022  .github\n.rw-r--r-- neil neil  613 B  Sun Aug 14 21:44:38 2022  .gitignore\n.rw-r--r-- neil neil  151 B  Sun Aug 14 19:56:13 2022  .gitlab-ci.yml\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:21:25 2022  .quarto\n.rw-r--r-- neil neil  386 B  Thu Sep 15 22:05:23 2022  _quarto.yaml\n.rw-r--r-- neil neil  263 B  Sun Aug 14 10:59:13 2022  _quarto.yml~\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:05:24 2022  _site\n.rw-r--r-- neil neil  1.1 KB Thu Sep 15 22:05:23 2022  about.qmd\n.rw-r--r-- neil neil  455 B  Sun Aug 14 11:02:13 2022  about.qmd~\ndrwxr-xr-x neil neil  4.0 KB Thu Sep 15 22:05:23 2022  img\n.rw-r--r-- neil neil  185 B  Sun Aug 14 22:22:04 2022  index.qmd\n.rw-r--r-- neil neil  191 B  Sun Aug 14 10:59:13 2022  index.qmd~\n.rw-r--r-- neil neil   34 KB Sun Aug 14 21:14:38 2022  LICENSE\n.rw-r--r-- neil neil  1.7 KB Thu Sep 15 22:05:23 2022  links.qmd\n.rw-r--r-- neil neil  237 B  Thu Sep 15 21:46:30 2022  links.qmd~\ndrwxr-xr-x neil neil  4.0 KB Wed Sep 14 20:24:25 2022  posts\n.rw-r--r-- neil neil  378 B  Thu Aug 25 23:20:16 2022  README.md\n.rw-r--r-- neil neil   13 B  Sun Aug 14 21:58:38 2022  requirements.txt\n.rw-r--r-- neil neil   17 B  Sun Aug 14 21:24:35 2022  styles.css\ndrwxr-xr-x neil neil  4.0 KB Thu Aug 25 23:20:16 2022  www\n\n\nAliases - lsd\nalias ls='lsd'\nalias l='ls -lha'\nalias lla='ls -la'\nalias lt='ls --tree'\n\n\n\ntldr\ntldr is very similar to cheat in that it shows short, simple examples of using a command. There are a number of different clients written in C, Node and Python as well as a few others. It depends on jq so you will have to install that if you want to use yq.\n\nExample - tldr\n❱ tldr tldr\n\n  tldr\n\n  Display simple help pages for command-line tools from the tldr-pages project.\n  More information: https://tldr.sh.\n\n  - Print the tldr page for a specific command (hint: this is how you got here!):\n    tldr command\n\n  - Print the tldr page for a specific subcommand:\n    tldr command-subcommand\n\n  - Print the tldr page for a command for a specific [p]latform:\n    tldr -p android|linux|osx|sunos|windows command\n\n  - [u]pdate the local cache of tldr pages:\n    tldr -u\n\n\n\nyq\nyq is to YAML (YAML Ain’t Markup Language) what jq is to JSON. Written in Python it allows fast and efficient parsing, searching and selecting of YAML files.\n\nExample - yq\n❱ tldr yq\n\n  yq\n\n  A lightweight and portable command-line YAML processor.\n  More information: https://mikefarah.gitbook.io/yq/.\n\n  - Output a YAML file, in pretty-print format (v4+):\n    yq eval path/to/file.yaml\n\n  - Output a YAML file, in pretty-print format (v3):\n    yq read path/to/file.yaml --colors\n\n  - Output the first element in a YAML file that contains only an array (v4+):\n    yq eval '.[0]' path/to/file.yaml\n\n  - Output the first element in a YAML file that contains only an array (v3):\n    yq read path/to/file.yaml '[0]'\n\n  - Set (or overwrite) a key to a value in a file (v4+):\n    yq eval '.key = \"value\"' --inplace path/to/file.yaml\n\n  - Set (or overwrite) a key to a value in a file (v3):\n    yq write --inplace path/to/file.yaml 'key' 'value'\n\n  - Merge two files and print to stdout (v4+):\n    yq eval-all 'select(filename == \"path/to/file1.yaml\") * select(filename == \"path/to/file2.yaml\")' path/to/file1.yaml path/to/file2.yaml\n\n  - Merge two files and print to stdout (v3):\n    yq merge path/to/file1.yaml path/to/file2.yaml --colors"
  },
  {
    "objectID": "posts/cli-alternatives/index.html#installation",
    "href": "posts/cli-alternatives/index.html#installation",
    "title": "Linux Command Line Alternatives",
    "section": "Installation",
    "text": "Installation\nMost of these programmes will be available in your systems package manager, if they are not you should consult the project page directly for install instructions.\n\nLinux\n# Gentoo\nemerge -av bat duf fd jq lsd tldr yq\n\n# Arch\npacman -Syu bat duf fd jq lsd tldr yq\n\n# Ubuntu\nsudo apt-install bat duf fd jq lsd tldr yq\n\n\nOSX\nbrew install bat duf fd jq lsd tldr yq\n\n\nWindows\nWARNING None of these have been tested I do not have access to a Windows system running PowerShell. They use Scoop a command-line installer for Windows.\nscoop install lsd"
  },
  {
    "objectID": "posts/cli-alternatives/index.html#links",
    "href": "posts/cli-alternatives/index.html#links",
    "title": "Linux Command Line Alternatives",
    "section": "Links",
    "text": "Links\n\nbat\ncheat\nduf\nexa\nfd\njq\nlsd\ntldr\nyq"
  },
  {
    "objectID": "posts/pre-commit/index.html",
    "href": "posts/pre-commit/index.html",
    "title": "Pre-Commit : Protecting your future self",
    "section": "",
    "text": "Pre-commit is a powerful tool for executing a range of hooks prior to making commits to your Git history. This is useful because it means you can automatically run a range of linting tools on your code across an array of languages to ensure your code is up-to-scratch before you make the commit."
  },
  {
    "objectID": "posts/pre-commit/index.html#background",
    "href": "posts/pre-commit/index.html#background",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Background",
    "text": "Background\nPre-commit is written in Python but that isn’t a limitation as it will lint YAML, JSON, C, JavaScript, Go, Rust, TOML, Terraform, Jupyter Notebooks, and so on. The list of supported hooks is vast.\nFor those unfamiliar with version control and Git in particular this will likely all sound alien. If you are new to the world of version control and Git I can highly recommend the Git & Github through GitKraken Client - From Zero to Hero! course offered by the Research Software Engineering at the University of Sheffield and developed by Alumni Anna Krystalli.\n\nWhat is a “hook”?\nIn computing a “hook” refers to something that is run prior to or in response to a requested action. In the context of the current discussion we are talking about hooks that relate to actions undertaken in Git version control and specifically actions that are run before a “commit” is made.\nWhen you have initialised a directory to be under Git version control the settings and configuration are stored in the .git/ sub-directory. There is the .git/config file for the repositories configuration but also the .git/hooks/ directory that is populated with a host of *.sample files with various different names that give you an in-road into what different hooks you might want to run. Its worth spending a little time reading through these if you haven’t done so yet as they provide useful examples of how various hooks work.\n\n\nWhy pre-commit hooks?\nTypically when writing code you should lint your code to ensure it conforms to agreed style guides and remove any “code smells” that may be lingering (code that violates design principles). It won’t guarantee that your code is perfect but its a good starting point to improving it. People who write a lot of code have good habits of doing these checks manually prior to making commits. Experienced coders will have configured their Integrated Development Environment (IDE) to apply many such “hooks” on saving a file they have been working on.\nAt regular points in your workflow you save your work and check it into Git by making a commit and that is where pre-commit comes in to play because it will run all the hooks it has been configured to run against the files you are including in your commit. If any of the hooks fail then your commit is not made. In some cases pre-commit will automatically correct the errors (e.g. removing trailing white-space; applying black formatting if configured) but in others you have to correct them yourself before a commit can be successfully made.\nInitially this can be jarring, but it saves you, and more importantly those who you are asking to review your code, time and effort. Your code meets the required style and is a little bit cleaner before being sent out for review. Long term linting your code is beneficial (see Linting - What is all the fluff about?)."
  },
  {
    "objectID": "posts/pre-commit/index.html#installation",
    "href": "posts/pre-commit/index.html#installation",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Installation",
    "text": "Installation\nPre-commit is written in Python and so you will need Python installed on your system in order to use it. Aside from that there is little else extra that is required to be manually installed as pre-commit installs virtual environments specific for each enabled hook.\nMost systems provide pre-commit in their package management system but typically you should install pre-commit within your virtual environment or under your user account.\npip install pre-commit\nconda install -c conda-forge pre-commit\nIf you are working on a Python project then you should include pre-commit as a requirement (either in requirements-dev.txt) or under the dev section of [options.extras_require] in your setup.cfg as shown below.\n[options.extras_require]\ndev =\n  pre-commit\n  pytest\n  pytest-cov"
  },
  {
    "objectID": "posts/pre-commit/index.html#configuration",
    "href": "posts/pre-commit/index.html#configuration",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Configuration",
    "text": "Configuration\nConfiguration of pre-commit is via a file in the root of your Git version controlled directory called .pre-commit-config.yaml. This file should be included in your Git repository, you can create a blank file or pre-commit can generate a sample configuration for you.\n# Empty configuration\ntouch .pre-commit-config.yaml\n# Auto-generate basic configuration\npre-commit sample-config > .pre-commit-config.yaml\ngit add .pre-commit-config.yaml\n\nHooks\nEach hook is associated with a repository (repo) and a version (rev) within it. Many are available from the https://github.com/pre-commit/pre-commit-hooks. The default set of pre-commit hooks might look like the following.\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n      rev: v4.3.0 # Use the ref you want to point at\n      hooks:\n          - id: trailing-whitespace\n            types: [file, text]\n          - id: check-docstring-first\n          - id: check-case-conflict\n          - id: end-of-file-fixer\n            types: [python]\n          - id: requirements-txt-fixer\n          - id: check-yaml\n\n\nHooks from External Repositories\nSome hooks are available from dedicated repositories, for example the following runs Black, Flake8 and Pylint on your code and should follow under the above (with the same level of indenting to be valid YAML).\n  - repo: https://github.com/psf/black\n    rev: 22.6.0\n    hooks:\n        - id: black\n          types: [python]\n\n  - repo: https://gitlab.com/pycqa/flake8.git\n    rev: 3.9.2\n    hooks:\n        - id: flake8\n          additional_dependencies: [flake8-print]\n          types: [python]\n  - repo: https://github.com/pycqa/pylint\n    rev: v2.15.3\n    hooks:\n        - id: pylint\nAn extensive list of supported hooks is available. It lists the repository from which the hook is derived along with its name.\n\n\nLocal Hooks\nYou can also define new hook and configure them under the - repo: local.\n  - repo: local\n    hooks:\n      - id: <id>\n        name: <descriptive name>\n        language: python\n        entry:\n        types: [python]\nFor some examples of locally defined hooks see the Pandas .pre-commit-config.yaml."
  },
  {
    "objectID": "posts/pre-commit/index.html#usage",
    "href": "posts/pre-commit/index.html#usage",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Usage",
    "text": "Usage\nBefore pre-commit will run you need to install it within your repository. This puts the file .git/hooks/pre-commit in place that contains the hooks you have configured to run. To install this you should have your .pre-commit-config.yaml in place and then run the following.\npre-commit install\nOnce installed and configured there really isn’t much to be said for using pre-commit, just make commits and before you can make a successful commit pre-commit must run with all the hooks you have configured passing. By default pre-commit only runs on files that are staged and ready to be committed, if you have unstaged files these will be stashed prior to running the pre-commit hook and restored afterwards. Should you wish to run these manually without making a commit then, after activating a virtual environment if you are using one, simply make a git commit or you can run.\npre-commit run\nIf any of the configured hooks fail then the commit will not be made. Some hooks such as black may reformat files in place and you can then make another commit recording those changes and the hook should pass. Its important to pay close attention to the output.\nIf you want to run a specific hook you simply add the <id> after run.\npre-commit run <id>\nOr if you want to force running against all files (except unstaged ones) you can do so.\npre-commit run --all-files # Across all files/hooks\nAnd these two options can be combined to run a specific hook against all files.\npre-commit run <id> --all-files\nYou may find that you wish to switch branches to work on another feature or fix a bug but that your current work doesn’t pass the pre-commit and you don’t wish to sort that out immediately. The solution to this is to use git stash to temporarily save your current uncommitted work and restore the working directory and index to its previous state. You are then free to switch branches and work on another feature or fix a bug, commit and push those changes and then switch back.\nImagine you are working on branch a but are asked to fix a bug on branch b. You go to commit your work but find that a does not pass pre-commit but you wish to work on b anyway. Starting on branch a you stash your changes, switch branches, make and commit your changes to branch b then switch back to a and unstash your work there.\ngit stash\ngit checkout b\n... # Work on branch b\ngit add <changed_files_on_branch_b>\ngit commit -m \"Fixing bug on branch b\"\ngit push\ngit checkout a\ngit stash apply"
  },
  {
    "objectID": "posts/pre-commit/index.html#updating",
    "href": "posts/pre-commit/index.html#updating",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Updating",
    "text": "Updating\nYou can update hooks locally by running pre-commit autoupdate. This will update your .pre-commit-config.yaml with the latest version of repositories you have configured and these will run both locally and if you use CI/CD as described below. However this will not update any packages that are part of the - repo: local that you may have implemented and it is your responsibility to handle these."
  },
  {
    "objectID": "posts/pre-commit/index.html#pre-commit-cicd",
    "href": "posts/pre-commit/index.html#pre-commit-cicd",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Pre-commit CI/CD",
    "text": "Pre-commit CI/CD\nIdeally contributors will have setup their system to work with pre-commit and be running such checks prior to making pushes. It is however useful to enable running pre-commit as part of your Continuous Integration/Development pipeline (CI/CD). This can be done with both GitLab and GitHub although similar methods are available for many continuous integration systems.\n\nGitHub\nGitHub actions reside in the .github/workflows/ directory of your project. A simple pre-commit action is available on the Marketplace at pre-commit/action. Copy this template to .github/workflows/pre-commit.yml and include it in your Git repository.\ngit add .github/workflows/pre-commit.yml\ngit commit -m \"Adding pre-commit GitHub Action\" && git push\n\n\nGitLab\nIf you use GitLab the following article describes how to configure a CI job to run as part of your repository.\n\nHow to use pre-commit to automatically correct commits and merge requests with GitLab CI"
  },
  {
    "objectID": "posts/pre-commit/index.html#links",
    "href": "posts/pre-commit/index.html#links",
    "title": "Pre-Commit : Protecting your future self",
    "section": "Links",
    "text": "Links\n\nPre-commit\nSupported hooks\nGitHub Action\nGitLab CI"
  },
  {
    "objectID": "posts/whos_to_blame/index.html",
    "href": "posts/whos_to_blame/index.html",
    "title": "Who’s to Blame",
    "section": "",
    "text": "Git blame shows who made changes to which line of code for a given point in its history."
  },
  {
    "objectID": "posts/whos_to_blame/index.html#usage",
    "href": "posts/whos_to_blame/index.html#usage",
    "title": "Who’s to Blame",
    "section": "Usage",
    "text": "Usage\nGit blame works on individual files and so requires a filename, there are a host of options, for example -e prints the authors email address -w ignores changes to white space and -L 10,20 restricts output to the specified line range. If you want a the blame for a specific revision then you must include the hash.\ngit blame -e -w -L 10,20 f923la git_blame.org"
  },
  {
    "objectID": "posts/whos_to_blame/index.html#alias",
    "href": "posts/whos_to_blame/index.html#alias",
    "title": "Who’s to Blame",
    "section": "Alias",
    "text": "Alias\nSome people don’t like the pejorative nature of the word blame. That’s ok though, with a tweak to our configuration its possible to use the alias praise or simply who.\n# blame alias\ngit config --global alias.praise blame\ngit praise -L1,30 git_blame.org\n# who alias\ngit config --global alias.who blame\ngit who -L1,30 git_blame.org\nFor more detailed information on the array of options refer to the official documentation or see git blame --help."
  },
  {
    "objectID": "posts/whos_to_blame/index.html#ignoring-blame",
    "href": "posts/whos_to_blame/index.html#ignoring-blame",
    "title": "Who’s to Blame",
    "section": "Ignoring blame",
    "text": "Ignoring blame\nSometimes the case arises where you want to ignore blame. Perhaps the most common example is when an existing code base has been linted to conform to a particular style guide. Looking at who performed these changes is not informative and masks who made the changes and why. Its possible to ignore specific commits on the command line with --ignore-revs <hash> <file>, but it will quickly become tedious to remember to ignore all blame across multiple commits. Fortunately you can save the commits to ignore to the file .git-blame-ignore-revs (along with comments) so that they are stored. The full commit (40 characters) of hashes should be used.\n# PEP8 compliance for module X\nc00177a6121f86c001f338feff3280fd576fdbf3\n\n# PEP8 compliance for module Y\ndb27fa5f18299ca631efc430512a3f358c2b154f\nNow that you have the revisions in place to be ignored when reporting blame you can choose not to use it.\ngit blame --ignore-revs-file .gitblame-ignore-revs git_blame.org\n…but this is tedious to remember to have to do each time and ideally others on your team should use this file too. You can configure Git to use this file by modifying the local configuration. Make sure to add it to your repository so that others can use it.\ngit config blame.ignoreRevsFile .git-blame-ignore-revs\ngit add .git-blame-ignore-revs\nAs of 2022-03-08 GitHub will also ignore commits in the blame view that are listed in .git-blame-ignore-revs providing this file is in the root of your project folder."
  },
  {
    "objectID": "posts/whos_to_blame/index.html#links",
    "href": "posts/whos_to_blame/index.html#links",
    "title": "Who’s to Blame",
    "section": "Links",
    "text": "Links\n\nGeneral\n\nAtlassian | git blame\n\n\n\nResources\n\nIgnoring bulk change commits with git blame\nLittle things I like to do with Git\nIs there a way to customize the output of git blame"
  },
  {
    "objectID": "posts/linting/index.html",
    "href": "posts/linting/index.html",
    "title": "Linting - What is all the fluff about?",
    "section": "",
    "text": "NB This article originally appeared on RSE University of Sheffield but is updated here.\nIf you’ve been dabbling in programming for a while you may have heard of “linting your code” which is a process of static code analysis to remove the “fluff” from your code. Just as physically linting your clothes removes unwanted fluff, linting your code removes “fluff” and can help…\nThis helps reduce the technical debt which impacts the amount of time required for maintenance and further development of a code base. The main focus of this article is the use of linting to ensure consistent coding style, it focuses on Python under Linux but similar tools are available for other operating systems and languages."
  },
  {
    "objectID": "posts/linting/index.html#style-matters",
    "href": "posts/linting/index.html#style-matters",
    "title": "Linting - What is all the fluff about?",
    "section": "Style Matters",
    "text": "Style Matters\nWhat has style got to do with writing code? Trends come and go in fashion but coding styles are meant to be relatively static and not change with the season, although they can and do evolve over time. This is because using a consistent and widely used style when writing code makes it easier for other people, often your future self, to read and understand the code you have written. If code is easier to understand then its easier to modify, update, extend, improve and in general maintain.\nA useful insight from Gudio van Rossum, the creator of Python is that “code is read much more often than it is written” and so it should be easy to understand and not obfuscate its intent. Python is quite good for this as it is an expressive language which encourages coders to be explicit when naming variables, functions, classes and so forth so that their purpose and intention is clear, although the same is true of most modern languages. However, going a step further and using consistent styles to format and layout code helps enhance this."
  },
  {
    "objectID": "posts/linting/index.html#linting-in-python",
    "href": "posts/linting/index.html#linting-in-python",
    "title": "Linting - What is all the fluff about?",
    "section": "Linting in Python",
    "text": "Linting in Python\nThe most widely used Python style is defined in the long established PEP 8: The Style Guide for Python Code. There are a number of tools available that will lint your Python code for you and most integrate with your IDE, whether that is Visual Studio Code, PyCharm or Emacs. Some of the formatting and linting tools available for Python are…\n\nPylint - checks for errors in Python code, tries to enforce a coding standard and looks for code smells.\nYAPF - takes the code and reformats it to the best formatting that conforms to the style guide.\nBlack - The Uncompromising Code Formatter\nFlake8 - Your Tool For Style Guide Enforcement\nProspector - Python Static Analysis\nmypy - Optional Static Typing for Python\n\nHere we will work through linting and formatting the simple file below (available as a download here) using PyLint and Black.\nimport numpy as np\nfrom pathlib import Path\nfrom typing import Union\nimport csv\n\ndef save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -> None:\n    \"\"\"Save a list of random numbers (floats) to the given file.\n\n    The stated number of random numbers will be saved to the given target file, if the directory structure\n    doesn't exist it will be created. Output will by default be over-written.\n    Parameters\n    ----------\n    size : int\n        Number of random numbers to generate\n    seed: int\n        Seed for random number generation\n    save_as : Union[str, Path]\n        Directory/file to save numbers to.\n    \"\"\"\n    rng = np.random.default_rng()\n    random_numbers = rng.random(size)\n\n    with Path(save_as).open('w') as out:\n        writer = csv.write(out)\n        writer.writerows(random_numbers)\n\nLinting with PyLint\nWe will lint this file using Pylint to find out what errors there are and how its style can be improved to conform with PEP8 guidelines.\nFirst you need to install pylint, typically in your virtual environment.\npip install pylint\nPylint can be configured using a ~/.pylintrc file in your home directory and over time this will grow as you customise your configuration but for now we will make one simple change from the default which is to increase the accepted line length. Create the file and save it with the following content.\n[FORMAT]\n## Maximum number of characters on a single line.\nmax-line-length=120\nOpen a terminal and navigate to the location you saved the example file save_random_numbers.py activate the virtual environment you installed pylint under if its not already being used and then type the following to run Pylint against your code…\npylint save_random_numbers.py\nYou should see output similar to the following…\n ❱ pylint save_random_numbers.py\n************* Module save_random_numbers\nsave_random_numbers.py:1:0: C0114: Missing module docstring (missing-module-docstring)\nsave_random_numbers.py:5:66: E0602: Undefined variable 'Union' (undefined-variable)\nsave_random_numbers.py:5:35: W0613: Unused argument 'seed' (unused-argument)\nsave_random_numbers.py:2:0: C0411: standard import \"from pathlib import Path\" should be placed before \"import numpy as np\" (wrong-import-order)\nsave_random_numbers.py:3:0: C0411: standard import \"import csv\" should be placed before \"import numpy as np\" (wrong-import-order)\n\n-------------------------------------------------------------------\nYour code has been rated at 0.00/10\nThe output tells us which module has been inspected on the first line. Each subsequent line indicates\n\nThe file.\nThe line the problem has been encountered on.\nThe column.\nA somewhat cryptic error code and then a message about the problem\nA more descriptive generic message associated with the error code.\n\nAt the moment we are only looking at one file, but when using PyLint against larger code bases this information is vital in helping direct you to the location of code that needs changing. At the end PyLint rates your code, ideally you should aim to get a score of 10.0/10.\nThe messages are quite informative, taking each in turn we can work through resolving them.\n\nMissing module docstring (missing-module-docstring)\nEach Python module should have a docstring as the very first line that describes what it does. In this example it might be considered superflous but its good practice to get in the habit of writing these as it comes in useful when documentation is automatically generated from the docstrings in the code. To fix it we can add a short docstring at the top.\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport numpy as np\nfrom pathlib import Path\n\n\nUndefined variable 'Union' (undefined-variable)\nThis error arises because the type hint uses Union but it hasn’t been imported. It’s from the typing module so we can import it.\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport numpy as np\nfrom pathlib import Path\nfrom typing import Union\n\n\nUnused argument 'seed' (unused-argument)\nThis is very useful to be informed about because the seed argument, according to the docstring, is meant to be used in the call to the random number generator and ensures we will get the same set of random numbers generated each time we call the function with that seed, however, as Pylint has informed us we haven’t actually used it within the save_random_number() function. We can correct that by adding it when we instantiate the random number generator.\nrng = np.random.default_rng(seed=seed)\n\n\nstandard import \"from pathlib import Path\" should be placed before \"import numpy as np\" (wrong-import-order)\nThis message, like the one that follows it, is telling us that the order in which we have imported modules is incorrect, because the PEP8 guide recommends that core modules, which both csv and pathlib are, should be imported before other modules. We can correct this by changing the order (and because we have added an import from the typing module which is also a core module we move that too).\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport csv\nfrom pathlib import Path\nfrom typing import Union\n\nimport numpy as np\nOnce corrected your file should look like this…\n\"\"\"Module for saving randomly generated numbers.\"\"\"\nimport csv\nfrom pathlib import Path\nfrom typing import Union\nimport numpy as np\n\ndef save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -> None:\n    \"\"\"Save a list of random numbers (floats) to the given file.\n\n    The stated number of random numbers will be saved to the given target file, if the directory structure\n    doesn't exist it will be created. Output will by default be over-written.\n\n    Parameters\n    ----------\n    size : int\n        Number of random numbers to generate\n    seed: int\n        Seed for random number generation\n    save_as : Union[str, Path]\n        Directory/file to save numbers to.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    random_numbers = rng.random(size)\n\n    with Path(save_as).open('w') as out:\n        writer = csv.write(out)\n        writer.writerows(random_numbers)\n…and you can now run PyLint against it to see if you’ve improved your score.\n ❱ pylint save_random_numbers.py\n************* Module save_random_numbers\nsave_random_numbers.py:7:66: E1136: Value 'Union' is unsubscriptable (unsubscriptable-object)\n\n------------------------------------------------------------------\nYour code has been rated at 5.00/10 (previous run: 4.00/10, +1.00)\nThat is an improvement in score (of +1.00) but we now have another error telling us that E1136: Value 'Union' is unsubscriptable (unsubscriptable-object). You are unlikely to know what all the error codes mean, but there are a few handy on-line lists all PyLint codes or all PyLint messages and what they are telling you are worth consulting (The Little Book of Python Anti-Patterns is also useful). In this instance PyLint has returned a false-positive because Union can and should be subscripted here because it means the argument can be either a string (str) or a pathlib Path (Path). So how do we get around this complaint?\nYou can disable PyLint from complaining about specific error codes/messages on a per-file basis by adding a line that disables them. You can use either codes or messages (the bit in the brackets at the end of the line, in this case unsubscriptable-object) and it is advisable to use the message form as it is more informative to those who read your code subequently.\nIf we add the following line it prevents PyLint from reporting the specific error…\nimport numpy as np\n\n# pylint: disable=unsubscriptable-object\n\ndef save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -> None:\n…running PyLint against our code again we get a much better score.\n ❱ pylint save_random_numbers_tidy.py\n\n-------------------------------------------------------------------\nYour code has been rated at 10.00/10 (previous run: 5.00/10, +5.00)\n\n\n\nConfiguring PyLint\nThe last error we encountered is something that is likely to crop up again if you use Typehints liberally throughout your Python code (and I would encourage you to do so). Rather than having to remember to disable the error in each file/module we create we can configure PyLint via its configuration file ~/.pylintrc to always ignore this error. To do so add the following…\n[MESSAGES CONTROL]\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).\ndisable=unsubscriptable-object\nFor more on configuriong PyLint refer to the documentation and also details of how to integrate with your editor and IDE\n\n\nAutomated Formatting with Black\nBlack is The Uncompromising Code Formatter and is very strict about the way in which it formats code. This could be a good or bad thing depending on your point of view, but it does result in highly consistent code when applied to all files. It formats files in place, so be mindful of this if you run it against one of your files it will change it.\nInstall black in your virtual environment and make a backup of your save_random_number.py file that you have just tidied up with linting.\npip install black\ncp save_random_numbers.py tidy_save_random_numbers.py\nTo run black against your code pass it the input file, it will re-write it and you can then compare it against the backup you just made…\nblack save_random_numbers.py\n❱ diff save_random_numbers.py tidy_save_random_numbers.py\n5,8c5\n<\n< def save_random_numbers(\n  <     size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\"\n  < ) -> None:\n---\n> def save_random_numbers(size: int, seed: int = 87653546, save_as: Union[str, Path] = \"./random_numbers.txt\") -> None:\n27c24\n<     with Path(save_as).open(\"w\") as out:\n---\n>     with Path(save_as).open('w') as out:\nIn this instance Black hasn’t changed much but it has reformatted the def save~randomnumbers~(...) line and moved the with Path() line as a consequence."
  },
  {
    "objectID": "posts/linting/index.html#when-to-lint",
    "href": "posts/linting/index.html#when-to-lint",
    "title": "Linting - What is all the fluff about?",
    "section": "When to Lint",
    "text": "When to Lint\nIt is worth linting your code from the outset of a project as not only does it result in a consistent style across your code base it also avoids the problem that can arise when applying linting retrospectively. If an existing code base has linting applied then the git blame, which indicates who the last person to edit a section was, then resides with the person who applied the linting, rather than the original author of the code. Its possible though that the person who applied the linting knows very little about the underlying functionality of the code but they may receive questions about it if they are indicated as the last person to have modified particular lines.\nFortunately there are a number of ways to automate and integrate linting into your workflow."
  },
  {
    "objectID": "posts/linting/index.html#automating-linting",
    "href": "posts/linting/index.html#automating-linting",
    "title": "Linting - What is all the fluff about?",
    "section": "Automating Linting",
    "text": "Automating Linting\n\nIDE Integration\nWhen programming it is really useful to use an Integrated Development Environment (IDE) as most allow the integration of linting tools and apply them to your code automatically, whether its using PyLint, YAPF, Black or otherwise. Setup and configuration is beyond the scope of this article but some links are provided to useful resources to get you started.\n\n\nVSCode\nVSCode supports linting in most languages, and both Python and R are supported along with other languages.\n\n\nPyCharm\nPyCharm supports automated formatting of code, for more information please refer to Reformat and rearrange code | PyCharm.\n\n\nEmacs\nThere are various options available for linting within Emacs, which you use depends on your preferences but LSP mode integrates with YAPF (via yapfify), Flake8 (via flycheck) and Black (via blacken)."
  },
  {
    "objectID": "posts/linting/index.html#git-integration",
    "href": "posts/linting/index.html#git-integration",
    "title": "Linting - What is all the fluff about?",
    "section": "Git Integration",
    "text": "Git Integration\nIf you are using an IDE then if configured correctly your code should be linted automatically for you, but an additional step that can capture anything that hasn’t been correctly formatted is to use a git hook to run linting on your code prior to making commits. There is git-pylint-commit-hook available on PyPi which runs automatically when you make commits to .py files."
  },
  {
    "objectID": "posts/linting/index.html#continuous-integration",
    "href": "posts/linting/index.html#continuous-integration",
    "title": "Linting - What is all the fluff about?",
    "section": "Continuous Integration",
    "text": "Continuous Integration\nIncluding a linting stage in your Continuous Integration (CI) pipeline pays dividends as we all make mistakes and sometimes forget to lint our code before making pushes."
  },
  {
    "objectID": "posts/linting/index.html#megalinter",
    "href": "posts/linting/index.html#megalinter",
    "title": "Linting - What is all the fluff about?",
    "section": "Megalinter",
    "text": "Megalinter\nPerhaps not necessary for everyone but worth mentioning the beast that is MegaLinter which will lint code across multiple languages and integrates easily into your pipeline (GitHub Action, CI on GitLab, Jenkins etc.). A useful article on doing so is Limit your technical debt and secure your code base using MegaLinter."
  },
  {
    "objectID": "posts/linting/index.html#pre-commit",
    "href": "posts/linting/index.html#pre-commit",
    "title": "Linting - What is all the fluff about?",
    "section": "Pre-commit",
    "text": "Pre-commit\nPre-commit is a Python package that adds a set of configurable hooks for linting your code, and not just Python, using a Git pre-commit hook. Hooks are run conditional on certain changes in states, in this case code that is run before commits are made. It creates a virtual Python Environment and installs the required packages there to lint your code. More will be written on this in a subsequent post."
  },
  {
    "objectID": "posts/linting/index.html#links",
    "href": "posts/linting/index.html#links",
    "title": "Linting - What is all the fluff about?",
    "section": "Links",
    "text": "Links\n\nPython\n\nFlake8 - Your Tool For Style Guide Enforcement\nBlack - The Uncompromising Code Formatter\nLinting Python in Visual Studio Code\nPylint - Overview of all Pylint messages\n\n\n\nR\n\nGitHub - r-lib/lintr: Static Code Analysis for R\nIntroduction to R: Linting R (and R Markdown)\n\n\n\nC++\n\ncpplint"
  },
  {
    "objectID": "posts/git-ssh/index.html",
    "href": "posts/git-ssh/index.html",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "",
    "text": "How to configure individual Git repositories to use specific SSH keys. This is useful if you have more than one account on a forge, for example a personal and work account."
  },
  {
    "objectID": "posts/git-ssh/index.html#background",
    "href": "posts/git-ssh/index.html#background",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Background",
    "text": "Background\nTypically when pushing and pulling changes to a forge such as GitHub, GitLab or Codeberg you use an SSH (Secure SHell) key to authenticate that you have permission to access the repository."
  },
  {
    "objectID": "posts/git-ssh/index.html#ssh-keys",
    "href": "posts/git-ssh/index.html#ssh-keys",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "SSH Keys",
    "text": "SSH Keys\n\nConcept\nSSH keys are, in conjunction with “keychains”, used to save you having to enter a password each time you connect from one computer to another. They are generated on your computer and consist of two parts, a private key which remains on your computer and a public key which you place on remote computers you wish to connect to. There is a password associated with your key which is required to “unlock” your private key on your computer. Only an unlocked private key will match with a public key. Think of the public key as the lock on your front door, and the private key the key you carry on your traditional, physical, keychain/keyring. Only when the two match will things be unlocked, although you have to unlock your private key when you want to use it just as you have to get your keys out of your pocket (although “keychains” help with this).\n\n\nGeneration\nThere are different algorithms for generating SSH key pairs. DSA is no longer considered secure and RSA keys should have at least 2048-bits if not 4096-bits. A good choice these days is to use an elliptic curve based key such as ed25519 as they are shorter and faster. For more on why you should use this key see the article Upgrade your SSH keys!.\nTo generate a key use the following command entering a secure (i.e. long) password.\nssh-keygen -o -a 100 -t ed25519\nYou will be prompted for a filename to save your keys to, so you should know where to find them (the default is ~/.ssh/id_ed25519[.pub]). You have a private key ~/.ssh/id_ed25519 and a public ~/.ssh/id_ed25519.pub and we will use this to set up authentication on your Git Forge.\n\n\nForge Configuration\nUnder your account settings on your chosen Git Forge navigate to Settings > SSH and GPG Keys and select Add New Key on (GitHub). On GitLab navigate to Preferences > SSH Keys GitLab), this page allows you to add a new key.\nYou need to copy and paste your public key into the Key box on these pages and give it a name (typically the hostname of your computer is a good choice). To view your public key simply use cat and copy and paste it. You can optionally choose to set an expiration date for your key which is good practice but means you have to generate new keys in the future.\ncat ~/.ssh/id_ed25519.pub"
  },
  {
    "objectID": "posts/git-ssh/index.html#git-global-ssh-configuration",
    "href": "posts/git-ssh/index.html#git-global-ssh-configuration",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Git Global SSH Configuration",
    "text": "Git Global SSH Configuration\nTypically your global configuration for which key to use is set in ~/.ssh/config with an entry similar to the below.\nHost github.com\n     User git\n     Port 22\n     PreferredAuthentications publickey\n     IdentityFile ~/.ssh/id_ed25519\nHere it uses the User name git on port 22. The preferred authentication method is using a publickey and the private key used is stored locally at ~/.ssh/id_ed25519.\nWhen asked to connect to a forge using SSH (e.g. git pull or git push) will look through the ~/.ssh/config file to see if there is a configuration section that matches the target and if so use the configuration defined there-in. You will then be prompted for your SSH private key password.\n\nWhat are Keychains?\nYou may be wondering how an SSH key makes your life easier, you are still prompted to enter a password when trying to interact with a Git Forge, or use it in a more traditional manner to connect over SSH to another server. This is where the magic of a “keychain” steps in to make your life easier, you still have to enter a password but only once to add your SSH key to the “keychain”. Typically keychains are front-ends for interacting with and managing SSH agent. The name is apt since you add your SSH key to the keychain once, typically on log-in, and are asked for your password to unlock it and then stores it in the SSH agent. Then each time SSH requires an SSH key it retrieves it from the keychain rather than prompting you for a password.\nThere are many different implementations of keychain such as the Funtoo Keychain Project, Seahorse the GNOME GUI management tool,"
  },
  {
    "objectID": "posts/git-ssh/index.html#git-per-repository-configuration",
    "href": "posts/git-ssh/index.html#git-per-repository-configuration",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Git Per Repository Configuration",
    "text": "Git Per Repository Configuration\nWe now get to the meat of this post, how to configure individual repositories to use specific SSH keys. This may be desirable if you have two accounts on the same forge e.g. both on GitHub.com or both on GitLab.com? As of Git 2.10.0 you can configure each repository to use a specific key (source). At the command line…\ncd a/git/repository\ngit config core.sshCommand \"ssh -i ~/.ssh/work_ed25519 -F /dev/null\"\ngit config --local user.name \"Username\"\ngit config --local user.email \"repos@username.com\"\nThis adds the following to the repositories configuration which is stored under .git/config and you can of course enter this directly to the configuration file yourself.\n[core]\n    sshCommand = ssh -i ~/.ssh/work_ed25519 -F /dev/null\n[user]\n    name = Username\n    email = repos@username.com\nWhat is this doing? Well it’s instructing Git to run ssh using the private key file (with the -i flag to specify the identity_file) that is located at ~/.ssh/work_ed25519. Providing you have…\n\nAlready uploaded the public key (work_ed25519.pub) to your GitHub account.\nStored this key in a Keychain as described above.\n\n…you shouldn’t be prompted for a password.\nYou can now configure, on a repository basis, which SSH key is used by Git when pushing/pulling changes from the remote origin (typically a forge such as GitHub, GitLab, Codeberg or so forth). If however you have multiple projects you wish to setup with an alternative SSH key configuration it can be tedious to configure each repository. Thankfully Git >= 2.13 introduced Conditional includes to the configuration."
  },
  {
    "objectID": "posts/git-ssh/index.html#conditional-includes",
    "href": "posts/git-ssh/index.html#conditional-includes",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Conditional Includes",
    "text": "Conditional Includes\nYou global configuration is stored in ~/.gitconfig and defines key variables such user and name, the default editor and many other options, including a customised sshCommand as was added above to a local .git/config file.\nGit 2.13 introduced the aforementioned Conditional includes which works “_by setting a includeIf.<condition>.path variable to the name of the file to be included.”. For our current case-use the <condition> we are interested in is whether the path, which is interpreted as a pattern, is a gitdir then we include what follows.\nFor example, we place all of our work related Git repositories under the ~/work/ directory and wish to use ~/.ssh/work_ed25519 for these and keep all of our personal repositories elsewhere and wish to use our main ~/.ssh/id_ed25519 key for those.\nOut ~/.gitconfig should look like\n[user]\n    name = Your Name\n    email = your.personal@email.com\n\n[includeIf \"gitdir:~/work/\"]    # Directory paths ending in '/** has the globbing wildcard '**' added by default.\n    path = ~/work/.gitconfig_work\n\n[core]\n    sshCommand = ssh -i ~/.ssh/id_ed25519 -F /dev/null\nThen our ~/work/.gitconfig_work can contain the alternative values we wish to use for all repositories under the ~/work/ directory.\n[user]\n    name = Your Name\n    email = your.work@email.com\n\n[core]\n    sshCommand = ssh -i ~/.ssh/work_ed25519 -F /dev/null"
  },
  {
    "objectID": "posts/git-ssh/index.html#commit-verification-with-ssh",
    "href": "posts/git-ssh/index.html#commit-verification-with-ssh",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Commit verification with SSH",
    "text": "Commit verification with SSH\nVerification of commits is a useful security feature, but beyond the scope of this article but as doing so with SSH keys is a recently supported feature on GitHub (see blog SSH commit verification now supported) I felt it worth mentioning."
  },
  {
    "objectID": "posts/git-ssh/index.html#links",
    "href": "posts/git-ssh/index.html#links",
    "title": "Git : Custom SSH credentials for git repositories",
    "section": "Links",
    "text": "Links\n\nSSH\n\nSSH Academy\nOpenSSH Key Management, Part 1\nOpenSSH Key Management, Part 2\nOpenSSH Key Management, Part 3\n\n\n\nForges\n\nGitHub | Connect with SSH\nGitLab | Use SSH keys to communicate with GitLab\nCodeberg | Adding an SSH key to your account"
  },
  {
    "objectID": "posts/sphinx-docs/index.html",
    "href": "posts/sphinx-docs/index.html",
    "title": "Sphinx Documentation",
    "section": "",
    "text": "How to generate documentation websites for your Python package using Sphinx, including generating API documentation automatically, build multiple versions across releases and automatically build and host them on GitHub Pages."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#pre-requisites",
    "href": "posts/sphinx-docs/index.html#pre-requisites",
    "title": "Sphinx Documentation",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nThe instructions here assume that you have your Python Packaging well structured, under version control and backed up on GitHub."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#initial-setup",
    "href": "posts/sphinx-docs/index.html#initial-setup",
    "title": "Sphinx Documentation",
    "section": "Initial Setup",
    "text": "Initial Setup\nSphinx comes with the sphinx-quickstart interactive tool which will help setup your repository with a basic conf.py and Makefile. There are a number of command line options but it is also interactive so you can answer questions to configure your setup. I like to keep the source and build directories separate and so use the --sep flag as well as the --makefile flag to generate a Makefile for building documentation on GNU/Linux or OSX (if you use M$-Win the use the --batchfile flag instead).\nI keep documentation under docs/ directory within the root of the package directory.\ncd ~/path/to/package\nmkdir docs\ncd docs\nsphinx-quickstart --makefile"
  },
  {
    "objectID": "posts/sphinx-docs/index.html#conf.py",
    "href": "posts/sphinx-docs/index.html#conf.py",
    "title": "Sphinx Documentation",
    "section": "conf.py",
    "text": "conf.py\nConfiguration is via a conf.py the automatically generated conf.py produced by sphinx-quickstart. It is well commented and instructive on how to use it to configure Sphinx and contains details on adding/modifying various sections of the this file.\nKey sections are the list of extensions that your documentation uses."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#index.rst",
    "href": "posts/sphinx-docs/index.html#index.rst",
    "title": "Sphinx Documentation",
    "section": "index.rst",
    "text": "index.rst\nThe front-page of your website, typically index.html for static sites, is derived from index.rst. You can write welcome details about your project and link to other pages you have written. Typically I write all but the front matter in Markdown.\nWelcome to my packages documentation\n====================================\n\nThis is my package, there are many packages like it but this one is mine.\n\nIncluding Markdown\nI already know Markdown fairly well and would rather use that to write documents (as I do with thisblog). Fortunately Sphinx can incorporate documentation written in Markdown using the myst_parser package. Simply include it in the extensions.\n\nextensions = [\n    \"myst_parser\",\n]\n\nBy default it works with extensions of .md but if there are other flavours you wish to include (e.g. .Rmd for RMarkdown or .qmd for Quarto) you add them to the source_suffix in docs/conf.py\n\nsource_suffix = {\".rst\": \"restructuredtext\", \".md\": \"markdown\"}\n\nIn your index.rst you can then list the Markdown filenames, without extensions. For example if you have an installation.md and configuration.md place them in the same directory as index.rst (the root docs/) directory and have in your index.rst have…\nWelcome to my packages documentation\n====================================\n\nThis is my package, there are many packages like it but this one is mine.\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Getting Started\n   introduction\n   configuration\n\nMarkdown Tables\nIf you have tables in Markdown (and its likely that you will) then you will need the sphinx-markdown-tables package which ensures they are rendered correctly.\n\n\nMermaid Diagrams\nFurther Sphinx has support for Mermaid diagrams that have been written in Markdown documents via the sphinxcontrib-mermaid package. This means that you can include all sorts of neat diagrams such as Git Graph.\n\n\n\n\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true,'showCommitLabel': true, 'rotateCommitLabel': true}} }%%\ngitGraph\n    commit\n    commit\n    branch bug1\n    checkout main\n    commit\n    checkout bug1\n    commit\n    commit\n    checkout main\n    branch bug2\n    checkout bug2\n    commit\n    commit\n    checkout bug1\n    commit\n    checkout main\n    merge bug1 tag: \"v0.1.1\"\n    checkout bug2\n    commit\n    commit\n    checkout main\n    merge bug2 tag: \"v0.1.2\"\n    commit"
  },
  {
    "objectID": "posts/sphinx-docs/index.html#including-api-documentation",
    "href": "posts/sphinx-docs/index.html#including-api-documentation",
    "title": "Sphinx Documentation",
    "section": "Including API Documentation",
    "text": "Including API Documentation\nAs you write your package it is good practice include docstrings for each module/class/method/function that you write. For Python there are several different styles for writing these, my personal preference is for numpydoc style but regardless of your preference you should write them. They are invaluable to users (including your future self) to understand how the code works and as many modern Integrated Development Environments (IDEs) supporting functionality to show the documentation for functions as you type they are an invaluable reference. If you’re an Emacs user then you can leverage the numpydoc package to automatically insert NumPy docstrings in Python function definitions based on the function definition, it automatically detects names, type hints, exceptions and return types to generate the docstring (yet another reason to use Emacs!).\nWhilst it is useful to have this API available in an IDE as you work it is also useful to include the reference on a packages website and this is relatively straight-forward with Sphinx which provides several tools and extensions.\n\nsphinx-apidoc\nThe first is the the sphinx-apidoc command to generate documentation from the embedded docstrings. This is a command line tool that could be added to the Makefile.\n\n\nsphinx-autoapi\nHowever, rather than learning the intricacies of using this command the package Sphinx extensions sphinx-autoapi can be leveraged to automatically build the API documentation for you. This is particularly useful when you come to build multiple versions of your documentation as it means you do not have to include the .rst files that sphinx-apidoc generates in your repository they are generated on the fly when Sphinx builds each version of the documentation.\nConfiguration is via docs/conf.py and the package needs referencing in the extensions section then configuring at a bare minimum which directories to generate documentation from. i\n\nextensions = [\n    \"autoapi.extension\",\n]\n\n# -- autoapi configuration ---------------------------------------------------\nautotype_api = \"python\"\nautoapi_dirs = [\"../mypackage\"]\n\nThere are a lot of subtle configuration options and I would recommend reading the documentation and working through the Tutorials and How To Guides.\nThis has the added advantage that it works with ReadTheDocs.\n\n\nSphinx Autosummary\nIn addition the sphinx_ext_autosummary automates summarising the API docstrings.\nAdd the package as a dependency to the extensions…\n\nextensions = [\n    \"sphinx.ext.autosummary\"\n]\n\nUnder the index.rst you should include a section header for the api that references an api.rst page for inclusion.\n.. toctree::\n   :maxdepth: 2\n   :caption: API\n\n   api\nAnd then create the api.rst page which need only have the following. By including :recursive: the sub-modules will be included automatically.\nAPI\n===\n\n.. autosummary::\n   :recursive:\n   :toctree: generated\n\n   mypackage"
  },
  {
    "objectID": "posts/sphinx-docs/index.html#multiple-versions",
    "href": "posts/sphinx-docs/index.html#multiple-versions",
    "title": "Sphinx Documentation",
    "section": "Multiple Versions",
    "text": "Multiple Versions\nOver time code and in turn documentation changes, not just the API but the documents written to demonstrate installation and usage of software. Not everyone always uses the latest version of your software and so it can be useful to provision documentation for each version that is available. Fortunately the Sphinx extension sphinx-multiversion makes this relatively painless.\nYou need to include it in the list of extensions of docs/conf.py\n\nextensions = [\n    \"sphinx_multiversion\",\n]\n\n\nConfiguring Versions\n\nSidebar\nFor versions to not just be built but available you need to include a section on your site that allows selecting which version of the documentation to view. Sidebars are included via HTML templates and you need to configure the path to this directory and the name of the HTML file within it. The following options in the conf.py configure the _templates directory and within it the versioning.html file.\n\ntemplates_path = [\n    \"_templates\",\n]\n\nhtml_sidebars =  {\"**\":   \"versioning.html\",}\n\nThe versioning.html file can take a number of formats, refer to the documentation for all options, but the following is an example.\n{% if versions %}\n<h3>{{ _('Versions') }}</h3>\n<ul>\n  {%- for item in versions %}\n  <li><a href=\"{{ item.url }}\">{{ item.name }}</a></li>\n  {%- endfor %}\n</ul>\n{% endif %}\nEnsure this file is under Git version control, it is needed to build your pages on GitHub.\n\n\nTags/Branches\nIf no options are set then sphinx-multiversion will build documentation for all branhces, which is probably undesirable. Typically you want to restrict this to the released versions which are identified by git tags and perhaps your main/master branch. If you prefix your tags with v and you want to build the documentation for the HEAD of your main (or master) branch then you should set options as shown below for sphinx-multiversion. I like to be able to test documentation builds and so I have a section that allows me to include a given branch.\n\nsmv_tag_whitelist = r\"^v\\d+.*$\"  # Tags begining with v#\nsmv_branch_whitelist = r\"^main$\"  # main branch\n# If testing changes locally comment out the above and the smv_branch_whitelist below instead. Replace the branch name\n# you are working on (\"ns-rse/testing-branch\" in the example below) with the branch you are working on and run...\n#\n# cd docs\n# sphinx-multiversion . _build/html\n#\n# smv_branch_whitelist = r\"^(main|ns-rse/testing-branch)$\"  # main branch\nsmv_released_pattern = r\"^tags/.*$\"  # Tags only\n# smv_released_pattern = r\"^(/.*)|(main).*$\"  # Tags and HEAD of main\nsmv_outputdir_format = \"{ref.name}\"\n\nIf you are testing locally be sure to revert the commented sections so that the branch is not built on GitHub Pages."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#themes",
    "href": "posts/sphinx-docs/index.html#themes",
    "title": "Sphinx Documentation",
    "section": "Themes",
    "text": "Themes\nThere are a number of different themes available for including in your package. Which is used is defined by the html_theme variable in docs/conf.py. I like the pydata-sphinx-theme that is used by Pandas/Matplotlib.\n\nhtml_theme = \"pydata_sphinx_theme\""
  },
  {
    "objectID": "posts/sphinx-docs/index.html#package-dependencies",
    "href": "posts/sphinx-docs/index.html#package-dependencies",
    "title": "Sphinx Documentation",
    "section": "Package Dependencies",
    "text": "Package Dependencies\nSince the documentation is part of your package it is important to include all of the dependencies that are required for building the documentation dependencies of your package so they can be easily installed and are available to Sphinx (since Sphinx will try loading anything listed in your docs/conf.py). These days you should really be using pyproject.toml to configure and manage your package, if you are unfamiliar with the packaging process see my post on Python Packaging.\n\n[project.optional-dependencies]\n\ndocs = [\n  \"Sphinx\",\n  \"myst_parser\",\n  \"numpydoc\",\n  \"pydata_sphinx_theme\",\n  \"sphinx-autoapi\",\n  \"sphinx-autodoc-typehints\",\n  \"sphinx-multiversion\",\n  \"sphinx_markdown_tables\",\n  \"sphinx_rtd_theme\",\n  \"sphinxcontrib-mermaid\",\n]\n\nEnsure all of these dependencies are installed in your Virtual Environment.\ncd ~/path/to/package\npip install .[docs]"
  },
  {
    "objectID": "posts/sphinx-docs/index.html#building-documentation",
    "href": "posts/sphinx-docs/index.html#building-documentation",
    "title": "Sphinx Documentation",
    "section": "Building Documentation",
    "text": "Building Documentation\nYou are now ready to build your documentation locally.\ncd ~/path/to/package/docs\nmkdir -p _build/html\nsphinx-multiversion . _build/html\nOutput should reside under the ~/path/to/package/docs/_build/html/ directory and there should be a directory for every tag as well as main (or master).\n\nDeploying on GitHub Pages\nThe final stage is to leverage GitHub Pages to host your documentation. This can be achieved using a GitHub Action. These are a way of running certain tasks automatically on GitHub in response to certain actions. You can configure your actions to use those defined by others. I found the action-sphinx-docs-to-gh-pages action for generating Sphinx documentation but it didn’t support generating API documentation nor multiple versions of documentation so I have forked it and added this functionality (I intend to work with the authors and push the changes upstream).\nTo use this action you need to create a file in ~/path/to/package/.github/workflows/ called sphinx_docs_to_gh_pages.yaml and copy and paste the following YAML.\nname: Sphinx docs to gh-pages\n\non:\n  push:\n  workflow_dispatch:\n\njobs:\n  sphinx_docs_to_gh-pages:\n    runs-on: ubuntu-latest\n    name: Sphinx docs to gh-pages\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n      - name: Setup Python\n        uses: actions/setup-python@v4.3.0\n        with:\n          python-version: 3.9\n      - name: Installing the Documentation requirements\n        run: |\n          pip3 install .[docs]\n      - name: Running Sphinx to gh-pages Action\n        uses: ns-rse/action-sphinx-docs-to-gh-pages@main\n        with:\n          # When testing set this branch to your branch, when working switch to main. It WILL fail if not\n          # defined as it defaults to 'main'.\n          branch: main\n          dir_docs: docs\n          sphinxapiexclude: '../*setup* ../*tests* ../*.ipynb ../demo.py ../make_baseline.py ../jupyter_notebook_config.py ../demo_ftrs.py'\n          sphinxapiopts: '--separate -o . ../'\n          sphinxopts: ''\n          multiversion: true\n          multiversionopts: ''\n\nSave, add and commit to your Git repository and push the changes to GitHub."
  },
  {
    "objectID": "posts/sphinx-docs/index.html#links",
    "href": "posts/sphinx-docs/index.html#links",
    "title": "Sphinx Documentation",
    "section": "Links",
    "text": "Links\n\nSphinx\n\n\nSphinx Extensions\n\nmyst_parser\nsphinx-autoapi\nsphinx-markdown-tables\nsphinx-multiversion\nsphinxcontrib-mermaid\nsphinx-ext-autosummary\n\n\n\nGitHub\n\nGitHub Pages\nGitHub Action\nSphinx docs to GitHub Pages · Actions · GitHub Marketplace ( my fork with added sphinx-multiversion support)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Neil and work as a Research Software Engineer at the University of Sheffield. My education started with BSc in Zoology and Genetics followed by a MSc in Genetic Epidemiology. My academic career started with just over a decade as a Genetics Statistician where I learnt GNU/Linux system administration and reproducible research practices before shifting to Medical Statistics and working on Clinical Trials for a similar amount of time and taught myself R before a stint as a Data Scientist in industry company where I not only learnt Python but about good software development practices.\nHere you’ll find posts about Research Software Engineering, Git/GitHub/GitLab, GNU/Linux (Gentoo, Arch, OpenWRT), Python, Bash, R, Emacs, Org-mode, Statistics, Genetics, Evolution and more.\nWhen not working I enjoy climbing, running, cycling, cooking, hiking, photography, gardening and spending time with my family.\n\n\n\n\n\n\n  \n\n\n\n\nGitLab CI - Automatic Publishing to PyPI\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-commit and R Packaging\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Useful Hooks\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nSphinx Documentation\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPython Packaging\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nBrowser Extensions\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit.ci : Integrating Pre-Commit into CI/CD\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nRunning in 2022\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nWho’s to Blame\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Customising and Updating\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nLinux Command Line Alternatives\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nPre-Commit : Protecting your future self\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nGit : Custom SSH credentials for git repositories\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\n  \n\n\n\n\nLinting - What is all the fluff about?\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nNeil Shephard\n\n\n\n\n\n\nNo matching items"
  }
]